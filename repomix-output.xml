This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py
- Files matching these patterns are excluded: worktrees/**, **/*.worktree
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
dev/
  benchmark_segmentation.py
  compare_segmentation_methods.py
  create_test_cases.py
  debug_segmentation.py
  sweep_temperature_params.py
  verify_ai_classification.py
relay-server/
  convert-postman-to-md.py
scripts/
  _deprecated/
    deduplicate_spells.py
    fetch_all_spells.py
    read_spells_compendium.py
  chapter2_to_html.py
  delete_all_actors.py
  fetch_actors.py
  fetch_items.py
  full_pipeline.py
  generate_import_diagram.py
  generate_scene_art.py
  pit_fiend_end_to_end.py
  process_and_upload.py
  process_pit_fiend.py
  process_statblock.py
  test_10_images.py
  test_make_run.py
  test_parallel_image_gen.py
  test_redline_walls.py
  test_reference_image.py
src/
  actors/
    __init__.py
    extract_npcs.py
    extract_stat_blocks.py
    generate_actor_biography.py
    generate_actor_file.py
    models.py
    orchestrate.py
    parse_stat_blocks.py
    process_actors.py
    statblock_parser.py
  foundry/
    actors/
      __init__.py
      converter.py
      deduplicate.py
      manager.py
      models.py
      parser.py
      spell_cache.py
    items/
      __init__.py
      deduplicate.py
      fetch.py
      manager.py
    __init__.py
    client.py
    export_from_foundry.py
    folders.py
    icon_cache.py
    journals.py
    upload_journal_to_foundry.py
    xml_to_journal_html.py
  models/
    __init__.py
    journal.py
    xml_document.py
  pdf_processing/
    image_asset_processing/
      __init__.py
      detect_maps.py
      extract_map_assets.py
      extract_maps.py
      models.py
      preprocess_image.py
      segment_maps.py
      validate_segmentation.py
    __init__.py
    get_toc.py
    pdf_to_html.py
    pdf_to_xml.py
    split_pdf.py
    valid_xml_tags.py
    xml_to_html.py
  scene_extraction/
    __init__.py
    create_gallery.py
    extract_context.py
    generate_artwork.py
    identify_scenes.py
    models.py
  util/
    __init__.py
    gemini.py
    parallel_image_gen.py
  wall_detection/
    polygonize.py
    redline_walls.py
  api.py
  logging_config.py
tests/
  actors/
    __init__.py
    test_extract_npcs.py
    test_extract_stat_blocks.py
    test_integration_full_workflow.py
    test_models.py
    test_orchestrate_integration.py
    test_orchestrate.py
    test_parse_stat_blocks.py
    test_process_actors.py
  api/
    __init__.py
    test_api_integration.py
    test_api.py
  foundry/
    actors/
      __init__.py
      test_attack_save.py
      test_converter.py
      test_innate_spellcasting.py
      test_multiattack.py
      test_parser.py
      test_pit_fiend_integration.py
      test_roundtrip_integration.py
      test_spell_cache.py
    items/
      __init__.py
      test_deduplicate.py
      test_fetch.py
      test_manager.py
    __init__.py
    test_actors.py
    test_client.py
    test_icon_cache.py
    test_journals.py
    test_spell_via_give.py
    test_upload_journal.py
    test_upload_script.py
    test_xml_to_journal_html.py
  integration/
    __init__.py
    test_image_insertion_e2e.py
  models/
    __init__.py
    test_integration.py
    test_journal_image_positioning.py
    test_journal.py
    test_xml_document.py
    test_xml_to_html_workflow.py
  pdf_processing/
    image_asset_processing/
      __init__.py
      conftest.py
      test_detect_maps.py
      test_extract_map_assets.py
      test_extract_maps.py
      test_preprocess_image.py
      test_segment_maps.py
    __init__.py
    test_pdf_to_xml.py
    test_xml_to_html.py
  scene_extraction/
    __init__.py
    test_create_gallery.py
    test_extract_context.py
    test_full_workflow.py
    test_generate_artwork.py
    test_identify_scenes.py
    test_models.py
    test_real_api.py
    test_scene_metadata.py
  util/
    __init__.py
    test_gemini.py
  __init__.py
  conftest.py
ui/
  backend/
    app/
      models/
        __init__.py
        chat.py
        scene.py
      routers/
        __init__.py
        chat.py
      services/
        __init__.py
        command_parser.py
        gemini_service.py
      tools/
        __init__.py
        actor_creator.py
        base.py
        image_generator.py
        registry.py
      __init__.py
      config.py
      main.py
    tests/
      integration/
        __init__.py
        test_image_generation_flow.py
      routers/
        __init__.py
        test_chat.py
      services/
        __init__.py
        test_gemini_service.py
      tools/
        __init__.py
        test_base.py
        test_image_generator.py
        test_registry.py
      __init__.py
      test_chat_router.py
      test_command_parser.py
      test_config.py
      test_gemini_service.py
      test_main.py
      test_models.py
ui_prototypes/
  screenshot_html.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="dev/benchmark_segmentation.py">
"""Test segmentation reliability by running the same page 10 times."""
import asyncio
import io
import os
import fitz
from pathlib import Path
from dotenv import load_dotenv
from google import genai
from PIL import Image
import pytesseract

load_dotenv()

# Test cases directory (relative to project root when running as: python dev/benchmark_segmentation.py)
TEST_CASES_DIR = Path("dev_output/segmentation_benchmarks/test_cases")


def check_extraction_quality(extracted_path: str, reference_path: str) -> dict:
    """Check extraction quality against reference.

    Quality criteria:
    1. Dimensions within 20% of reference in both axes

    Note: Text content is calculated but not used for quality determination
    (word count validation happens during segmentation at 100 word threshold)

    Args:
        extracted_path: Path to extracted image
        reference_path: Path to reference good extraction

    Returns:
        Dict with quality metrics
    """
    if not os.path.exists(reference_path):
        return {"quality": "unknown", "reason": "No reference image"}

    # Load images
    extracted_img = Image.open(extracted_path)
    reference_img = Image.open(reference_path)

    ex_w, ex_h = extracted_img.size
    ref_w, ref_h = reference_img.size

    # Check dimensions (20% tolerance)
    width_diff_pct = abs(ex_w - ref_w) / ref_w * 100
    height_diff_pct = abs(ex_h - ref_h) / ref_h * 100

    dimension_ok = width_diff_pct <= 20 and height_diff_pct <= 20

    # Check text content using OCR (for reporting only, not quality determination)
    try:
        extracted_text = pytesseract.image_to_string(extracted_img)
        reference_text = pytesseract.image_to_string(reference_img)

        ex_text_len = len(extracted_text.strip())
        ref_text_len = len(reference_text.strip())

        text_ratio = ex_text_len / ref_text_len if ref_text_len > 0 else 0
    except Exception as e:
        # If OCR fails, just report as unknown
        text_ratio = 0
        ex_text_len = 0
        ref_text_len = 0

    # Quality is based ONLY on dimensions (text validation happens at 100 word threshold during segmentation)
    quality = "good" if dimension_ok else "bad"

    return {
        "quality": quality,
        "dimension_ok": dimension_ok,
        "width_diff_pct": width_diff_pct,
        "height_diff_pct": height_diff_pct,
        "text_ratio": text_ratio,
        "extracted_text_len": ex_text_len,
        "reference_text_len": ref_text_len,
        "reason": [] if quality == "good" else [
            f"Dimensions off by {max(width_diff_pct, height_diff_pct):.1f}%" if not dimension_ok else None
        ]
    }


def segment_single_attempt_sync(page_image: bytes, attempt_num: int, output_dir: Path, temp_dir: Path, reference_path: str, temperature: float = 0.5):
    """Segment a single attempt (synchronous)."""
    from src.pdf_processing.image_asset_processing.segment_maps import segment_with_imagen

    # Initial output path in temp/ (will be moved after quality check)
    temp_output_path = str(temp_dir / f"attempt_{attempt_num:02d}_temp.png")

    try:
        segment_with_imagen(page_image, "navigation_map", temp_output_path, temperature=temperature)

        # Check if file was created and has reasonable size
        if os.path.exists(temp_output_path):
            file_size = os.path.getsize(temp_output_path)
            img = Image.open(temp_output_path)
            width, height = img.size

            # Success criteria: file exists, reasonable size, reasonable dimensions
            # Map should be roughly 1000-3000px wide and 800-2500px tall
            # (excluding full page which would be ~1700x2200)
            is_success = (
                file_size > 50000 and  # At least 50KB
                500 < width < 3500 and
                400 < height < 2500 and
                not (width > 1600 and height > 2000)  # Not full page
            )

            # Check quality if extraction succeeded
            quality_info = None
            quality_suffix = "fail"
            if is_success and os.path.exists(reference_path):
                quality_info = check_extraction_quality(temp_output_path, reference_path)
                if quality_info and quality_info["quality"] == "good":
                    quality_suffix = "pass"

            # Move file from temp/ to main directory with pass/fail suffix
            final_output_path = str(output_dir / f"attempt_{attempt_num:02d}_{quality_suffix}.png")
            os.rename(temp_output_path, final_output_path)
            # temp files (_preprocessed.png and _with_red_perimeter.png) remain in temp/

            return {
                "attempt": attempt_num,
                "success": is_success,
                "width": width,
                "height": height,
                "file_size": file_size,
                "output_path": final_output_path,
                "quality": quality_info
            }
        else:
            return {
                "attempt": attempt_num,
                "success": False,
                "error": "File not created"
            }

    except Exception as e:
        return {
            "attempt": attempt_num,
            "success": False,
            "error": str(e)
        }


async def main(test_case: str = "cragmaw_hideout", temperature: float = 0.5):
    # Setup
    from datetime import datetime

    # Load test case
    test_case_dir = TEST_CASES_DIR / test_case
    if not test_case_dir.exists():
        raise ValueError(f"Test case '{test_case}' not found in {TEST_CASES_DIR}")

    page_image_path = test_case_dir / "page.png"
    reference_image_path = test_case_dir / "reference.png"

    if not page_image_path.exists():
        raise ValueError(f"page.png not found in test case '{test_case}'")
    if not reference_image_path.exists():
        raise ValueError(f"reference.png not found in test case '{test_case}'")

    # Load page image
    page_image = page_image_path.read_bytes()

    # Create output directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(f"dev_output/segmentation_benchmarks/runs/{test_case}_{timestamp}")
    output_dir.mkdir(parents=True, exist_ok=True)

    # Create temp subdirectory for intermediate files
    temp_dir = output_dir / "temp"
    temp_dir.mkdir(exist_ok=True)

    # Save preprocessed image once (shared across all attempts) in temp/
    from src.pdf_processing.image_asset_processing.preprocess_image import remove_existing_red_pixels
    preprocessed_image = remove_existing_red_pixels(page_image)
    preprocessed_path = temp_dir / "preprocessed.png"
    Image.open(io.BytesIO(preprocessed_image)).save(preprocessed_path)

    print("=" * 70)
    print("SEGMENTATION RELIABILITY TEST")
    print("=" * 70)
    print(f"Test Case: {test_case}")
    print(f"Temperature: {temperature}")
    print(f"Iterations: 10 (parallel using ThreadPoolExecutor)")
    print(f"Output: {output_dir}")
    print()

    # Run 10 attempts in parallel using thread pool
    print("Running 10 segmentation attempts in parallel...")
    from concurrent.futures import ThreadPoolExecutor
    import functools

    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor(max_workers=10) as executor:
        tasks = [
            loop.run_in_executor(
                executor,
                functools.partial(segment_single_attempt_sync, page_image, i + 1, output_dir, temp_dir, str(reference_image_path), temperature)
            )
            for i in range(10)
        ]
        results = await asyncio.gather(*tasks)

    # Analyze results
    print("\n" + "=" * 70)
    print("RESULTS")
    print("=" * 70)

    successes = []
    failures = []
    good_quality = []
    bad_quality = []

    for result in results:
        if result["success"]:
            successes.append(result)
            quality_info = result.get("quality")

            # Display result with quality
            quality_str = ""
            if quality_info:
                if quality_info["quality"] == "good":
                    quality_str = " [GOOD QUALITY]"
                    good_quality.append(result)
                else:
                    reasons = [r for r in quality_info["reason"] if r]
                    quality_str = f" [BAD QUALITY: {', '.join(reasons)}]"
                    bad_quality.append(result)

            print(f"‚úì Attempt {result['attempt']:2d}: SUCCESS - {result['width']}x{result['height']} ({result['file_size']:,} bytes){quality_str}")
        else:
            failures.append(result)
            error = result.get("error", "Unknown error")
            print(f"‚úó Attempt {result['attempt']:2d}: FAILED  - {error}")

    # Summary
    success_rate = len(successes) / len(results) * 100
    quality_rate = len(good_quality) / len(successes) * 100 if successes else 0

    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Total attempts:  {len(results)}")
    print(f"Successful:      {len(successes)}")
    print(f"Failed:          {len(failures)}")
    print(f"Success rate:    {success_rate:.1f}%")
    print()
    print(f"Quality tracking (of successful extractions):")
    print(f"  Good quality:  {len(good_quality)} ({quality_rate:.1f}%)")
    print(f"  Bad quality:   {len(bad_quality)}")
    if bad_quality:
        print(f"\n  Bad quality reasons:")
        for result in bad_quality:
            quality_info = result.get("quality", {})
            reasons = [r for r in quality_info.get("reason", []) if r]
            print(f"    Attempt {result['attempt']:2d}: {', '.join(reasons)}")

    if successes:
        avg_width = sum(r['width'] for r in successes) / len(successes)
        avg_height = sum(r['height'] for r in successes) / len(successes)
        avg_size = sum(r['file_size'] for r in successes) / len(successes)

        print(f"\nAverage dimensions (successful): {avg_width:.0f}x{avg_height:.0f}")
        print(f"Average file size (successful):  {avg_size:,.0f} bytes")

    # Show sample output paths
    if successes:
        print(f"\nSample successful extraction: {successes[0]['output_path']}")
    if failures and 'output_path' in failures[0]:
        print(f"Sample failed extraction:     {failures[0].get('output_path', 'N/A')}")

    print()

    # Return results for programmatic use
    return {
        "test_case": test_case,
        "temperature": temperature,
        "total": len(results),
        "successful": len(successes),
        "failed": len(failures),
        "success_rate": success_rate,
        "good_quality": len(good_quality),
        "bad_quality": len(bad_quality),
        "quality_rate": quality_rate
    }


if __name__ == "__main__":
    import sys

    # Parse command line arguments: [test_case] [temperature]
    # If no test case specified, run all test cases
    test_case = None  # None means run all
    temperature = 0.5  # default

    if len(sys.argv) > 1:
        test_case = sys.argv[1]

    if len(sys.argv) > 2:
        try:
            temperature = float(sys.argv[2])
            if temperature < 0 or temperature > 1:
                print("Error: Temperature must be between 0 and 1")
                sys.exit(1)
        except ValueError:
            print("Error: Temperature must be a number")
            sys.exit(1)

    # Get available test cases
    available_cases = [d.name for d in TEST_CASES_DIR.iterdir() if d.is_dir()]

    if test_case is None:
        # Run all test cases
        print("=" * 70)
        print("RUNNING ALL TEST CASES")
        print("=" * 70)
        print(f"Test cases: {', '.join(available_cases)}")
        print(f"Temperature: {temperature}")
        print()

        all_results = []
        for case in available_cases:
            result = asyncio.run(main(case, temperature))
            all_results.append(result)

        # Print combined summary
        print("\n\n" + "=" * 70)
        print("COMBINED SUMMARY (ALL TEST CASES)")
        print("=" * 70)
        total_successful = sum(r['successful'] for r in all_results)
        total_failed = sum(r['failed'] for r in all_results)
        total_good = sum(r['good_quality'] for r in all_results)
        total_bad = sum(r['bad_quality'] for r in all_results)
        total_attempts = total_successful + total_failed

        success_rate = (total_successful / total_attempts * 100) if total_attempts > 0 else 0
        quality_rate = (total_good / total_successful * 100) if total_successful > 0 else 0

        print(f"Total attempts:  {total_attempts}")
        print(f"Successful:      {total_successful} ({success_rate:.1f}%)")
        print(f"Failed:          {total_failed}")
        print()
        print(f"Quality (of successful extractions):")
        print(f"  Good quality:  {total_good} ({quality_rate:.1f}%)")
        print(f"  Bad quality:   {total_bad}")
        print()
    else:
        # Run single test case
        if test_case not in available_cases:
            print(f"Error: Test case '{test_case}' not found")
            print(f"Available test cases: {', '.join(available_cases)}")
            sys.exit(1)

        asyncio.run(main(test_case, temperature))
</file>

<file path="dev/compare_segmentation_methods.py">
#!/usr/bin/env python3
"""Test script to compare PyMuPDF extraction vs Imagen segmentation.

This script:
1. Extracts the map from page 1 using PyMuPDF
2. Extracts the same map using Gemini Imagen segmentation
3. Compares the results (dimensions, file sizes)
"""
import os
import sys
import fitz
import logging

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from pdf_processing.image_asset_processing.extract_maps import extract_image_with_pymupdf
from pdf_processing.image_asset_processing.segment_maps import segment_with_imagen, SegmentationError

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def main():
    pdf_path = "data/pdfs/Strongholds_Followers_extraction_test.pdf"
    output_dir = "dev_output/segmentation_experiments"
    os.makedirs(output_dir, exist_ok=True)

    # Open PDF and get first page
    doc = fitz.open(pdf_path)
    page = doc[0]

    print("=" * 70)
    print("Testing Map Extraction Comparison on Page 1")
    print("=" * 70)

    # Test 1: PyMuPDF Extraction
    print("\n[1/2] Testing PyMuPDF extraction...")
    pymupdf_output = os.path.join(output_dir, "page1_pymupdf.png")
    pymupdf_success = extract_image_with_pymupdf(page, pymupdf_output)

    if pymupdf_success:
        file_size = os.path.getsize(pymupdf_output)
        from PIL import Image
        img = Image.open(pymupdf_output)
        print(f"  ‚úì PyMuPDF extraction succeeded")
        print(f"    - Dimensions: {img.width}x{img.height}")
        print(f"    - File size: {file_size:,} bytes")
        print(f"    - Saved to: {pymupdf_output}")
    else:
        print(f"  ‚úó PyMuPDF extraction failed (no large images found)")

    # Test 2: Imagen Segmentation
    print("\n[2/2] Testing Gemini Imagen segmentation...")
    imagen_output = os.path.join(output_dir, "page1_imagen_segmented.png")

    try:
        # Render page to image for Imagen
        pix = page.get_pixmap(dpi=150)
        page_image_bytes = pix.pil_tobytes(format="PNG")

        segment_with_imagen(page_image_bytes, "navigation_map", imagen_output)

        file_size = os.path.getsize(imagen_output)
        from PIL import Image
        img = Image.open(imagen_output)
        print(f"  ‚úì Imagen segmentation succeeded")
        print(f"    - Dimensions: {img.width}x{img.height}")
        print(f"    - File size: {file_size:,} bytes")
        print(f"    - Saved to: {imagen_output}")

    except SegmentationError as e:
        print(f"  ‚úó Imagen segmentation failed: {e}")
    except Exception as e:
        print(f"  ‚úó Imagen segmentation error: {e}")

    doc.close()

    # Comparison
    print("\n" + "=" * 70)
    if pymupdf_success and os.path.exists(imagen_output):
        print("COMPARISON RESULTS:")
        print("  Both methods succeeded!")
        print(f"\n  Compare the images visually:")
        print(f"  - PyMuPDF:  {pymupdf_output}")
        print(f"  - Imagen:   {imagen_output}")
        print("\n  Open both images to verify they extracted the same map.")
    elif pymupdf_success:
        print("RESULT: Only PyMuPDF succeeded (expected if Imagen doesn't work)")
    elif os.path.exists(imagen_output):
        print("RESULT: Only Imagen succeeded (PyMuPDF couldn't find embedded image)")
    else:
        print("RESULT: Both methods failed")

    print("=" * 70)

if __name__ == "__main__":
    main()
</file>

<file path="dev/create_test_cases.py">
"""Create test input/output pairs for PyMuPDF extraction."""
import asyncio
import fitz
import os
from pathlib import Path
from dotenv import load_dotenv
from google import genai
from src.pdf_processing.image_asset_processing.extract_maps import extract_image_with_pymupdf_async

load_dotenv()

async def create_test_case(pdf_path: str, page_num: int, case_name: str):
    """Create input/output pair for a test case."""
    input_dir = Path("dev/tests_assets/pdf_processing/image_asset_processing/test_extract_maps/input_images")
    output_dir = Path("dev/tests_assets/pdf_processing/image_asset_processing/test_extract_maps/output_images")

    doc = fitz.open(pdf_path)
    page = doc[page_num]

    # Save full page as input
    pix = page.get_pixmap(dpi=150)
    input_path = input_dir / f"{case_name}_page{page_num + 1}.png"
    pix.save(str(input_path))
    print(f"‚úì Saved input: {input_path.name}")

    # Extract map using PyMuPDF + AI classification
    output_path = output_dir / f"{case_name}_page{page_num + 1}_extracted.png"
    result = await extract_image_with_pymupdf_async(page, str(output_path), use_ai_classification=True)

    if result:
        print(f"‚úì Saved output: {output_path.name}")
        # Get dimensions
        from PIL import Image
        img = Image.open(output_path)
        print(f"  Extracted map: {img.width}x{img.height}")
    else:
        print(f"‚úó No map found on page {page_num + 1}")

    doc.close()
    return result

async def main():
    pdf_path = "data/pdfs/Strongholds_Followers_extraction_test.pdf"

    # Create test cases for pages 1, 2, and 6
    test_cases = [
        (0, "test_case_1"),  # Page 1
        (1, "test_case_2"),  # Page 2
        (5, "test_case_3"),  # Page 6
    ]

    print("Creating test cases...\n")
    for page_num, case_name in test_cases:
        print(f"{'='*60}")
        print(f"Test Case: {case_name} (Page {page_num + 1})")
        print(f"{'='*60}")
        await create_test_case(pdf_path, page_num, case_name)
        print()

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="dev/debug_segmentation.py">
#!/usr/bin/env python3
"""Debug tool to compare segmentation extraction against expected output.

Runs the segmentation process on an input image and compares the result
to the expected output image.
"""
import os
import sys
import logging
from PIL import Image
import numpy as np

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from pdf_processing.image_asset_processing.segment_maps import segment_with_imagen, SegmentationError

logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def compare_images(actual_path: str, expected_path: str):
    """Compare two images and report differences."""
    actual = Image.open(actual_path)
    expected = Image.open(expected_path)

    print("\n" + "=" * 70)
    print("IMAGE COMPARISON")
    print("=" * 70)

    print(f"\nActual (segmented):   {actual.width}x{actual.height} ({actual.mode})")
    print(f"Expected (reference): {expected.width}x{expected.height} ({expected.mode})")

    # Dimension comparison
    width_diff = abs(actual.width - expected.width)
    height_diff = abs(actual.height - expected.height)

    if actual.size == expected.size:
        print("‚úì Dimensions match exactly")
    else:
        print(f"‚úó Dimension difference: {width_diff}px width, {height_diff}px height")

    # Pixel similarity (if same size)
    if actual.size == expected.size and actual.mode == expected.mode:
        actual_array = np.array(actual)
        expected_array = np.array(expected)

        # Calculate pixel-wise difference
        diff = np.abs(actual_array.astype(int) - expected_array.astype(int))
        mean_diff = np.mean(diff)
        max_diff = np.max(diff)

        print(f"\nPixel differences:")
        print(f"  Mean: {mean_diff:.2f}")
        print(f"  Max:  {max_diff}")

        # Similarity percentage (0 = identical, 255 = completely different)
        similarity = 100 * (1 - mean_diff / 255)
        print(f"  Similarity: {similarity:.1f}%")

        if similarity > 95:
            print("‚úì Images are very similar")
        elif similarity > 80:
            print("‚ö† Images are similar but have noticeable differences")
        else:
            print("‚úó Images are significantly different")

    print("=" * 70)

def main():
    input_name = "example_keep.png"
    test_dir = "dev/tests_assets/pdf_processing/image_asset_processing/test_segment_maps"

    input_path = os.path.join(test_dir, "input_images", input_name)
    expected_path = os.path.join(test_dir, "output_images", input_name)
    actual_path = "dev_output/debug_segmentation/actual_output.png"

    # Create output directory
    os.makedirs("dev_output/debug_segmentation", exist_ok=True)

    print("=" * 70)
    print("SEGMENTATION DEBUGGING TOOL")
    print("=" * 70)
    print(f"\nInput:    {input_path}")
    print(f"Expected: {expected_path}")
    print(f"Output:   {actual_path}")

    # Load input image
    print("\n[1/3] Loading input image...")
    input_img = Image.open(input_path)
    print(f"  Loaded: {input_img.width}x{input_img.height} ({input_img.mode})")

    # Convert to bytes for segmentation
    import io
    img_buffer = io.BytesIO()
    input_img.save(img_buffer, format='PNG')
    input_bytes = img_buffer.getvalue()

    # Run segmentation
    print("\n[2/3] Running Gemini segmentation...")
    try:
        segment_with_imagen(input_bytes, "navigation_map", actual_path)
        print(f"  ‚úì Segmentation succeeded")
    except SegmentationError as e:
        print(f"  ‚úó Segmentation failed: {e}")
        return 1
    except Exception as e:
        print(f"  ‚úó Unexpected error: {e}")
        return 1

    # Compare results
    print("\n[3/3] Comparing with expected output...")
    compare_images(actual_path, expected_path)

    print(f"\nüìÅ Files saved to dev_output/debug_segmentation/")
    print(f"   - actual_output.png (segmented result)")
    print(f"   - actual_output_with_red_perimeter.png (debug)")
    print(f"\nüí° Open both images to visually compare:")
    print(f"   Expected: {expected_path}")
    print(f"   Actual:   {actual_path}")

    return 0

if __name__ == "__main__":
    exit(main())
</file>

<file path="dev/sweep_temperature_params.py">
"""Test segmentation reliability across different temperature values."""
import asyncio
import sys
from pathlib import Path
from benchmark_segmentation import main as run_single_test, TEST_CASES_DIR


async def main():
    """Run tests for temperatures from 0.0 to 1.0 in 0.1 increments."""
    # Get available test cases
    test_cases = [d.name for d in TEST_CASES_DIR.iterdir() if d.is_dir()]

    print("=" * 70)
    print("TEMPERATURE SWEEP TEST")
    print("=" * 70)
    print(f"Test cases: {', '.join(test_cases)}")
    print("Testing temperatures: 0.0, 0.1, 0.2, ..., 1.0")
    print("10 attempts per temperature per test case")
    print()

    temperatures = [round(t * 0.1, 1) for t in range(11)]  # 0.0 to 1.0 in 0.1 increments

    # Store results by test case
    all_results = {test_case: [] for test_case in test_cases}

    for test_case in test_cases:
        print(f"\n{'=' * 70}")
        print(f"TEST CASE: {test_case}")
        print(f"{'=' * 70}")

        for temp in temperatures:
            print(f"\nTesting temperature: {temp}")

            try:
                result = await run_single_test(test_case=test_case, temperature=temp)
                all_results[test_case].append(result)
            except Exception as e:
                print(f"Error testing temperature {temp}: {e}")
                all_results[test_case].append({
                    "test_case": test_case,
                    "temperature": temp,
                    "error": str(e)
                })

    # Print individual test case summaries
    for test_case in test_cases:
        print("\n\n" + "=" * 70)
        print(f"SUMMARY: {test_case.upper()}")
        print("=" * 70)
        print()
        print(f"{'Temp':>6} | {'Success':>7} | {'Failed':>6} | {'Success %':>10} | {'Good':>4} | {'Bad':>4} | {'Quality %':>10}")
        print("-" * 70)

        results = all_results[test_case]
        for result in results:
            if "error" in result:
                print(f"{result['temperature']:>6.1f} | ERROR: {result['error']}")
            else:
                temp = result['temperature']
                successful = result['successful']
                failed = result['failed']
                success_rate = result['success_rate']
                good = result['good_quality']
                bad = result['bad_quality']
                quality_rate = result['quality_rate']

                print(f"{temp:>6.1f} | {successful:>7} | {failed:>6} | {success_rate:>9.1f}% | {good:>4} | {bad:>4} | {quality_rate:>9.1f}%")

        print()

        # Find best temperature for this test case
        valid_results = [r for r in results if "error" not in r and r['successful'] > 0]

        if valid_results:
            best_quality = max(valid_results, key=lambda r: r['quality_rate'])
            best_success = max(valid_results, key=lambda r: r['success_rate'])

            print(f"Best quality rate:  Temperature {best_quality['temperature']} ({best_quality['quality_rate']:.1f}%)")
            print(f"Best success rate:  Temperature {best_success['temperature']} ({best_success['success_rate']:.1f}%)")

    # Print combined summary
    print("\n\n" + "=" * 70)
    print("COMBINED SUMMARY (ALL TEST CASES)")
    print("=" * 70)
    print()
    print(f"{'Temp':>6} | {'Success':>7} | {'Failed':>6} | {'Success %':>10} | {'Good':>4} | {'Bad':>4} | {'Quality %':>10}")
    print("-" * 70)

    for temp in temperatures:
        # Aggregate across all test cases for this temperature
        total_successful = 0
        total_failed = 0
        total_good = 0
        total_bad = 0
        has_error = False

        for test_case in test_cases:
            results = all_results[test_case]
            result = next((r for r in results if r.get('temperature') == temp), None)
            if result and "error" not in result:
                total_successful += result['successful']
                total_failed += result['failed']
                total_good += result['good_quality']
                total_bad += result['bad_quality']
            elif result:
                has_error = True

        if has_error:
            print(f"{temp:>6.1f} | ERROR")
        else:
            total_attempts = total_successful + total_failed
            success_rate = (total_successful / total_attempts * 100) if total_attempts > 0 else 0
            quality_rate = (total_good / total_successful * 100) if total_successful > 0 else 0

            print(f"{temp:>6.1f} | {total_successful:>7} | {total_failed:>6} | {success_rate:>9.1f}% | {total_good:>4} | {total_bad:>4} | {quality_rate:>9.1f}%")

    print()

    # Find best overall temperature
    combined_results = []
    for temp in temperatures:
        total_successful = 0
        total_failed = 0
        total_good = 0
        total_bad = 0

        for test_case in test_cases:
            results = all_results[test_case]
            result = next((r for r in results if r.get('temperature') == temp), None)
            if result and "error" not in result:
                total_successful += result['successful']
                total_failed += result['failed']
                total_good += result['good_quality']
                total_bad += result['bad_quality']

        if total_successful > 0:
            total_attempts = total_successful + total_failed
            combined_results.append({
                'temperature': temp,
                'success_rate': (total_successful / total_attempts * 100),
                'quality_rate': (total_good / total_successful * 100)
            })

    if combined_results:
        best_quality = max(combined_results, key=lambda r: r['quality_rate'])
        best_success = max(combined_results, key=lambda r: r['success_rate'])

        print(f"Best combined quality rate:  Temperature {best_quality['temperature']} ({best_quality['quality_rate']:.1f}%)")
        print(f"Best combined success rate:  Temperature {best_success['temperature']} ({best_success['success_rate']:.1f}%)")
        print()


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="dev/verify_ai_classification.py">
"""Test AI classification on PyMuPDF extraction candidates."""
import asyncio
import fitz
import os
from pathlib import Path
from dotenv import load_dotenv
from google import genai
from src.pdf_processing.image_asset_processing.detect_maps import is_map_image_async

load_dotenv()

MIN_IMAGE_SIZE = 200
PAGE_AREA_THRESHOLD = 0.10

async def main():
    pdf_path = "data/pdfs/Strongholds_Followers_extraction_test.pdf"
    output_dir = Path("dev/tests_assets/pdf_processing/image_asset_processing/test_extract_maps/extracted_images")

    doc = fitz.open(pdf_path)
    page = doc[0]

    # Calculate page area threshold
    page_area = page.rect.width * page.rect.height
    area_threshold = page_area * PAGE_AREA_THRESHOLD

    print(f"Page area: {page_area:.0f} px¬≤")
    print(f"Area threshold (10%): {area_threshold:.0f} px¬≤\n")

    # Extract all images meeting size thresholds
    images = page.get_images()
    candidates = []

    for idx, img_ref in enumerate(images):
        xref = img_ref[0]
        try:
            img_info = doc.extract_image(xref)
            img_width = img_info['width']
            img_height = img_info['height']
            img_area = img_width * img_height

            # Filter 1: Must be above minimum size
            if img_width < MIN_IMAGE_SIZE or img_height < MIN_IMAGE_SIZE:
                continue

            # Filter 2: Must occupy enough page area
            if img_area > area_threshold:
                candidates.append((idx + 1, img_info, img_area, img_width, img_height))
                print(f"Candidate {len(candidates)}: Image {idx + 1}")
                print(f"  Size: {img_width}x{img_height}")
                print(f"  Area: {img_area:,} px¬≤ ({100*img_area/page_area:.1f}% of page)")

                # Save candidate image
                img_path = output_dir / f"candidate_{len(candidates)}_image{idx+1}_{img_width}x{img_height}.{img_info['ext']}"
                with open(img_path, "wb") as f:
                    f.write(img_info['image'])
                print(f"  Saved: {img_path.name}\n")
        except Exception as e:
            print(f"Failed to extract image {idx + 1}: {e}")

    doc.close()

    # Classify all candidates with AI
    api_key = os.getenv("GeminiImageAPI")
    client = genai.Client(api_key=api_key)

    print(f"Classifying {len(candidates)} candidates with Gemini Vision...\n")

    classification_tasks = [
        is_map_image_async(client, img_info['image'], width, height)
        for _, img_info, _, width, height in candidates
    ]
    results = await asyncio.gather(*classification_tasks)

    # Report results
    print("=" * 60)
    print("CLASSIFICATION RESULTS")
    print("=" * 60)
    for (img_num, img_info, img_area, width, height), is_map in zip(candidates, results):
        status = "‚úì IS A MAP" if is_map else "‚úó NOT A MAP"
        print(f"\nImage {img_num}: {width}x{height} - {status}")
        if is_map:
            # Save the winning map
            map_path = output_dir / f"WINNER_image{img_num}_{width}x{height}.{img_info['ext']}"
            with open(map_path, "wb") as f:
                f.write(img_info['image'])
            print(f"  Saved as: {map_path.name}")

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="relay-server/convert-postman-to-md.py">
# Run with full path to postman collection file as a single parameter, in Postman 2.1 format
# See "Postman to Wiki" at https://medium.com/@sreedharc/postman-to-wiki-e7d31c76db57 for more information
# Generates Markdown for Confluence Wiki from a given Collection file exported from Postman per Collections 2.1 schema

import json
import sys
import os
import re
from operator import itemgetter
from itertools import groupby

input_file = sys.argv[1]
working_dir = os.path.dirname(input_file)
serial_num = 1
output_dir = "generated"


def clean(s):
    # Python 3 replacement for string_escape decoding
    s = s.encode('latin1').decode('unicode_escape')
    s = s.replace('{{', '$').replace('}}', '').replace('\r\n', '')
    if s.startswith('"') and s.endswith('"'):
        return s[1:-1]
    return s


def clean_list(a_list):
    return [clean(i) for i in str(a_list).split(',')]


def escape_markup(value):
    return re.sub(r"([{\}\[\]])", r"\\\1", value)


def encode_full_url(url):
    return (
        re.sub(r"\s+", " ", url).replace("{{", "$").replace("}}", "")
            .replace("%", "%25").replace("'", "%27")
            .replace("{", "%7B").replace("}", "%7D")
            .replace("\"", "%22").replace(" ", "%20")
            .replace("[", "%5B").replace("]", "%5D")
    )


def format_raw_data(data):
    try:
        return 'json', json.dumps(json.loads(data), indent=2)
    except:
        return 'xml', data


# Generates markdown for the section describing Collection Variables
def gen_variables_section(list_of_variables):
    op = "## Variables Used in this Collection\n\n"
    op += "| Name | Description | Example |\n"
    op += "| ---- | ----------- | ------- |\n"
    for variable in sorted(list_of_variables, key=itemgetter('key')):
        op += f"| {variable['key']} | | {variable['value']} |\n"
    return op


# Generates markdown for request section
def gen_request_section(item):
    item = json.loads(json.dumps(item))
    # Heading, e.g. GET Users
    request_section = f"## **{item['request']['method']}** {item['name']}\n\n"

    # Description
    if "description" in item["request"]:
        request_section += f"{item['request']['description']}\n\n"

    request_section += "### Request\n\n"
    # URL format
    request_section += "#### Request URL\n\n"
    request_section += f"```\n{encode_full_url(item['request']['url']['raw'])}\n```\n\n"

    # Headers
    request_section += "#### Request Headers\n\n"
    request_section += "| Key | Value | Description |\n"
    request_section += "| --- | ----- | ----------- |\n"
    for header in item["request"]["header"]:
        description = header["description"] if "description" in header else " "
        request_section += f"| {header['key']} | {escape_markup(header['value'])} | {description} |\n"
    request_section += "\n"

    # Request Parameters
    add_param_section = False
    param_section = "#### Request Parameters\n\n"
    param_section += "| Parameter Type | Key | Value | Description |\n"
    param_section += "| -------------- | --- | ----- | ----------- |\n"

    url = item["request"]["url"]
    # Path Parameters
    if "path" in url:
        path_parameters = url["path"]
        for param in path_parameters:
            if "{{" in param:
                add_param_section = True
                param_section += f"| Path Parameter | {param.replace('{{', '$').replace('}}', '')} | | |\n"
    # Query String Parameters
    if "query" in url:
        qs_parameters = url["query"]
        for param in qs_parameters:
            add_param_section = True
            description = param["description"] if "description" in param else " "
            param_section += f"| Query String Parameter | {param['key']} | {escape_markup(param['value'])} | {description} |\n"

    print(param_section)

    if add_param_section:
        print("ADD PARAMETERS SECTION")
        request_section += param_section + "\n"

    if str(item["request"]["method"]) != "GET":
        request_section += "#### Request Payload\n\n"
        language = "json"
        # print(item["request"]["body"])
        if (not("body" in item["request"]) or item["request"]["body"] is None):
            # print("No Payload")
            payload = "{}"
        else:
            # print("CLEANED UP")
            if "raw" in item["request"]["body"]:
                try:
                    p = json.dumps(item["request"]["body"]["raw"])
                    x = clean(json.dumps(json.loads(p)))
                    # print(x)
                    if not x:
                        payload = "{}"
                    else:
                        language, data = format_raw_data(x)
                        payload = data
                except:
                    payload = item["request"]["body"]["raw"]
            else:
                payload = "{}"
        # print("PAYLOAD: \n" + payload)
        # Request payload as code snippet
        request_section += f"```{language}\n{payload}\n```\n\n"
    else:
        print("No payload for GET")

    return request_section


# Generates markdown for response section
def gen_response_section(item):
    response = json.loads(json.dumps(item["response"]))
    response_section = "### Response\n\n"
    # print(item["name"] + " - " + str(len(response)))
    if len(response) > 0:
        code = "200"
        if "code" in response[0]:
            code = str(response[0]["code"])
        else:
            print(response[0])

        status = "OK"
        if "status" in response[0]:
            status = str(response[0]["status"])
        else:
            print(response[0])

        response_section += f"#### Status: {code} {status}\n\n"

        language = "json"
        if "body" in response[0]:
            body = response[0]["body"]
            if body is None:
                body = "{}"
            # print("BODY: \n" + body)
            language, body = format_raw_data(body)
        else:
            body = " "
            print("Warning - response body is missing")

        # Response body as code snippet
        response_section += f"```{language}\n{body}\n```\n\n"
    return response_section


# Generates a section for the given resource
def gen_resource_section(name, group):
    global serial_num
    global working_dir
    global output_dir
    resource_sections = ""
    if not os.path.exists(os.path.join(working_dir, output_dir)):
        os.mkdir(os.path.join(working_dir, output_dir))
    for item in group:
        print("\t Processing " + str(item["request"]["method"]))
        resource_section = gen_request_section(item) + gen_response_section(item) + "\n"
        # Write to a separate output file
        # Sanitize the name by replacing invalid filename characters
        safe_name = name.replace("/", "").replace("\\", "-").replace(":", "-").replace("*", "-") \
                       .replace("?", "-").replace("\"", "-").replace("<", "-").replace(">", "-") \
                       .replace("|", "-")
        file_name = safe_name + "-" + str(item["request"]["method"]) + ".md"
        output_path = os.path.join(working_dir, output_dir, file_name)
        # Specify UTF-8 encoding when writing the file
        with open(output_path, 'w', encoding='utf-8') as output_file:
            output_file.write(resource_section)
        serial_num = serial_num + 1
        resource_sections += resource_section
    return resource_sections


# Read in the input as JSON
with open(input_file, encoding="utf-8") as json_data:  # Specify UTF-8 encoding
    input_data = json.load(json_data)
    # Extract all variables and print them out as a table
    v = gen_variables_section(input_data["variable"])
    
    # Now extract all requests ("items" in the collection) sorted by resource name ("name" in the collection)...
    items = sorted(input_data["item"], key=lambda x: (x['name'], x['request']['method']))
    
    # ... and generate documentation for supported methods by resource, in alphabetical order (GET, PATCH, POST, etc.)
    resource_full = "# API Documentation\n\n"
    resource_full += v + "\n"
    
    for name, group in groupby(items, key=lambda y: (y['name'])):
        print("Exporting: " + name)
        resource_full += gen_resource_section(name, group)
    
    # with open(os.path.join(working_dir, output_dir, "full.md"), 'w') as output_file:
    #     output_file.write(resource_full)
</file>

<file path="scripts/_deprecated/deduplicate_spells.py">
#!/usr/bin/env python3
"""
Deduplicate spells by name, prioritizing official sources.

Priority order:
1. Player's Handbook (dnd-players-handbook)
2. D&D 5e 2024 rules (dnd5e.spells24)
3. D&D 5e SRD (dnd5e.spells)
4. Other sources
"""

import json
from pathlib import Path
from typing import Dict, List
from collections import defaultdict


def get_source_priority(uuid: str) -> int:
    """
    Get priority score for a spell's source (lower = higher priority).

    Args:
        uuid: Spell UUID like "Compendium.dnd5e.spells.abc123"

    Returns:
        Priority score (0 = highest priority)
    """
    if 'dnd-players-handbook' in uuid:
        return 0  # Highest priority
    elif 'dnd5e.spells24' in uuid:
        return 1  # 2024 rules
    elif 'dnd5e.spells' in uuid:
        return 2  # Classic SRD
    else:
        return 3  # Other sources (homebrew, modules, etc.)


def deduplicate_spells(spells: List[Dict]) -> List[Dict]:
    """
    Deduplicate spells by name, keeping highest priority source.

    Args:
        spells: List of spell dicts with 'name' and 'uuid' fields

    Returns:
        Deduplicated list of spells
    """
    # Group by name
    spells_by_name = defaultdict(list)
    for spell in spells:
        name = spell.get('name', '').strip()
        if name:
            spells_by_name[name].append(spell)

    # For each name, pick the highest priority source
    deduplicated = []
    for name, spell_variants in spells_by_name.items():
        # Sort by priority (lower score = higher priority)
        spell_variants_sorted = sorted(
            spell_variants,
            key=lambda s: get_source_priority(s.get('uuid', ''))
        )

        # Take the first one (highest priority)
        best_spell = spell_variants_sorted[0]
        deduplicated.append(best_spell)

        # Log if we had duplicates
        if len(spell_variants) > 1:
            sources = [s.get('uuid', 'unknown').split('.')[1] if '.' in s.get('uuid', '') else 'unknown'
                      for s in spell_variants]
            print(f"  {name}: kept {sources[0]}, removed {', '.join(sources[1:])}")

    # Sort by name
    deduplicated_sorted = sorted(deduplicated, key=lambda s: s.get('name', ''))

    return deduplicated_sorted


def main():
    # Load all spells
    input_file = Path(__file__).parent.parent / 'data' / 'foundry_examples' / 'all_spells.json'
    output_file = Path(__file__).parent.parent / 'data' / 'foundry_examples' / 'spells_deduplicated.json'

    if not input_file.exists():
        print(f"Error: Input file not found: {input_file}")
        return

    print(f"Loading spells from {input_file.name}...\n")
    with open(input_file) as f:
        all_spells = json.load(f)

    print(f"Total spells before deduplication: {len(all_spells)}\n")

    # Count by source
    sources = defaultdict(int)
    for spell in all_spells:
        uuid = spell.get('uuid', '')
        if 'dnd-players-handbook' in uuid:
            sources['Player\'s Handbook'] += 1
        elif 'dnd5e.spells24' in uuid:
            sources['D&D 5e 2024'] += 1
        elif 'dnd5e.spells' in uuid:
            sources['D&D 5e SRD'] += 1
        else:
            sources['Other'] += 1

    print("Sources before deduplication:")
    for source, count in sorted(sources.items()):
        print(f"  {source}: {count}")

    print("\nDeduplicating...\n")
    deduplicated = deduplicate_spells(all_spells)

    print(f"\n{'='*60}")
    print(f"Total unique spell names: {len(deduplicated)}")
    print(f"Removed {len(all_spells) - len(deduplicated)} duplicates")
    print(f"{'='*60}\n")

    # Count by source after deduplication
    sources_after = defaultdict(int)
    for spell in deduplicated:
        uuid = spell.get('uuid', '')
        if 'dnd-players-handbook' in uuid:
            sources_after['Player\'s Handbook'] += 1
        elif 'dnd5e.spells24' in uuid:
            sources_after['D&D 5e 2024'] += 1
        elif 'dnd5e.spells' in uuid:
            sources_after['D&D 5e SRD'] += 1
        else:
            sources_after['Other'] += 1

    print("Sources after deduplication:")
    for source, count in sorted(sources_after.items()):
        print(f"  {source}: {count}")

    # Save deduplicated list
    with open(output_file, 'w') as f:
        json.dump(deduplicated, f, indent=2)

    print(f"\n‚úì Saved {len(deduplicated)} deduplicated spells to {output_file.name}")

    # Show first 20
    print(f"\n{'Name':<40} {'Source':<25} {'UUID'}")
    print("-" * 120)
    for spell in deduplicated[:20]:
        uuid = spell.get('uuid', 'UNKNOWN')
        source = uuid.split('.')[1] if '.' in uuid else 'unknown'
        print(f"{spell.get('name', 'UNKNOWN'):<40} {source:<25} {uuid}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/_deprecated/fetch_all_spells.py">
#!/usr/bin/env python3
"""
Fetch all spells from FoundryVTT via the REST API.

Uses alphabet-based querying to work around the 200-result search limit.
Deduplicates by UUID and saves to JSON file.
"""

import sys
import os
from pathlib import Path
from dotenv import load_dotenv
import requests
import json
from string import ascii_lowercase

# Load environment variables
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(env_path)

def fetch_all_spells(output_file: str = None) -> list[dict]:
    """
    Fetch all spells from FoundryVTT using alphabet strategy.

    For letters that hit the 200-result limit, uses two-letter combinations.

    Returns:
        List of spell dicts with name, uuid, and other metadata
    """
    relay_url = os.getenv("FOUNDRY_RELAY_URL")
    api_key = os.getenv("FOUNDRY_LOCAL_API_KEY")
    client_id = os.getenv("FOUNDRY_LOCAL_CLIENT_ID")

    if not all([relay_url, api_key, client_id]):
        raise ValueError("Missing required environment variables (FOUNDRY_RELAY_URL, FOUNDRY_LOCAL_API_KEY, FOUNDRY_LOCAL_CLIENT_ID)")

    print("Fetching all spells using alphabet strategy...\n")

    all_spells = {}  # Use dict to deduplicate by UUID
    letters_at_limit = []  # Track letters that hit 200

    def search_query(query: str) -> list:
        """Helper to search with a query string."""
        url = f"{relay_url}/search"
        headers = {"x-api-key": api_key}
        params = {
            "clientId": client_id,
            "filter": "documentType:Item,subType:spell",
            "query": query
        }

        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        return data.get('results', data) if isinstance(data, dict) else data

    # Query with each letter
    for letter in ascii_lowercase:
        try:
            results = search_query(letter)

            # Deduplicate by UUID
            for spell in results:
                uuid = spell.get('uuid')
                if uuid:
                    all_spells[uuid] = spell

            print(f"Letter '{letter}': {len(results)} results (total unique: {len(all_spells)})")

            # If we hit the limit, we need to drill down with two-letter combos
            if len(results) == 200:
                letters_at_limit.append(letter)

        except Exception as e:
            print(f"Error fetching '{letter}': {e}")

    # For letters that hit 200, query with two-letter combinations
    if letters_at_limit:
        print(f"\n‚ö† Letters at 200 limit: {', '.join(letters_at_limit)}")
        print("Querying with two-letter combinations...\n")

        for letter in letters_at_limit:
            for second in ascii_lowercase:
                query = f"{letter}{second}"
                try:
                    results = search_query(query)

                    for spell in results:
                        uuid = spell.get('uuid')
                        if uuid:
                            all_spells[uuid] = spell

                    if results:
                        print(f"  '{query}': {len(results)} results (total unique: {len(all_spells)})")

                    # If even two-letter combo hits 200, warn
                    if len(results) == 200:
                        print(f"    ‚ö† WARNING: '{query}' hit 200 limit - may need 3-letter combos!")

                except Exception as e:
                    print(f"  Error fetching '{query}': {e}")

    # Also try empty query for spells that don't match letters
    try:
        results = search_query("")
        for spell in results:
            uuid = spell.get('uuid')
            if uuid:
                all_spells[uuid] = spell
        print(f"\nEmpty query: {len(results)} results (total unique: {len(all_spells)})")
    except Exception as e:
        print(f"Error with empty query: {e}")

    spells_list = list(all_spells.values())
    spells_sorted = sorted(spells_list, key=lambda s: s.get('name', ''))

    print(f"\n{'='*60}")
    print(f"Total unique spells found: {len(spells_sorted)}")
    print(f"{'='*60}\n")

    # Save to file if specified
    if output_file:
        with open(output_file, 'w') as f:
            json.dump(spells_sorted, f, indent=2)
        print(f"‚úì Saved {len(spells_sorted)} spells to {output_file}")

    return spells_sorted


def main():
    # Default output location
    output_file = Path(__file__).parent.parent / 'data' / 'foundry_examples' / 'all_spells.json'

    spells = fetch_all_spells(str(output_file))

    # Show first 20
    print(f"\n{'Name':<40} {'UUID'}")
    print("-" * 120)
    for spell in spells[:20]:
        print(f"{spell.get('name', 'UNKNOWN'):<40} {spell.get('uuid', 'UNKNOWN')}")

    print(f"\nUUID format: Compendium.dnd5e.spells.{{id}}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/_deprecated/read_spells_compendium.py">
#!/usr/bin/env python3
"""
Read FoundryVTT spells compendium from LevelDB and extract spell UUIDs and metadata.

This script directly parses the LevelDB .ldb file to extract JSON spell data.
"""

import json
import re
import sys
from pathlib import Path


def extract_spells_from_ldb(ldb_file: Path) -> list[dict]:
    """
    Parse LevelDB .ldb file and extract spell JSON objects.

    LevelDB stores records with keys like "!items!{id}" followed by JSON data.
    We scan for these patterns and extract complete JSON objects.
    """
    with open(ldb_file, 'rb') as f:
        data = f.read()

    results = []

    # Work with binary data to find JSON patterns
    marker = b'!items!'
    pos = 0

    while True:
        # Find next item marker
        pos = data.find(marker, pos)
        if pos == -1:
            break

        # Skip past the marker and ID
        pos += len(marker)

        # Find the opening brace of JSON (skip the ID bytes)
        json_start = data.find(b'{', pos)
        if json_start == -1 or json_start - pos > 100:  # ID should be short
            continue

        # Extract a large chunk that should contain the full JSON
        chunk = data[json_start:json_start + 50000]

        # Find balanced JSON in binary
        brace_count = 0
        json_end = -1
        in_string = False
        escape_next = False
        i = 0

        while i < len(chunk):
            byte = chunk[i:i+1]

            # Only process printable ASCII and common JSON characters
            if byte == b'\\' and not escape_next:
                escape_next = True
                i += 1
                continue

            if escape_next:
                escape_next = False
                i += 1
                continue

            if byte == b'"' and not escape_next:
                in_string = not in_string
            elif not in_string:
                if byte == b'{':
                    brace_count += 1
                elif byte == b'}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_end = i + 1
                        break

            i += 1

        if json_end > 0:
            json_bytes = chunk[:json_end]
            try:
                # Decode JSON bytes, ignoring binary noise
                json_str = json_bytes.decode('utf-8', errors='ignore')

                # Remove any remaining control characters (except \t, \n, \r)
                json_str = ''.join(char if ord(char) >= 32 or char in '\t\n\r' else ' ' for char in json_str)

                obj = json.loads(json_str)

                # Filter for spells only
                if isinstance(obj, dict) and obj.get('type') == 'spell' and 'name' in obj:
                    results.append(obj)
            except (json.JSONDecodeError, UnicodeDecodeError):
                # Skip malformed JSON
                pass

        pos = json_start + 1

    return results


def main():
    # Path to the spells compendium
    compendium_dir = Path(__file__).parent.parent / "data" / "foundry_examples" / "compendium" / "spells"
    ldb_file = compendium_dir / "000005.ldb"

    if not ldb_file.exists():
        print(f"Error: LDB file not found at {ldb_file}", file=sys.stderr)
        sys.exit(1)

    print(f"Reading spells from {ldb_file}...\n")
    spells = extract_spells_from_ldb(ldb_file)

    print(f"Found {len(spells)} spell entries\n")

    if not spells:
        print("‚ö† No spells found. The LDB file may be empty or corrupted.")
        return

    # Print spells sorted by name
    spells_sorted = sorted(spells, key=lambda s: s.get('name', ''))

    print(f"{'Name':<35} {'Level':<8} {'School':<15} {'ID'}")
    print("-" * 90)

    for spell in spells_sorted:
        spell_id = spell.get('_id', 'UNKNOWN')
        name = spell.get('name', 'UNKNOWN')
        level = spell.get('system', {}).get('level', '?')
        school = spell.get('system', {}).get('school', '?')

        print(f"{name:<35} {str(level):<8} {school:<15} {spell_id}")

    # Save to JSON file
    output_file = compendium_dir.parent / "spells_extracted.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(spells_sorted, f, indent=2, ensure_ascii=False)

    print(f"\n‚úì Saved {len(spells)} spells to {output_file}")

    # Show example UUID format
    example_spell = spells_sorted[0]
    spell_id = example_spell.get('_id')
    uuid = f"Compendium.dnd5e.spells.Item.{spell_id}"

    print(f"\nExample spell: {example_spell.get('name')}")
    print(f"UUID format: {uuid}")
    print(f"\nAll spells follow the pattern: Compendium.dnd5e.spells.Item.{{spell_id}}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/delete_all_actors.py">
#!/usr/bin/env python3
"""Delete all actors in the FoundryVTT world."""

import sys
import os
import argparse
from pathlib import Path

# Add src to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from dotenv import load_dotenv
from foundry.client import FoundryClient

# Load environment variables
load_dotenv(PROJECT_ROOT / ".env")


def main():
    """Delete all actors in the world."""
    parser = argparse.ArgumentParser(description="Delete all actors from FoundryVTT world")
    parser.add_argument("--yes", "-y", action="store_true", help="Skip confirmation prompt")
    args = parser.parse_args()

    client = FoundryClient(target="local")

    print("Fetching all actors...")
    all_actors = client.actors.get_all_actors()

    if not all_actors:
        print("No actors found in the world.")
        return

    # Filter to only world actors (not compendium actors)
    world_actors = [
        actor for actor in all_actors
        if actor.get("uuid", actor.get("_id", "")).startswith("Actor.")
    ]

    print(f"Found {len(all_actors)} total actors ({len(world_actors)} in world, {len(all_actors) - len(world_actors)} in compendiums)")

    if not world_actors:
        print("No world actors to delete.")
        return

    print(f"\nWorld actors to delete:")
    for actor in world_actors:
        name = actor.get("name", "Unknown")
        actor_type = actor.get("type", "Unknown")
        uuid = actor.get("uuid", actor.get("_id"))
        print(f"  - {name} ({actor_type}) [{uuid}]")

    # Ask for confirmation unless --yes flag provided
    if not args.yes:
        try:
            response = input(f"\nAre you sure you want to delete all {len(world_actors)} world actors? (yes/no): ")
            if response.lower() != "yes":
                print("Deletion cancelled.")
                return
        except EOFError:
            print("\nNo input available. Use --yes flag to skip confirmation.")
            return

    print("\nDeleting actors...")
    deleted_count = 0
    failed_count = 0

    for actor in world_actors:
        name = actor.get("name", "Unknown")
        uuid = actor.get("uuid", actor.get("_id"))

        try:
            client.actors.delete_actor(uuid)
            deleted_count += 1
            print(f"  ‚úì Deleted: {name}")
        except Exception as e:
            failed_count += 1
            print(f"  ‚úó Failed to delete {name}: {e}")

    print(f"\nDeletion complete!")
    print(f"  Deleted: {deleted_count}")
    print(f"  Failed: {failed_count}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/fetch_actors.py">
#!/usr/bin/env python3
"""
Fetch all actors from FoundryVTT compendiums.

Extracts actor name, type, and UUID for all available actors.
Useful for building a local database of creatures and NPCs.
"""

import sys
import json
import logging
import os
from pathlib import Path
from string import ascii_lowercase
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

import requests

# Load environment
load_dotenv(Path(__file__).parent.parent / '.env')

logger = logging.getLogger(__name__)


def fetch_all_actors(
    relay_url: str = None,
    api_key: str = None,
    client_id: str = None,
    use_two_letter_fallback: bool = True
) -> list:
    """
    Fetch all actors from FoundryVTT using alphabet strategy.

    Args:
        relay_url: Relay server URL (default: from env)
        api_key: API key (default: from env)
        client_id: Client ID (default: from env)
        use_two_letter_fallback: Use two-letter combos for queries hitting 200 limit

    Returns:
        List of actor dicts with keys: name, type, uuid

    Raises:
        ValueError: If credentials are missing
        RuntimeError: If API call fails
    """
    # Get credentials from environment if not provided
    relay_url = relay_url or os.getenv("FOUNDRY_RELAY_URL")
    api_key = api_key or os.getenv("FOUNDRY_LOCAL_API_KEY")
    client_id = client_id or os.getenv("FOUNDRY_LOCAL_CLIENT_ID")

    if not all([relay_url, api_key, client_id]):
        raise ValueError(
            "Missing required credentials. Set FOUNDRY_RELAY_URL, "
            "FOUNDRY_LOCAL_API_KEY, and FOUNDRY_LOCAL_CLIENT_ID in .env"
        )

    endpoint = f"{relay_url}/search"

    headers = {
        "x-api-key": api_key,
        "Content-Type": "application/json"
    }

    all_actors = {}  # Use dict to deduplicate by UUID
    letters_at_limit = []

    def search_query(query: str) -> list:
        """Execute search query and return results."""
        params = {
            "clientId": client_id,
            "filter": "Actor",
            "query": query
        }

        logger.debug(f"Querying actors: '{query}'")

        try:
            response = requests.get(endpoint, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            # Handle both list and dict response formats
            results = data.get("results", []) if isinstance(data, dict) else data

            logger.debug(f"  Found {len(results)} actors")
            return results

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to search for actors with query '{query}': {e}")
            raise RuntimeError(f"Failed to search for actors: {e}") from e

    # Query with each letter of alphabet
    logger.info("Fetching actors (a-z)...")
    for letter in ascii_lowercase:
        results = search_query(letter)

        # Track if we hit the 200 limit
        if len(results) == 200:
            logger.warning(f"  Letter '{letter}' returned max results (200), may be incomplete")
            letters_at_limit.append(letter)

        # Add to results (deduplicating by UUID)
        for actor in results:
            uuid = actor.get("uuid")
            if uuid and uuid not in all_actors:
                all_actors[uuid] = {
                    "name": actor.get("name"),
                    "type": actor.get("subType"),  # Actor type: npc, character, vehicle
                    "uuid": uuid
                }

    # Empty query to catch actors not starting with letters
    logger.info("Fetching actors (non-alphabetic)...")
    results = search_query("")
    for actor in results:
        uuid = actor.get("uuid")
        if uuid and uuid not in all_actors:
            all_actors[uuid] = {
                "name": actor.get("name"),
                "type": actor.get("subType"),  # Actor type: npc, character, vehicle
                "uuid": uuid
            }

    # Two-letter fallback for queries that hit 200 limit
    if letters_at_limit and use_two_letter_fallback:
        logger.info(f"Using two-letter fallback for: {', '.join(letters_at_limit)}")
        for first in letters_at_limit:
            for second in ascii_lowercase:
                query = f"{first}{second}"
                results = search_query(query)

                # Add new results
                for actor in results:
                    uuid = actor.get("uuid")
                    if uuid and uuid not in all_actors:
                        all_actors[uuid] = {
                            "name": actor.get("name"),
                            "type": actor.get("subType"),  # Actor type: npc, character, vehicle
                            "uuid": uuid
                        }

    return list(all_actors.values())


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Fetch all actors from FoundryVTT compendiums')
    parser.add_argument(
        '--output',
        '-o',
        help='Output JSON file path (default: data/foundry_examples/actors.json)'
    )
    parser.add_argument(
        '--verbose',
        '-v',
        action='store_true',
        help='Show detailed logging'
    )

    args = parser.parse_args()

    # Set up logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format='%(message)s'
    )

    # Determine output file
    if args.output:
        output_file = Path(args.output)
    else:
        output_dir = Path(__file__).parent.parent / 'data' / 'foundry_examples'
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / "actors.json"

    print("Fetching all actors from FoundryVTT...\n")

    # Fetch actors
    actors = fetch_all_actors()

    print(f"\n{'='*60}")
    print(f"Total actors fetched: {len(actors)}")

    # Show type breakdown
    type_counts = {}
    for actor in actors:
        actor_type = actor.get('type', 'unknown')
        type_counts[actor_type] = type_counts.get(actor_type, 0) + 1

    print(f"\nActor types:")
    for actor_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):
        print(f"  {actor_type}: {count}")

    # Save to JSON
    with open(output_file, 'w') as f:
        json.dump(actors, f, indent=2)

    print(f"\n‚úì Saved {len(actors)} actors to {output_file}")


if __name__ == '__main__':
    main()
</file>

<file path="scripts/fetch_items.py">
#!/usr/bin/env python3
"""
Fetch items from FoundryVTT compendiums.

Supports fetching any item subtype (spell, weapon, equipment, etc.)
and automatically deduplicates by source priority.
"""

import sys
import json
import argparse
from pathlib import Path
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from foundry.items import fetch_items_by_type, deduplicate_items, get_source_stats

# Load environment
load_dotenv(Path(__file__).parent.parent / '.env')


def main():
    parser = argparse.ArgumentParser(description='Fetch items from FoundryVTT compendiums')
    parser.add_argument(
        'subtype',
        help='Item subtype to fetch (e.g., spell, weapon, equipment, consumable, container, loot)'
    )
    parser.add_argument(
        '--output',
        '-o',
        help='Output JSON file path (default: data/foundry_examples/<subtype>s.json)'
    )
    parser.add_argument(
        '--deduplicate',
        '-d',
        action='store_true',
        default=True,
        help='Deduplicate items by name (default: True)'
    )
    parser.add_argument(
        '--no-deduplicate',
        dest='deduplicate',
        action='store_false',
        help='Keep all duplicates from different sources'
    )
    parser.add_argument(
        '--verbose',
        '-v',
        action='store_true',
        help='Show detailed logging'
    )

    args = parser.parse_args()

    # Set up logging
    import logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format='%(message)s'
    )

    # Determine output file
    if args.output:
        output_file = Path(args.output)
    else:
        output_dir = Path(__file__).parent.parent / 'data' / 'foundry_examples'
        output_dir.mkdir(parents=True, exist_ok=True)

        # Pluralize subtype for filename
        plural = f"{args.subtype}s" if not args.subtype.endswith('s') else args.subtype
        output_file = output_dir / f"{plural}.json"

    print(f"Fetching all '{args.subtype}' items from FoundryVTT...\n")

    # Fetch items
    items = fetch_items_by_type(args.subtype)

    print(f"\n{'='*60}")
    print(f"Total items fetched: {len(items)}")
    print(f"{'='*60}\n")

    # Show source stats before deduplication
    print("Sources before deduplication:")
    for source, count in sorted(get_source_stats(items).items()):
        print(f"  {source}: {count}")

    # Deduplicate if requested
    if args.deduplicate:
        print("\nDeduplicating by name...\n")
        items = deduplicate_items(items, verbose=args.verbose)

        print(f"\n{'='*60}")
        print(f"Unique items after deduplication: {len(items)}")
        print(f"{'='*60}\n")

        print("Sources after deduplication:")
        for source, count in sorted(get_source_stats(items).items()):
            print(f"  {source}: {count}")

    # Save to file
    with open(output_file, 'w') as f:
        json.dump(items, f, indent=2)

    print(f"\n‚úì Saved {len(items)} items to {output_file}")

    # Show first 20
    print(f"\n{'Name':<40} {'Source':<25} {'UUID'}")
    print("-" * 120)
    for item in items[:20]:
        uuid = item.get('uuid', 'UNKNOWN')
        source = uuid.split('.')[1] if '.' in uuid else 'unknown'
        print(f"{item.get('name', 'UNKNOWN'):<40} {source:<25} {uuid}")

    if len(items) > 20:
        print(f"... and {len(items) - 20} more")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/full_pipeline.py">
#!/usr/bin/env python3
"""
Full pipeline orchestration: PDF ‚Üí XML ‚Üí Scene Art ‚Üí Actors ‚Üí FoundryVTT ‚Üí Export

This script coordinates the complete workflow:
1. Split PDF into chapter PDFs (split_pdf.py)
2. Generate XML from chapters using Gemini (pdf_to_xml.py)
2.5. Generate scene artwork from XML (generate_scene_art.py)
3. Process actors and NPCs (process_actors.py)
4. Upload XML to FoundryVTT (upload_to_foundry.py)
5. Export journal from FoundryVTT to HTML (export_from_foundry.py)

Each step can be skipped with flags for resuming interrupted runs.
"""

import os
import sys
import subprocess
import logging
from pathlib import Path
from dotenv import load_dotenv

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.logging_config import setup_logging

logger = setup_logging(__name__)


def run_pdf_split(project_root: Path) -> None:
    """
    Run split_pdf.py to split source PDF into chapter PDFs.

    Args:
        project_root: Project root directory

    Raises:
        RuntimeError: If split fails
    """
    split_script = project_root / "src" / "pdf_processing" / "split_pdf.py"

    logger.info("=" * 60)
    logger.info("STEP 1: Splitting PDF into chapters")
    logger.info("=" * 60)

    try:
        result = subprocess.run(
            [sys.executable, str(split_script)],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )

        if result.returncode != 0:
            logger.error(f"PDF split failed: {result.stderr}")
            raise RuntimeError(f"split_pdf.py failed with code {result.returncode}")

        logger.info("‚úì PDF split completed successfully")
        if result.stdout:
            logger.debug(f"Output: {result.stdout}")

    except subprocess.TimeoutExpired:
        logger.error("PDF split timed out after 5 minutes")
        raise RuntimeError("PDF split timed out")
    except Exception as e:
        logger.error(f"Failed to run split_pdf.py: {e}")
        raise


def run_pdf_to_xml(project_root: Path, chapter_file: str = None) -> Path:
    """
    Run pdf_to_xml.py to generate XML from chapter PDFs using Gemini.

    Args:
        project_root: Project root directory
        chapter_file: Optional specific chapter file to process

    Returns:
        Path to the generated run directory

    Raises:
        RuntimeError: If XML generation fails
    """
    xml_script = project_root / "src" / "pdf_processing" / "pdf_to_xml.py"

    logger.info("=" * 60)
    logger.info("STEP 2: Generating XML from PDFs using Gemini")
    logger.info("=" * 60)
    logger.info("This may take several minutes depending on PDF size...")

    try:
        cmd = [sys.executable, str(xml_script)]
        if chapter_file:
            cmd.extend(["--file", chapter_file])

        result = subprocess.run(
            cmd,
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=3600  # 1 hour timeout for Gemini API calls
        )

        if result.returncode != 0:
            logger.error(f"XML generation failed: {result.stderr}")
            raise RuntimeError(f"pdf_to_xml.py failed with code {result.returncode}")

        logger.info("‚úì XML generation completed successfully")
        if result.stdout:
            logger.debug(f"Output: {result.stdout}")

        # Find the latest run directory
        runs_dir = project_root / "output" / "runs"
        if not runs_dir.exists():
            raise RuntimeError("No runs directory found after XML generation")

        run_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]
        if not run_dirs:
            raise RuntimeError("No run directories found after XML generation")

        latest_run = sorted(run_dirs, key=lambda d: d.name)[-1]
        logger.info(f"XML files generated in: {latest_run / 'documents'}")
        return latest_run

    except subprocess.TimeoutExpired:
        logger.error("XML generation timed out after 1 hour")
        raise RuntimeError("XML generation timed out")
    except Exception as e:
        logger.error(f"Failed to run pdf_to_xml.py: {e}")
        raise


def process_actors(run_dir: Path, target: str = "local") -> dict:
    """
    Process actors and NPCs from XML files.

    Args:
        run_dir: Path to run directory containing XML files
        target: Target environment ('local' or 'forge')

    Returns:
        Actor processing statistics dict

    Raises:
        RuntimeError: If actor processing fails
    """
    logger.info("=" * 60)
    logger.info("STEP 3: Processing actors and NPCs")
    logger.info("=" * 60)

    # Import here to avoid circular dependencies
    from src.actors.process_actors import process_actors_for_run

    try:
        result = process_actors_for_run(str(run_dir), target=target)

        logger.info(
            f"‚úì Actor processing complete: "
            f"{result['stat_blocks_created']} creatures created, "
            f"{result['stat_blocks_reused']} reused, "
            f"{result['npcs_created']} NPCs created"
        )

        if result.get("errors"):
            logger.warning(f"{len(result['errors'])} error(s) occurred during actor processing")
            for error in result["errors"]:
                logger.error(f"  {error}")

        return result

    except Exception as e:
        logger.error(f"Actor processing failed: {e}")
        raise RuntimeError(f"Actor processing failed: {e}")


def run_scene_artwork_generation(
    run_dir: Path,
    style_prompt: str = None,
    continue_on_error: bool = False
) -> None:
    """
    Generate scene artwork from chapter XML files.

    Args:
        run_dir: Run directory containing documents/ folder
        style_prompt: Optional custom style prompt for image generation
        continue_on_error: Continue processing other chapters if one fails

    Raises:
        RuntimeError: If scene generation fails and continue_on_error is False
    """
    logger.info("=" * 60)
    logger.info("STEP 2.5: Generating scene artwork")
    logger.info("=" * 60)

    output_dir = run_dir / "scene_artwork"

    try:
        # Import scene generation functions
        sys.path.insert(0, os.path.join(os.path.dirname(__file__)))
        from generate_scene_art import process_chapter, sanitize_filename

        # Find XML files to process
        xml_files = list((run_dir / "documents").glob("*.xml"))

        if not xml_files:
            logger.warning("No XML files found, skipping scene artwork generation")
            return

        logger.info(f"Processing {len(xml_files)} chapter file(s)...")
        output_dir.mkdir(parents=True, exist_ok=True)

        total_scenes = 0
        total_images = 0
        failed_chapters = []

        for xml_file in xml_files:
            try:
                stats = process_chapter(xml_file, output_dir, style_prompt)
                total_scenes += stats["scenes_found"]
                total_images += stats["images_generated"]
                logger.info(f"  ‚úì {xml_file.name}: {stats['scenes_found']} scenes, {stats['images_generated']} images")
            except Exception as e:
                logger.error(f"Failed to process {xml_file.name}: {e}")
                failed_chapters.append(xml_file.name)
                if not continue_on_error:
                    raise

        logger.info("‚úì Scene artwork generation completed")
        logger.info(f"  Total scenes: {total_scenes}, Images: {total_images}")

        if failed_chapters:
            logger.warning(f"  Failed chapters: {', '.join(failed_chapters)}")

    except Exception as e:
        logger.error(f"Scene artwork generation failed: {e}")
        raise RuntimeError(f"Scene artwork generation failed: {e}") from e


def upload_to_foundry(run_dir: Path, target: str = "local", journal_name: str = None) -> dict:
    """
    Upload XML files to FoundryVTT.

    Args:
        run_dir: Path to run directory containing XML files
        target: Target environment ('local' or 'forge')
        journal_name: Optional journal name

    Returns:
        Upload statistics dict with 'journal_uuid' key

    Raises:
        RuntimeError: If upload fails
    """
    logger.info("=" * 60)
    logger.info("STEP 4: Uploading to FoundryVTT")
    logger.info("=" * 60)

    # Import here to avoid circular dependencies
    from src.foundry.upload_journal_to_foundry import upload_run_to_foundry

    try:
        result = upload_run_to_foundry(
            str(run_dir),
            target=target,
            journal_name=journal_name
        )

        if result["failed"] > 0 or result.get("errors"):
            logger.warning(
                f"Upload completed with errors: "
                f"{result['uploaded']} succeeded, {result['failed']} failed"
            )
            for error in result.get("errors", []):
                logger.error(f"  {error}")
        else:
            logger.info(f"‚úì Successfully uploaded {result['uploaded']} page(s) to FoundryVTT")

        return result

    except Exception as e:
        logger.error(f"Upload to FoundryVTT failed: {e}")
        raise RuntimeError(f"Upload failed: {e}")


def export_from_foundry(
    run_dir: Path,
    target: str = "local",
    journal_name: str = None,
    journal_uuid: str = None
) -> None:
    """
    Export journal from FoundryVTT to HTML in the run directory.

    Args:
        run_dir: Path to run directory to save export
        target: Target environment ('local' or 'forge')
        journal_name: Name of the journal to export (used for filename)
        journal_uuid: Optional UUID of journal (if provided, skips search)

    Raises:
        RuntimeError: If export fails
    """
    logger.info("=" * 60)
    logger.info("STEP 5: Exporting from FoundryVTT")
    logger.info("=" * 60)

    # Import here to avoid circular dependencies
    from src.foundry.client import FoundryClient
    from src.foundry.export_from_foundry import export_to_html

    try:
        # Initialize client
        client = FoundryClient(target=target)

        # Get journal UUID (either provided or search by name)
        if journal_uuid:
            logger.info(f"Using provided journal UUID: {journal_uuid}")
        else:
            # Need to search for journal by name
            if not journal_name:
                logger.error("Must provide either journal_uuid or journal_name")
                return

            logger.info(f"Searching for journal: {journal_name}")
            journal = client.get_journal_by_name(journal_name)

            if not journal:
                logger.warning(f"Journal not found: {journal_name} - skipping export")
                return

            # Extract UUID from search result
            journal_uuid = journal.get('uuid')
            if not journal_uuid:
                journal_id = journal.get('_id') or journal.get('id')
                if journal_id:
                    journal_uuid = f"JournalEntry.{journal_id}"

            if not journal_uuid:
                logger.error(f"Could not determine UUID for journal: {journal_name}")
                return

        # Get full journal data
        logger.info(f"Retrieving journal data: {journal_uuid}")
        journal_data = client.get_journal(journal_uuid)

        # Use journal name from data if not provided
        if not journal_name:
            data = journal_data.get('data', journal_data)
            journal_name = data.get('name', 'journal')

        # Export to HTML in run directory
        export_dir = run_dir / "foundry_export"
        export_dir.mkdir(exist_ok=True)
        output_path = export_dir / f"{journal_name}.html"

        export_to_html(journal_data, str(output_path), single_file=True)
        logger.info(f"‚úì Exported journal to: {output_path}")

    except Exception as e:
        logger.error(f"Export from FoundryVTT failed: {e}")
        raise RuntimeError(f"Export failed: {e}")


def main():
    """Main entry point for full pipeline orchestration."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Full pipeline: Split PDF ‚Üí Generate XML ‚Üí Upload to FoundryVTT ‚Üí Export HTML"
    )
    parser.add_argument(
        "--skip-split",
        action="store_true",
        help="Skip PDF splitting (use existing chapter PDFs)"
    )
    parser.add_argument(
        "--skip-xml",
        action="store_true",
        help="Skip XML generation (use latest run)"
    )
    parser.add_argument(
        "--skip-upload",
        action="store_true",
        help="Skip FoundryVTT upload"
    )
    parser.add_argument(
        "--skip-actors",
        action="store_true",
        help="Skip actor/NPC extraction and creation"
    )
    parser.add_argument(
        "--skip-export",
        action="store_true",
        help="Skip FoundryVTT export"
    )
    parser.add_argument(
        "--skip-scenes",
        action="store_true",
        help="Skip scene artwork generation"
    )
    parser.add_argument(
        "--actors-only",
        action="store_true",
        help="Only process actors (skip PDF splitting, XML generation, upload)"
    )
    parser.add_argument(
        "--chapter-file",
        help="Process only a specific chapter file (e.g., '01_Introduction.pdf')"
    )
    parser.add_argument(
        "--run-dir",
        help="Specific run directory to use (for --actors-only or --skip-xml)"
    )
    parser.add_argument(
        "--journal-name",
        help="Name for the FoundryVTT journal entry (default: 'D&D Module')"
    )
    parser.add_argument(
        "--target",
        choices=["local", "forge"],
        default="local",
        help="Target FoundryVTT environment (default: local)"
    )

    args = parser.parse_args()

    # Load environment variables
    load_dotenv()

    # Determine project root
    project_root = Path(__file__).parent.parent

    try:
        # Actors-only mode
        if args.actors_only:
            logger.info("\n" + "=" * 60)
            logger.info("Running in actors-only mode")
            logger.info("=" * 60)

            # Need to find run directory
            if args.run_dir:
                run_dir = Path(args.run_dir)
                if not run_dir.exists():
                    logger.error(f"Specified run directory not found: {run_dir}")
                    sys.exit(1)
            else:
                # Find latest run
                runs_dir = project_root / "output" / "runs"
                if not runs_dir.exists():
                    logger.error("No runs directory found. Run full pipeline first.")
                    sys.exit(1)

                run_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]
                if not run_dirs:
                    logger.error("No run directories found. Run full pipeline first.")
                    sys.exit(1)

                run_dir = sorted(run_dirs, key=lambda d: d.name)[-1]
                logger.info(f"Using latest run: {run_dir.name}")

            try:
                actor_stats = process_actors(run_dir, target=args.target)
                logger.info("=" * 60)
                logger.info("ACTORS-ONLY MODE COMPLETE!")
                logger.info("=" * 60)
                sys.exit(0)
            except Exception as e:
                logger.error(f"Actor processing failed: {e}")
                sys.exit(1)

        # Step 1: Split PDF
        if args.skip_split:
            logger.info("Skipping PDF split (--skip-split)")
        else:
            run_pdf_split(project_root)

        # Step 2: Generate XML
        if args.skip_xml:
            logger.info("Skipping XML generation (--skip-xml), using latest run...")

            if args.run_dir:
                run_dir = Path(args.run_dir)
                if not run_dir.exists():
                    logger.error(f"Specified run directory not found: {run_dir}")
                    sys.exit(1)
            else:
                runs_dir = project_root / "output" / "runs"

                if not runs_dir.exists():
                    logger.error("No runs directory found")
                    sys.exit(1)

                run_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]
                if not run_dirs:
                    logger.error("No run directories found")
                    sys.exit(1)

                run_dir = sorted(run_dirs, key=lambda d: d.name)[-1]

            logger.info(f"Using run: {run_dir.name}")
        else:
            run_dir = run_pdf_to_xml(project_root, chapter_file=args.chapter_file)

        # Step 2.5: Generate scene artwork (optional)
        if args.skip_scenes:
            logger.info("Skipping scene artwork generation (--skip-scenes)")
        else:
            try:
                # Get style prompt from environment variable if not in args
                style_prompt = os.getenv("IMAGE_STYLE_PROMPT")
                run_scene_artwork_generation(run_dir, style_prompt=style_prompt, continue_on_error=True)
            except Exception as e:
                logger.error(f"Scene artwork generation failed: {e}")
                logger.warning("Continuing with pipeline despite scene generation failure")

        # Step 3: Process actors and NPCs
        if args.skip_actors:
            logger.info("Skipping actor processing (--skip-actors)")
        else:
            try:
                actor_stats = process_actors(run_dir, target=args.target)
            except Exception as e:
                logger.error(f"Actor processing failed: {e}")
                logger.warning("Continuing with pipeline...")

        # Step 4: Upload to FoundryVTT
        upload_result = None
        if args.skip_upload:
            logger.info("Skipping FoundryVTT upload (--skip-upload)")
        else:
            upload_result = upload_to_foundry(
                run_dir,
                target=args.target,
                journal_name=args.journal_name
            )

            if upload_result["failed"] > 0 or upload_result.get("errors"):
                logger.warning("Some uploads failed, check logs above")
                sys.exit(1)

        # Step 5: Export from FoundryVTT
        if args.skip_export:
            logger.info("Skipping FoundryVTT export (--skip-export)")
        elif args.skip_upload:
            logger.info("Skipping export (upload was skipped)")
        else:
            # Use journal UUID from upload result (saves an API search call)
            journal_uuid = upload_result.get("journal_uuid") if upload_result else None
            journal_name = upload_result.get("journal_name") if upload_result else (args.journal_name or "D&D Module")

            export_from_foundry(
                run_dir,
                target=args.target,
                journal_name=journal_name,
                journal_uuid=journal_uuid
            )

        logger.info("=" * 60)
        logger.info("PIPELINE COMPLETE!")
        logger.info("=" * 60)
        sys.exit(0)

    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"PIPELINE FAILED: {e}")
        logger.error("=" * 60)
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/pit_fiend_end_to_end.py">
#!/usr/bin/env python3
"""
Process Pit Fiend through COMPLETE end-to-end pipeline:
Raw text ‚Üí StatBlock ‚Üí ParsedActorData ‚Üí Upload ‚Üí Download ‚Üí Save JSON

This script demonstrates the full actor generation workflow using the new
parallel parser.
"""

import sys
import json
import asyncio
import re
from pathlib import Path
from dotenv import load_dotenv

# Setup paths
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from actors.models import StatBlock
from foundry.actors.parser import parse_stat_block_parallel
from foundry.actors.converter import convert_to_foundry
from foundry.client import FoundryClient
from foundry.actors.spell_cache import SpellCache

load_dotenv(project_root / ".env")


def parse_raw_text_to_statblock(text: str) -> StatBlock:
    """
    Parse raw D&D 5e stat block text into StatBlock model.

    This is a simple regex-based parser for the standard stat block format.
    For production, consider using Gemini to parse more complex formats.
    """
    lines = text.strip().split('\n')

    # Extract name (first non-empty line)
    name = lines[1].strip() if len(lines) > 1 else "Unknown"

    # Extract basic stats with regex
    ac_match = re.search(r'Armor Class (\d+)', text)
    hp_match = re.search(r'Hit Points (\d+)', text)
    cr_match = re.search(r'Challenge (\d+)', text)

    armor_class = int(ac_match.group(1)) if ac_match else 10
    hit_points = int(hp_match.group(1)) if hp_match else 1
    challenge_rating = float(cr_match.group(1)) if cr_match else 0

    # Extract size and type
    type_match = re.search(r'(Tiny|Small|Medium|Large|Huge|Gargantuan) ([^,]+)', text)
    size = type_match.group(1).lower() if type_match else None
    creature_type = type_match.group(2).lower() if type_match else None

    # Extract alignment
    alignment_match = re.search(r', ([^,\n]+Evil|Neutral|Good)', text)
    alignment = alignment_match.group(1).lower() if alignment_match else None

    # Extract abilities
    abilities = {}
    ability_pattern = r'(STR|DEX|CON|INT|WIS|CHA)\s*\n\s*(\d+)'
    for match in re.finditer(ability_pattern, text):
        abilities[match.group(1)] = int(match.group(2))

    # Split into sections
    traits_section = extract_section(text, "Traits", "Actions")
    actions_section = extract_section(text, "Actions", "Reactions")

    # Extract traits (split by double newlines or paragraph breaks)
    traits = []
    if traits_section:
        # Split on trait names (capitalized words followed by period)
        trait_entries = re.split(r'\n(?=[A-Z][a-z]+ (?:[A-Z][a-z]+ )*[A-Z][a-z]+\.)', traits_section)
        traits = [t.strip() for t in trait_entries if t.strip() and len(t.strip()) > 20]

    # Extract actions
    actions = []
    if actions_section:
        action_entries = re.split(r'\n(?=[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\.)', actions_section)
        actions = [a.strip() for a in action_entries if a.strip() and len(a.strip()) > 20]

    return StatBlock(
        name=name,
        raw_text=text,
        armor_class=armor_class,
        hit_points=hit_points,
        challenge_rating=challenge_rating,
        size=size,
        type=creature_type,
        alignment=alignment,
        abilities=abilities if abilities else None,
        traits=traits,
        actions=actions,
        reactions=[]
    )


def extract_section(text: str, start_marker: str, end_marker: str = None) -> str:
    """Extract text between two section markers."""
    pattern = f"{start_marker}\s*\n(.*?)(?:{end_marker}|\Z)"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip() if match else ""


async def main():
    print("=" * 80)
    print("PIT FIEND END-TO-END PIPELINE")
    print("Using NEW parallel parser")
    print("=" * 80)

    # Step 1: Read raw text
    print("\n1. Reading raw stat block from pit_fiend.txt...")
    input_path = project_root / "data" / "foundry_examples" / "pit_fiend.txt"
    with open(input_path, "r") as f:
        raw_text = f.read()
    print(f"   ‚úì Read {len(raw_text)} characters")

    # Step 2: Parse raw text ‚Üí StatBlock
    print("\n2. Parsing raw text to StatBlock...")
    stat_block = parse_raw_text_to_statblock(raw_text)
    print(f"   ‚úì Parsed StatBlock:")
    print(f"     - Name: {stat_block.name}")
    print(f"     - AC: {stat_block.armor_class}, HP: {stat_block.hit_points}, CR: {stat_block.challenge_rating}")
    print(f"     - Traits: {len(stat_block.traits)}")
    print(f"     - Actions: {len(stat_block.actions)}")
    print(f"     - Reactions: {len(stat_block.reactions)}")

    # Step 3: Load spell cache
    print("\n3. Loading spell cache...")
    spell_cache = SpellCache()
    spell_cache.load()
    print(f"   ‚úì Spell cache loaded ({len(spell_cache._spell_by_name)} spells)")

    # Step 4: StatBlock ‚Üí ParsedActorData (using parallel parser)
    print("\n4. Parsing StatBlock to ParsedActorData (parallel Gemini calls)...")
    print(f"   This will make {len(stat_block.actions) + len(stat_block.traits)} parallel API calls")

    parsed_actor = await parse_stat_block_parallel(
        stat_block,
        spell_cache=spell_cache,
        model_name="gemini-2.0-flash-exp"
    )

    print(f"   ‚úì Parsed to ParsedActorData:")
    print(f"     - Attacks: {len(parsed_actor.attacks)}")
    print(f"     - Traits: {len(parsed_actor.traits)}")
    print(f"     - Multiattack: {parsed_actor.multiattack is not None}")
    print(f"     - Innate Spellcasting: {parsed_actor.innate_spellcasting is not None}")

    if parsed_actor.innate_spellcasting:
        print(f"       - Spells: {len(parsed_actor.innate_spellcasting.spells)}")

    # Step 5: Convert to FoundryVTT format
    print("\n5. Converting to FoundryVTT format...")
    foundry_json, spell_uuids = convert_to_foundry(parsed_actor, spell_cache=spell_cache)
    print(f"   ‚úì Converted to FoundryVTT format")
    print(f"     - Items in payload: {len(foundry_json['items'])}")
    print(f"     - Spells via /give: {len(spell_uuids)}")

    # Item breakdown
    weapon_count = sum(1 for i in foundry_json["items"] if i["type"] == "weapon")
    feat_count = sum(1 for i in foundry_json["items"] if i["type"] == "feat")
    print(f"     - Weapons: {weapon_count}")
    print(f"     - Feats: {feat_count}")

    # Step 6: Upload to FoundryVTT
    print("\n6. Uploading to FoundryVTT...")
    client = FoundryClient(target="local")
    actor_uuid = client.actors.create_actor(foundry_json, spell_uuids=spell_uuids)
    print(f"   ‚úì Uploaded: {actor_uuid}")

    # Step 7: Download from FoundryVTT
    print("\n7. Downloading from FoundryVTT...")
    downloaded = client.actors.get_actor(actor_uuid)
    print(f"   ‚úì Downloaded: {downloaded['name']}")
    print(f"     - Total items: {len(downloaded['items'])}")

    # Item breakdown
    downloaded_weapons = [i for i in downloaded["items"] if i["type"] == "weapon"]
    downloaded_feats = [i for i in downloaded["items"] if i["type"] == "feat"]
    downloaded_spells = [i for i in downloaded["items"] if i["type"] == "spell"]

    print(f"     - Weapons: {len(downloaded_weapons)}")
    print(f"     - Feats: {len(downloaded_feats)}")
    print(f"     - Spells: {len(downloaded_spells)}")

    # Step 8: Save to file
    output_path = project_root / "output" / "pit_fiend_end_to_end.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w") as f:
        json.dump(downloaded, f, indent=2)

    print(f"\n8. Saved to: {output_path}")
    print(f"   File size: {output_path.stat().st_size / 1024:.1f} KB")

    # Step 9: Verification
    print("\n9. Round-trip verification:")
    print(f"   Downloaded items:")
    for item in downloaded["items"]:
        print(f"     - {item['name']} ({item['type']})")

    # Verify no items were lost
    uploaded_items = {i["name"] for i in foundry_json["items"]}
    downloaded_items_no_spells = {i["name"] for i in downloaded["items"] if i["type"] != "spell"}
    missing_items = uploaded_items - downloaded_items_no_spells

    if missing_items:
        print(f"\n   ‚úó WARNING: Missing items after round-trip: {missing_items}")

    # Check for duplicate spells
    spell_names = [i["name"] for i in downloaded["items"] if i["type"] == "spell"]
    if len(spell_names) != len(set(spell_names)):
        from collections import Counter
        duplicates = [name for name, count in Counter(spell_names).items() if count > 1]
        print(f"\n   ‚úó WARNING: Duplicate spells detected: {duplicates}")

    print("\n" + "=" * 80)
    print("PIPELINE COMPLETE")
    print("=" * 80)
    print(f"\nRaw text ‚Üí StatBlock ‚Üí ParsedActorData ‚Üí FoundryVTT ‚Üí Downloaded JSON")
    print(f"Output saved to: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="scripts/process_and_upload.py">
#!/usr/bin/env python3
"""
Orchestration script: Convert XML to HTML and optionally upload to FoundryVTT.

This script coordinates the workflow:
1. Run xml_to_html.py to convert XML to HTML (or use existing run)
2. If FOUNDRY_AUTO_UPLOAD=true, upload HTML files to FoundryVTT

Keeps xml_to_html.py and upload_to_foundry.py decoupled.
"""

import os
import sys
import subprocess
import logging
from pathlib import Path
from dotenv import load_dotenv

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.logging_config import setup_logging

logger = setup_logging(__name__)


def run_xml_to_html(project_root: Path) -> Path:
    """
    Run xml_to_html.py conversion script.

    Args:
        project_root: Project root directory

    Returns:
        Path to the generated HTML directory

    Raises:
        RuntimeError: If conversion fails
    """
    xml_to_html_script = project_root / "src" / "pdf_processing" / "xml_to_html.py"

    logger.info("Running XML to HTML conversion...")

    try:
        result = subprocess.run(
            [sys.executable, str(xml_to_html_script)],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout
        )

        if result.returncode != 0:
            logger.error(f"XML to HTML conversion failed: {result.stderr}")
            raise RuntimeError(f"xml_to_html.py failed with code {result.returncode}")

        logger.info("XML to HTML conversion completed successfully")
        logger.debug(f"Output: {result.stdout}")

        # Find the latest run directory
        runs_dir = project_root / "output" / "runs"
        if not runs_dir.exists():
            raise RuntimeError("No runs directory found after conversion")

        run_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]
        if not run_dirs:
            raise RuntimeError("No run directories found after conversion")

        latest_run = sorted(run_dirs, key=lambda d: d.name)[-1]
        html_dir = latest_run / "documents" / "html"

        if not html_dir.exists():
            raise RuntimeError(f"HTML directory not found: {html_dir}")

        logger.info(f"HTML files generated in: {html_dir}")
        return html_dir

    except subprocess.TimeoutExpired:
        logger.error("XML to HTML conversion timed out after 10 minutes")
        raise RuntimeError("Conversion timed out")
    except Exception as e:
        logger.error(f"Failed to run xml_to_html.py: {e}")
        raise


def upload_to_foundry(html_dir: Path, target: str = "local") -> dict:
    """
    Upload HTML files to FoundryVTT.

    Args:
        html_dir: Path to HTML directory
        target: Target environment ('local' or 'forge')

    Returns:
        Upload statistics dict

    Raises:
        RuntimeError: If upload fails
    """
    logger.info(f"Uploading to FoundryVTT ({target})...")

    # Import here to avoid circular dependencies
    from src.foundry.upload_journal_to_foundry import upload_run_to_foundry

    try:
        result = upload_run_to_foundry(str(html_dir), target=target)

        if result["failed"] > 0:
            logger.warning(
                f"Upload completed with errors: "
                f"{result['uploaded']} succeeded, {result['failed']} failed"
            )
        else:
            logger.info(f"Successfully uploaded {result['uploaded']} journals to FoundryVTT")

        return result

    except Exception as e:
        logger.error(f"Upload to FoundryVTT failed: {e}")
        raise RuntimeError(f"Upload failed: {e}")


def main():
    """Main entry point for orchestration script."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Convert XML to HTML and optionally upload to FoundryVTT"
    )
    parser.add_argument(
        "--skip-conversion",
        action="store_true",
        help="Skip XML to HTML conversion, use latest run"
    )
    parser.add_argument(
        "--upload",
        action="store_true",
        help="Upload to FoundryVTT after conversion (overrides FOUNDRY_AUTO_UPLOAD)"
    )
    parser.add_argument(
        "--no-upload",
        action="store_true",
        help="Skip upload even if FOUNDRY_AUTO_UPLOAD=true"
    )
    parser.add_argument(
        "--target",
        choices=["local", "forge"],
        default="local",
        help="Target FoundryVTT environment (default: local)"
    )

    args = parser.parse_args()

    # Load environment variables
    load_dotenv()

    # Determine project root
    project_root = Path(__file__).parent.parent

    try:
        # Step 1: XML to HTML conversion
        if args.skip_conversion:
            logger.info("Skipping conversion, using latest run...")
            runs_dir = project_root / "output" / "runs"

            if not runs_dir.exists():
                logger.error("No runs directory found")
                sys.exit(1)

            run_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]
            if not run_dirs:
                logger.error("No run directories found")
                sys.exit(1)

            latest_run = sorted(run_dirs, key=lambda d: d.name)[-1]
            html_dir = latest_run / "documents" / "html"

            if not html_dir.exists():
                logger.error(f"HTML directory not found in latest run: {html_dir}")
                sys.exit(1)

            logger.info(f"Using HTML from: {html_dir}")
        else:
            html_dir = run_xml_to_html(project_root)

        # Step 2: Upload to FoundryVTT (if enabled)
        should_upload = False

        if args.no_upload:
            logger.info("Upload disabled via --no-upload flag")
        elif args.upload:
            logger.info("Upload enabled via --upload flag")
            should_upload = True
        else:
            # Check environment variable
            auto_upload = os.getenv("FOUNDRY_AUTO_UPLOAD", "false").lower() == "true"
            if auto_upload:
                logger.info("Auto-upload enabled via FOUNDRY_AUTO_UPLOAD=true")
                should_upload = True
            else:
                logger.info("Auto-upload disabled (set FOUNDRY_AUTO_UPLOAD=true to enable)")

        if should_upload:
            target = os.getenv("FOUNDRY_TARGET", args.target)
            result = upload_to_foundry(html_dir, target=target)

            if result["failed"] > 0:
                logger.warning("Some uploads failed, check logs above")
                sys.exit(1)

        logger.info("Processing complete!")
        sys.exit(0)

    except Exception as e:
        logger.error(f"Processing failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/process_pit_fiend.py">
#!/usr/bin/env python3
"""Process Pit Fiend through full actor pipeline: parse ‚Üí upload ‚Üí download ‚Üí save."""

import sys
import json
from pathlib import Path
from dotenv import load_dotenv

# Setup paths
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from foundry.actors.models import (
    ParsedActorData, Attack, Trait, DamageFormula,
    Multiattack, InnateSpellcasting, InnateSpell, AttackSave,
    DamageModification
)
from foundry.actors.converter import convert_to_foundry
from foundry.client import FoundryClient
from foundry.actors.spell_cache import SpellCache

load_dotenv(project_root / ".env")

# Parse Pit Fiend from pit_fiend.txt
pit_fiend = ParsedActorData(
    source_statblock_name="Pit Fiend",
    name="Pit Fiend",
    size="large",
    creature_type="fiend",
    creature_subtype="devil",
    alignment="lawful evil",
    armor_class=19,
    hit_points=300,
    hit_dice="24d10+168",
    speed_walk=30,
    speed_fly=60,
    challenge_rating=20,
    abilities={
        "STR": 26,
        "DEX": 14,
        "CON": 24,
        "INT": 22,
        "WIS": 18,
        "CHA": 24
    },
    saving_throw_proficiencies=["dex", "con", "wis"],
    condition_immunities=["poisoned"],
    damage_immunities=DamageModification(types=["fire", "poison"]),
    damage_resistances=DamageModification(
        types=["cold", "bludgeoning", "piercing", "slashing"],
        condition="from nonmagical attacks that aren't silvered"
    ),
    truesight=120,
    languages=["Infernal", "Telepathy 120 ft."],
    multiattack=Multiattack(
        name="Multiattack",
        description="The pit fiend makes four attacks: one with its bite, one with its claw, one with its mace, and one with its tail.",
        num_attacks=4
    ),
    traits=[
        Trait(
            name="Fear Aura",
            description=(
                "Any creature hostile to the pit fiend that starts its turn within 20 feet "
                "of the pit fiend must make a DC 21 Wisdom saving throw, unless the pit fiend "
                "is incapacitated. On a failed save, the creature is frightened until the start "
                "of its next turn. If a creature's saving throw is successful, the creature is "
                "immune to the pit fiend's Fear Aura for the next 24 hours."
            ),
            activation="passive"
        ),
        Trait(
            name="Magic Resistance",
            description=(
                "The pit fiend has advantage on saving throws against spells and other "
                "magical effects."
            ),
            activation="passive"
        ),
        Trait(
            name="Magic Weapons",
            description="The pit fiend's weapon attacks are magical.",
            activation="passive"
        ),
    ],
    innate_spellcasting=InnateSpellcasting(
        ability="charisma",
        save_dc=21,
        spells=[
            InnateSpell(name="Detect Magic", frequency="at will"),
            InnateSpell(name="Fireball", frequency="at will"),
            InnateSpell(name="Hold Monster", frequency="3/day", uses=3),
            InnateSpell(name="Wall of Fire", frequency="3/day", uses=3),
        ]
    ),
    attacks=[
        Attack(
            name="Bite",
            attack_type="melee",
            attack_bonus=14,
            reach=5,
            damage=[
                DamageFormula(number=4, denomination=6, bonus="+8", type="piercing")
            ],
            # CRITICAL: AttackSave with ongoing poison damage
            attack_save=AttackSave(
                ability="con",
                dc=21,
                ongoing_damage=[DamageFormula(number=6, denomination=6, bonus="", type="poison")],
                duration_rounds=None,  # Repeats until success
                effect_description=(
                    "Poisoned - can't regain HP. Repeat save at end of each turn to end effect."
                )
            )
        ),
        Attack(
            name="Claw",
            attack_type="melee",
            attack_bonus=14,
            reach=10,
            damage=[
                DamageFormula(number=2, denomination=8, bonus="+8", type="slashing")
            ]
        ),
        Attack(
            name="Mace",
            attack_type="melee",
            attack_bonus=14,
            reach=10,
            damage=[
                DamageFormula(number=2, denomination=6, bonus="+8", type="bludgeoning"),
                DamageFormula(number=6, denomination=6, bonus="", type="fire")
            ]
        ),
        Attack(
            name="Tail",
            attack_type="melee",
            attack_bonus=14,
            reach=10,
            damage=[
                DamageFormula(number=3, denomination=10, bonus="+8", type="bludgeoning")
            ]
        ),
    ]
)

print("=" * 80)
print("PIT FIEND PIPELINE TEST")
print("=" * 80)

# Step 1: Load spell cache
print("\n1. Loading spell cache...")
spell_cache = SpellCache()
spell_cache.load()
print("   ‚úì Spell cache loaded")

# Step 2: Convert to FoundryVTT format
print("\n2. Converting to FoundryVTT format...")
foundry_json = convert_to_foundry(pit_fiend, spell_cache=spell_cache)
print(f"   ‚úì Converted to FoundryVTT format ({len(foundry_json['items'])} items)")

# Print item summary
weapon_count = sum(1 for i in foundry_json["items"] if i["type"] == "weapon")
feat_count = sum(1 for i in foundry_json["items"] if i["type"] == "feat")
spell_count = sum(1 for i in foundry_json["items"] if i["type"] == "spell")
print(f"   - Weapons: {weapon_count}")
print(f"   - Feats: {feat_count}")
print(f"   - Spells: {spell_count}")

# Verify Bite has 3 activities
bite = [i for i in foundry_json["items"] if i["name"] == "Bite"][0]
print(f"\n   Bite weapon activities: {len(bite['system']['activities'])}")
for act_id, act in bite["system"]["activities"].items():
    print(f"   - {act['type']}")
    if act["type"] == "damage":
        part = act["damage"]["parts"][0]
        print(f"     damage.parts[0]: {part}")

# Step 3: Upload to FoundryVTT
print("\n3. Uploading to FoundryVTT...")
client = FoundryClient(target="local")
actor_uuid = client.actors.create_actor(foundry_json)
print(f"   ‚úì Uploaded: {actor_uuid}")

# Step 4: Download from FoundryVTT
print("\n4. Downloading from FoundryVTT...")
downloaded = client.actors.get_actor(actor_uuid)
print(f"   ‚úì Downloaded: {downloaded['name']} ({len(downloaded['items'])} items)")

# Step 5: Save to file
output_path = project_root / "output" / "pit_fiend_roundtrip.json"
output_path.parent.mkdir(parents=True, exist_ok=True)

with open(output_path, "w") as f:
    json.dump(downloaded, f, indent=2)

print(f"\n5. Saved to: {output_path}")

# Step 6: Verify round-trip
print("\n6. Round-trip verification:")
print(f"   - Actor name: {downloaded['name']}")
print(f"   - Total items: {len(downloaded['items'])}")

downloaded_weapons = [i for i in downloaded["items"] if i["type"] == "weapon"]
downloaded_feats = [i for i in downloaded["items"] if i["type"] == "feat"]
downloaded_spells = [i for i in downloaded["items"] if i["type"] == "spell"]

print(f"   - Weapons: {len(downloaded_weapons)}")
print(f"   - Feats: {len(downloaded_feats)}")
print(f"   - Spells: {len(downloaded_spells)}")

# Verify Bite activities survived round-trip
bite_matches = [i for i in downloaded["items"] if i["name"] == "Bite"]
if bite_matches:
    bite_downloaded = bite_matches[0]
    print(f"\n   Bite activities after round-trip: {len(bite_downloaded['system']['activities'])}")
    for act_id, act in bite_downloaded["system"]["activities"].items():
        print(f"   - {act['type']}")
        if act["type"] == "damage":
            part = act["damage"]["parts"][0]
            print(f"     damage.parts[0]: {part}")
            # Verify object structure
            if isinstance(part, dict):
                print(f"     ‚úì Object structure preserved (number={part['number']}, denomination={part['denomination']}, types={part['types']})")
            else:
                print(f"     ‚úó Array structure detected: {part}")
else:
    print("\n   ‚úó WARNING: Bite weapon MISSING after round-trip!")

# Report all downloaded item names
print("\n   Downloaded items:")
for item in downloaded["items"]:
    print(f"     - {item['name']} ({item['type']})")

# Verify no items were lost
uploaded_weapons = {i["name"] for i in foundry_json["items"] if i["type"] == "weapon"}
downloaded_weapons = {i["name"] for i in downloaded["items"] if i["type"] == "weapon"}
missing_weapons = uploaded_weapons - downloaded_weapons

if missing_weapons:
    print(f"\n‚úó ERROR: Missing weapons after round-trip: {missing_weapons}")
    print(f"   Uploaded: {uploaded_weapons}")
    print(f"   Downloaded: {downloaded_weapons}")
    print("\n" + "=" * 80)
    print("PIPELINE FAILED - ITEMS LOST")
    print("=" * 80)
    sys.exit(1)

# Check for duplicate spells
uploaded_spell_names = [i["name"] for i in foundry_json["items"] if i["type"] == "spell"]
downloaded_spell_names = [i["name"] for i in downloaded["items"] if i["type"] == "spell"]
if len(downloaded_spell_names) != len(set(downloaded_spell_names)):
    from collections import Counter
    duplicates = [name for name, count in Counter(downloaded_spell_names).items() if count > 1]
    print(f"\n‚úó WARNING: Duplicate spells detected: {duplicates}")

print("\n" + "=" * 80)
print("PIPELINE COMPLETE - ALL ITEMS PRESERVED")
print("=" * 80)
</file>

<file path="scripts/process_statblock.py">
#!/usr/bin/env python3
"""
Process any stat block through complete end-to-end pipeline.

Usage:
    python scripts/process_statblock.py data/foundry_examples/gaint_octopus.txt
    python scripts/process_statblock.py data/foundry_examples/pit_fiend.txt
"""

import sys
import json
import asyncio
import re
from pathlib import Path
from dotenv import load_dotenv

# Setup paths
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from actors.statblock_parser import parse_raw_text_to_statblock
from foundry.actors.parser import parse_stat_block_parallel
from foundry.actors.converter import convert_to_foundry
from foundry.client import FoundryClient
from foundry.actors.spell_cache import SpellCache

load_dotenv(project_root / ".env")


async def main():
    if len(sys.argv) < 2:
        print("Usage: python scripts/process_statblock.py <path_to_txt_file>")
        print("Example: python scripts/process_statblock.py data/foundry_examples/gaint_octopus.txt")
        sys.exit(1)

    input_file = Path(sys.argv[1])
    if not input_file.exists():
        print(f"Error: File not found: {input_file}")
        sys.exit(1)

    creature_name = input_file.stem.replace("_", " ").title()

    print("=" * 80)
    print(f"PROCESSING: {creature_name}")
    print("=" * 80)

    # Step 1: Read raw text
    print(f"\n1. Reading stat block from {input_file.name}...")
    with open(input_file, "r") as f:
        raw_text = f.read()
    print(f"   ‚úì Read {len(raw_text)} characters")

    # Step 2: Parse raw text ‚Üí StatBlock (using Gemini)
    print("\n2. Parsing raw text to StatBlock (using Gemini)...")
    stat_block = await parse_raw_text_to_statblock(raw_text)
    print(f"   ‚úì Parsed StatBlock:")
    print(f"     - Name: {stat_block.name}")
    print(f"     - AC: {stat_block.armor_class}, HP: {stat_block.hit_points}, CR: {stat_block.challenge_rating}")
    print(f"     - Traits: {len(stat_block.traits)}")
    print(f"     - Actions: {len(stat_block.actions)}")
    if stat_block.skills:
        print(f"     - Skills: {', '.join(f'{k.title()} +{v}' for k, v in stat_block.skills.items())}")
    if stat_block.saving_throws:
        print(f"     - Saving Throws: {', '.join(f'{k.upper()} +{v}' for k, v in stat_block.saving_throws.items())}")

    # Step 3: Load spell cache
    print("\n3. Loading spell cache...")
    spell_cache = SpellCache()
    spell_cache.load()
    print(f"   ‚úì Spell cache loaded ({len(spell_cache._spell_by_name)} spells)")

    # Step 4: StatBlock ‚Üí ParsedActorData (using parallel parser)
    print("\n4. Parsing StatBlock to ParsedActorData (parallel Gemini calls)...")
    total_calls = len(stat_block.actions) + len(stat_block.traits)
    print(f"   This will make {total_calls} parallel API calls")

    parsed_actor = await parse_stat_block_parallel(
        stat_block,
        spell_cache=spell_cache
    )

    print(f"   ‚úì Parsed to ParsedActorData:")
    print(f"     - Attacks: {len(parsed_actor.attacks)}")
    print(f"     - Traits: {len(parsed_actor.traits)}")
    print(f"     - Multiattack: {parsed_actor.multiattack is not None}")
    print(f"     - Innate Spellcasting: {parsed_actor.innate_spellcasting is not None}")

    if parsed_actor.innate_spellcasting:
        print(f"       - Spells: {len(parsed_actor.innate_spellcasting.spells)}")

    # Step 5: Convert to FoundryVTT format
    print("\n5. Converting to FoundryVTT format...")
    foundry_json, spell_uuids = convert_to_foundry(parsed_actor, spell_cache=spell_cache)
    print(f"   ‚úì Converted to FoundryVTT format")
    print(f"     - Items in payload: {len(foundry_json['items'])}")
    print(f"     - Spells via /give: {len(spell_uuids)}")

    # Item breakdown
    weapon_count = sum(1 for i in foundry_json["items"] if i["type"] == "weapon")
    feat_count = sum(1 for i in foundry_json["items"] if i["type"] == "feat")
    print(f"     - Weapons: {weapon_count}")
    print(f"     - Feats: {feat_count}")

    # Step 6: Upload to FoundryVTT
    print("\n6. Uploading to FoundryVTT...")
    client = FoundryClient(target="local")
    actor_uuid = client.actors.create_actor(foundry_json, spell_uuids=spell_uuids)
    print(f"   ‚úì Uploaded: {actor_uuid}")

    # Step 7: Download from FoundryVTT
    print("\n7. Downloading from FoundryVTT...")
    downloaded = client.actors.get_actor(actor_uuid)
    print(f"   ‚úì Downloaded: {downloaded['name']}")
    print(f"     - Total items: {len(downloaded['items'])}")

    # Item breakdown
    downloaded_weapons = [i for i in downloaded["items"] if i["type"] == "weapon"]
    downloaded_feats = [i for i in downloaded["items"] if i["type"] == "feat"]
    downloaded_spells = [i for i in downloaded["items"] if i["type"] == "spell"]

    print(f"     - Weapons: {len(downloaded_weapons)}")
    print(f"     - Feats: {len(downloaded_feats)}")
    print(f"     - Spells: {len(downloaded_spells)}")

    # Step 8: Save to file
    output_filename = f"{input_file.stem}_end_to_end.json"
    output_path = project_root / "output" / output_filename
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w") as f:
        json.dump(downloaded, f, indent=2)

    print(f"\n8. Saved to: {output_path}")
    print(f"   File size: {output_path.stat().st_size / 1024:.1f} KB")

    # Step 9: Verification
    print("\n9. Downloaded items:")
    for item in downloaded["items"]:
        print(f"     - {item['name']} ({item['type']})")

    print("\n" + "=" * 80)
    print("PIPELINE COMPLETE")
    print("=" * 80)
    print(f"\nOutput saved to: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="scripts/test_10_images.py">
#!/usr/bin/env python3
"""Test generating 10 images from one reference."""

import asyncio
import sys
import time
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from util.parallel_image_gen import generate_images_parallel


async def test_10_from_reference():
    """Test generating 10 images from single reference."""
    print("\n=== Generating 10 Images from One Reference ===\n")

    reference_path = Path("output/test_ref.png")

    if not reference_path.exists():
        print("Creating test reference image...")
        from PIL import Image
        img = Image.new('RGB', (512, 512), color='blue')
        reference_path.parent.mkdir(parents=True, exist_ok=True)
        img.save(reference_path)
        print(f"‚úì Created: {reference_path}\n")

    # 10 different style prompts
    prompts = [
        "Transform into fantasy oil painting style",
        "Convert to watercolor illustration",
        "Make it look like a pencil sketch",
        "Style as anime artwork",
        "Render as pixel art",
        "Convert to impressionist painting",
        "Make it look like a comic book panel",
        "Transform into cyberpunk art style",
        "Style as medieval manuscript illustration",
        "Render as abstract art"
    ]

    output_dir = Path("output/test_10_images")

    print(f"Reference image: {reference_path}")
    print(f"Generating {len(prompts)} styled variations...")
    print(f"Max concurrent: 10")
    print(f"Output directory: {output_dir}\n")

    start_time = time.time()

    results = await generate_images_parallel(
        prompts,
        reference_image=str(reference_path),
        save_dir=output_dir,
        make_run=True,
        max_concurrent=10
    )

    elapsed = time.time() - start_time
    successful = [r for r in results if r is not None]

    print(f"\n{'='*70}")
    print(f"‚úì Generated {len(successful)}/{len(prompts)} images in {elapsed:.2f}s")
    print(f"  Average: {elapsed/len(successful) if successful else 0:.2f}s per image")
    print(f"  Speedup: {len(successful):.1f}x (vs sequential: {len(successful)*7:.0f}s)")
    print(f"{'='*70}\n")

    # Show directory structure
    print("Output structure:")
    if output_dir.exists():
        for subdir in sorted(output_dir.iterdir()):
            if subdir.is_dir():
                print(f"  üìÅ {subdir.name}/")
                images = sorted(subdir.glob("*.png"))
                for img in images:
                    size_mb = img.stat().st_size / (1024 * 1024)
                    # Show which prompt this was
                    img_idx = int(img.stem.split('_')[1])
                    prompt_preview = prompts[img_idx][:40] + "..." if len(prompts[img_idx]) > 40 else prompts[img_idx]
                    print(f"      üñºÔ∏è  {img.name} ({size_mb:.2f} MB) - \"{prompt_preview}\"")

                total_size = sum(img.stat().st_size for img in images) / (1024 * 1024)
                print(f"\n  Total: {len(images)} images, {total_size:.2f} MB")

    return results


async def main():
    """Run test."""
    print("=" * 70)
    print("Testing 10 Image Generation from Single Reference")
    print("=" * 70)

    try:
        await test_10_from_reference()

        print("\n" + "=" * 70)
        print("‚úì Test completed successfully!")
        print("=" * 70)

    except Exception as e:
        print(f"\n‚úó Test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
</file>

<file path="scripts/test_make_run.py">
#!/usr/bin/env python3
"""Test the make_run parameter for timestamped folders."""

import asyncio
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from util.parallel_image_gen import generate_images_parallel


async def test_make_run():
    """Test make_run parameter creates timestamped folder."""
    print("\n=== Testing make_run Parameter ===\n")

    prompts = [
        "A dark cave entrance, fantasy art",
        "A medieval tavern, fantasy art"
    ]

    output_dir = Path("output/test_make_run")

    print(f"Generating {len(prompts)} images with make_run=True...")
    print(f"Base directory: {output_dir}\n")

    results = await generate_images_parallel(
        prompts,
        save_dir=output_dir,
        make_run=True,  # Create timestamped subfolder
        max_concurrent=2
    )

    print(f"\n‚úì Generated {len([r for r in results if r])} images")

    # Check directory structure
    print(f"\nDirectory structure:")
    if output_dir.exists():
        for subdir in sorted(output_dir.iterdir()):
            if subdir.is_dir():
                print(f"  üìÅ {subdir.name}/")
                for img in sorted(subdir.glob("*.png")):
                    size_mb = img.stat().st_size / (1024 * 1024)
                    print(f"      üñºÔ∏è  {img.name} ({size_mb:.2f} MB)")
    else:
        print(f"  ‚úó Directory {output_dir} not found!")

    return results


async def test_without_make_run():
    """Test without make_run (saves directly to base dir)."""
    print("\n\n=== Testing WITHOUT make_run (Direct Save) ===\n")

    prompts = ["A simple forest, fantasy art"]
    output_dir = Path("output/test_direct_save")

    print(f"Generating 1 image with make_run=False...")
    print(f"Base directory: {output_dir}\n")

    results = await generate_images_parallel(
        prompts,
        save_dir=output_dir,
        make_run=False,  # Save directly to base directory
        max_concurrent=1
    )

    print(f"\n‚úì Generated {len([r for r in results if r])} images")

    # Check directory structure
    print(f"\nDirectory structure:")
    if output_dir.exists():
        for img in sorted(output_dir.glob("*.png")):
            size_mb = img.stat().st_size / (1024 * 1024)
            print(f"  üñºÔ∏è  {img.name} ({size_mb:.2f} MB)")
    else:
        print(f"  ‚úó Directory {output_dir} not found!")

    return results


async def main():
    """Run all tests."""
    print("=" * 70)
    print("Testing make_run Parameter")
    print("=" * 70)

    try:
        # Test 1: With make_run=True (timestamped folder)
        await test_make_run()

        # Test 2: Without make_run (direct save)
        await test_without_make_run()

        print("\n" + "=" * 70)
        print("‚úì All tests completed!")
        print("=" * 70)

    except Exception as e:
        print(f"\n‚úó Test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
</file>

<file path="scripts/test_parallel_image_gen.py">
#!/usr/bin/env python3
"""Manual test script for parallel image generation utility."""

import asyncio
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from util.parallel_image_gen import generate_images_parallel, generate_variations


async def test_basic_generation():
    """Test basic parallel image generation."""
    print("\n=== Test 1: Basic Parallel Generation (3 images) ===")

    prompts = [
        "A dark mysterious cave entrance, fantasy art style",
        "A cozy medieval tavern interior with fireplace, fantasy art style",
        "A mystical forest clearing with glowing mushrooms, fantasy art style"
    ]

    print(f"Generating {len(prompts)} images with max_concurrent=3...")
    results = await generate_images_parallel(prompts, max_concurrent=3)

    successful = [r for r in results if r is not None]
    print(f"‚úì Generated {len(successful)}/{len(prompts)} images successfully")

    for i, result in enumerate(results):
        if result:
            print(f"  Image {i}: {len(result)} bytes")
        else:
            print(f"  Image {i}: FAILED")

    return results


async def test_with_save():
    """Test image generation with auto-save."""
    print("\n=== Test 2: Generation with Auto-Save (2 images) ===")

    prompts = [
        "A goblin hideout entrance, fantasy art style",
        "A dragon's lair cave, fantasy art style"
    ]

    output_dir = Path("output/test_parallel_gen")
    print(f"Generating {len(prompts)} images and saving to {output_dir}...")

    results = await generate_images_parallel(
        prompts,
        max_concurrent=2,
        save_dir=output_dir
    )

    successful = [r for r in results if r is not None]
    print(f"‚úì Generated {len(successful)}/{len(prompts)} images successfully")

    # Check saved files
    for i in range(len(prompts)):
        filepath = output_dir / f"image_{i:03d}.png"
        if filepath.exists():
            print(f"  ‚úì Saved: {filepath} ({filepath.stat().st_size} bytes)")
        else:
            print(f"  ‚úó Missing: {filepath}")

    return results


async def test_variations():
    """Test generating variations of same prompt."""
    print("\n=== Test 3: Variations (4 versions of same prompt) ===")

    prompt = "A mysterious dungeon entrance with torches, fantasy art style"
    count = 4

    print(f"Generating {count} variations of: '{prompt}'...")
    results = await generate_variations(prompt, count=count, max_concurrent=4)

    successful = [r for r in results if r is not None]
    print(f"‚úì Generated {len(successful)}/{count} variations successfully")

    return results


async def test_high_concurrency():
    """Test with many images to verify concurrency control."""
    print("\n=== Test 4: High Concurrency (10 images, max_concurrent=5) ===")

    prompts = [f"A fantasy dungeon room {i}, art style" for i in range(10)]

    print(f"Generating {len(prompts)} images with max_concurrent=5...")
    import time
    start = time.time()

    results = await generate_images_parallel(prompts, max_concurrent=5)

    elapsed = time.time() - start
    successful = [r for r in results if r is not None]

    print(f"‚úì Generated {len(successful)}/{len(prompts)} images in {elapsed:.2f}s")
    print(f"  Average: {elapsed/len(successful):.2f}s per image")

    return results


async def main():
    """Run all manual tests."""
    print("=" * 70)
    print("Manual Test Suite for Parallel Image Generation")
    print("=" * 70)

    try:
        # Test 1: Basic generation
        await test_basic_generation()

        # Test 2: With save
        await test_with_save()

        # Test 3: Variations
        await test_variations()

        # Test 4: High concurrency (optional - costs money)
        # Uncomment to test:
        # await test_high_concurrency()

        print("\n" + "=" * 70)
        print("‚úì All manual tests completed successfully!")
        print("=" * 70)

    except Exception as e:
        print(f"\n‚úó Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
</file>

<file path="scripts/test_redline_walls.py">
#!/usr/bin/env python3
"""Test redline_walls function."""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from wall_detection.redline_walls import redline_walls


async def test_redline():
    """Test redlining a battle map."""
    print("=" * 70)
    print("Testing redline_walls Function")
    print("=" * 70)

    # Create a simple test battle map
    print("\nCreating test battle map...")
    from PIL import Image, ImageDraw

    # Create a simple map with walls
    img = Image.new('RGB', (1024, 1024), color='white')
    draw = ImageDraw.Draw(img)

    # Draw some "walls" in gray
    draw.rectangle([100, 100, 900, 150], fill='gray')  # Top wall
    draw.rectangle([100, 850, 900, 900], fill='gray')  # Bottom wall
    draw.rectangle([100, 100, 150, 900], fill='gray')  # Left wall
    draw.rectangle([850, 100, 900, 900], fill='gray')  # Right wall
    draw.rectangle([400, 400, 600, 450], fill='gray')  # Interior wall

    test_map_path = Path("output/test_battle_map.png")
    test_map_path.parent.mkdir(exist_ok=True)
    img.save(test_map_path)
    print(f"‚úì Created test map: {test_map_path}")

    # Test redlining
    print("\nGenerating redlined version...")
    results = await redline_walls(
        test_map_path,
        save_dir=Path("output/redlined_walls"),
        make_run=True,
        temperature=0.5
    )

    successful = sum(1 for r in results if r is not None)
    print(f"\n‚úì Generated {successful}/{len(results)} redlined images")

    # Show output
    print("\nOutput directory:")
    output_dir = Path("output/redlined_walls")
    if output_dir.exists():
        for subdir in sorted(output_dir.iterdir()):
            if subdir.is_dir():
                print(f"  üìÅ {subdir.name}/")
                for img in sorted(subdir.glob("*.png")):
                    size_mb = img.stat().st_size / (1024 * 1024)
                    print(f"      üñºÔ∏è  {img.name} ({size_mb:.2f} MB)")

    print("\n" + "=" * 70)
    print("‚úì Test completed!")
    print(f"View results in: {output_dir}")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(test_redline())
</file>

<file path="scripts/test_reference_image.py">
#!/usr/bin/env python3
"""Test the reference_image parameter."""

import asyncio
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from util.parallel_image_gen import generate_images_parallel


async def test_reference_image():
    """Test using a reference image for generation."""
    print("\n=== Testing reference_image Parameter ===\n")

    # Use a simple test image
    reference_path = Path("output/test_ref.png")

    if not reference_path.exists():
        print(f"‚ö†Ô∏è  Reference image not found: {reference_path}")
        print("Run test_make_run.py first to generate a reference image")
        return

    prompts = [
        "Transform this into a vibrant fantasy painting",
        "Make this look like a watercolor illustration"
    ]

    output_dir = Path("output/test_reference_image")

    print(f"Reference image: {reference_path}")
    print(f"Generating {len(prompts)} images conditioned on reference...")
    print(f"Output directory: {output_dir}\n")

    results = await generate_images_parallel(
        prompts,
        reference_image=str(reference_path),
        save_dir=output_dir,
        make_run=True,
        max_concurrent=2
    )

    successful = [r for r in results if r is not None]
    print(f"\n‚úì Generated {len(successful)}/{len(prompts)} images")

    # Check directory structure
    print(f"\nDirectory structure:")
    if output_dir.exists():
        for subdir in sorted(output_dir.iterdir()):
            if subdir.is_dir():
                print(f"  üìÅ {subdir.name}/")
                for img in sorted(subdir.glob("*.png")):
                    size_mb = img.stat().st_size / (1024 * 1024)
                    print(f"      üñºÔ∏è  {img.name} ({size_mb:.2f} MB)")

    return results


async def main():
    """Run test."""
    print("=" * 70)
    print("Testing Reference Image Parameter")
    print("=" * 70)

    try:
        await test_reference_image()

        print("\n" + "=" * 70)
        print("‚úì Test completed!")
        print("=" * 70)

    except Exception as e:
        print(f"\n‚úó Test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
</file>

<file path="src/actors/__init__.py">
"""Actor and NPC extraction modules."""

from .models import StatBlock, NPC

__all__ = ["StatBlock", "NPC"]
</file>

<file path="src/actors/extract_npcs.py">
"""Extract named NPCs from generated XML using Gemini."""

import logging
import json
from typing import List, Optional
from util.gemini import GeminiAPI
from .models import NPC

logger = logging.getLogger(__name__)


def identify_npcs_with_gemini(
    xml_content: str,
    api: Optional[GeminiAPI] = None
) -> List[NPC]:
    """
    Identify named NPCs from XML content using Gemini.

    Analyzes the XML structure and narrative text to find named characters
    with plot relevance. Links NPCs to their creature stat blocks if available.

    Args:
        xml_content: XML content to analyze
        api: Optional GeminiAPI instance (creates new one if not provided)

    Returns:
        List of NPC objects

    Raises:
        RuntimeError: If Gemini API call fails
    """
    if api is None:
        api = GeminiAPI()

    logger.debug(f"Analyzing XML for NPCs (length: {len(xml_content)} chars)")

    # Construct NPC identification prompt
    prompt = f"""Analyze this D&D module XML and identify all named NPCs (non-player characters).

For each named NPC, extract:
- name (string): The NPC's name (e.g., "Klarg", "Sildar Hallwinter")
- creature_stat_block_name (string): The creature type/stat block this NPC uses (e.g., "Bugbear", "Human Fighter")
  - Look for nearby <stat_block> tags or descriptions like "Klarg, a bugbear" ‚Üí "Bugbear"
  - Use the stat block name exactly as it appears in <stat_block name="...">
- description (string): Brief physical or personality description (1-2 sentences)
- plot_relevance (string): Why this NPC matters to the story (1-2 sentences)
- location (string, optional): Where the NPC is found (e.g., "Cragmaw Hideout", "Area 6")
- first_appearance_section (string, optional): Section where NPC first appears (e.g., "Chapter 1 ‚Üí Goblin Ambush")

IMPORTANT:
- Only include NAMED characters (e.g., "Klarg", "Sildar"), NOT generic enemies ("goblins", "bandits")
- Link NPCs to stat blocks by finding nearby <stat_block name="..."> tags or creature type mentions
- If a stat block isn't found, infer the creature type from context (e.g., "human fighter" ‚Üí "Human Fighter")

Return ONLY valid JSON array with these exact field names:
[
  {{
    "name": "Klarg",
    "creature_stat_block_name": "Bugbear",
    "description": "Leader of the Cragmaw goblins, wears a tattered cloak",
    "plot_relevance": "Guards stolen supplies, has taken Sildar prisoner",
    "location": "Cragmaw Hideout, Area 6",
    "first_appearance_section": "Chapter 1 ‚Üí Cragmaw Hideout"
  }}
]

If no named NPCs found, return empty array: []

Do not include markdown formatting or explanation.

XML content:
{xml_content}"""

    try:
        # Call Gemini
        response = api.generate_content(prompt)
        response_text = response.text.strip()

        # Remove markdown code blocks if present
        if response_text.startswith("```"):
            lines = response_text.split("\n")
            response_text = "\n".join(lines[1:-1])
            if response_text.startswith("json"):
                response_text = response_text[4:].strip()

        # Parse JSON
        parsed_data = json.loads(response_text)

        # Validate and create NPC objects
        npcs = []
        for npc_data in parsed_data:
            try:
                npc = NPC(**npc_data)
                npcs.append(npc)
            except Exception as e:
                logger.warning(f"Failed to validate NPC data: {e}")
                logger.debug(f"Invalid NPC data: {npc_data}")
                continue

        logger.info(f"Identified {len(npcs)} NPC(s) from XML")
        return npcs

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse Gemini response as JSON: {e}")
        logger.debug(f"Response text: {response_text}")
        raise RuntimeError(f"Invalid JSON from Gemini: {e}") from e

    except Exception as e:
        logger.error(f"Failed to extract NPCs: {e}")
        raise RuntimeError(f"NPC extraction failed: {e}") from e
</file>

<file path="src/actors/models.py">
"""Pydantic models for D&D 5e stat blocks and NPCs."""

from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Dict, List
from pydantic import BaseModel, field_validator


class StatBlock(BaseModel):
    """D&D 5e stat block structure with pre-split sections for parallel processing."""

    # Always preserve original text
    name: str
    raw_text: str

    # Required D&D 5e fields
    armor_class: int
    hit_points: int
    challenge_rating: float

    # Optional structured fields
    size: Optional[str] = None
    type: Optional[str] = None
    alignment: Optional[str] = None
    abilities: Optional[Dict[str, int]] = None  # STR, DEX, CON, INT, WIS, CHA
    speed: Optional[str] = None
    senses: Optional[str] = None
    languages: Optional[str] = None

    # Proficiencies (extracted from raw text)
    saving_throws: Optional[Dict[str, int]] = None  # {"dex": 8, "con": 13, "wis": 10}
    skills: Optional[Dict[str, int]] = None  # {"perception": 4, "stealth": 5}

    # Damage modifiers
    damage_resistances: Optional[str] = None  # e.g., "Fire, Cold; Bludgeoning from Nonmagical Attacks"
    damage_immunities: Optional[str] = None
    damage_vulnerabilities: Optional[str] = None
    condition_immunities: Optional[str] = None  # e.g., "Poisoned, Charmed"

    # Split into lists for parallel processing
    # Each item is a complete entry (e.g., "Scimitar. Melee Weapon Attack: +4 to hit...")
    traits: List[str] = []  # Special abilities (includes innate spellcasting if present)
    actions: List[str] = []  # Actions (includes multiattack if present)
    reactions: List[str] = []  # Reactions
    legendary_actions: List[str] = []  # Legendary actions

    @field_validator('armor_class')
    @classmethod
    def validate_ac(cls, v: int) -> int:
        """Validate armor class is in valid range."""
        if not (1 <= v <= 30):
            raise ValueError(f"Armor class {v} out of range (1-30)")
        return v

    @field_validator('hit_points')
    @classmethod
    def validate_hp(cls, v: int) -> int:
        """Validate hit points are positive."""
        if v < 1:
            raise ValueError(f"Hit points must be positive, got {v}")
        return v

    @field_validator('challenge_rating')
    @classmethod
    def validate_cr(cls, v: float) -> float:
        """Validate challenge rating is valid."""
        valid_crs = [0, 0.125, 0.25, 0.5] + list(range(1, 31))
        if v not in valid_crs:
            raise ValueError(f"Invalid challenge rating: {v}")
        return v


class NPC(BaseModel):
    """Named NPC with plot context and stat block reference."""

    name: str
    creature_stat_block_name: str  # Name of creature stat block this NPC uses
    description: str
    plot_relevance: str
    location: Optional[str] = None
    first_appearance_section: Optional[str] = None  # Where NPC first appears in module


@dataclass
class ActorCreationResult:
    """Complete result from actor creation pipeline with all intermediate outputs."""

    # Input
    description: str
    challenge_rating: Optional[float]

    # Intermediate outputs
    raw_stat_block_text: str
    stat_block: StatBlock
    parsed_actor_data: 'ParsedActorData'  # Forward reference since ParsedActorData is in foundry/actors/models.py

    # Final output
    foundry_uuid: str

    # File paths (for debugging/inspection)
    output_dir: Path

    # Metadata
    timestamp: str  # ISO format timestamp
    model_used: str  # e.g., "gemini-2.0-flash"

    # Optional file paths
    raw_text_file: Optional[Path] = None
    stat_block_file: Optional[Path] = None
    parsed_data_file: Optional[Path] = None
    foundry_json_file: Optional[Path] = None
</file>

<file path="src/actors/orchestrate.py">
"""Orchestrate full actor creation pipeline from description to FoundryVTT."""

import asyncio
import json
import logging
import os
from pathlib import Path
from datetime import datetime
from typing import Union, Optional, List
from pydantic import BaseModel

from actors.generate_actor_file import generate_actor_description
from actors.generate_actor_biography import generate_actor_biography
from actors.statblock_parser import parse_raw_text_to_statblock
from actors.models import ActorCreationResult
from foundry.actors.parser import parse_stat_block_parallel
from foundry.actors.converter import convert_to_foundry
from foundry.actors.spell_cache import SpellCache
from foundry.icon_cache import IconCache
from foundry.client import FoundryClient

logger = logging.getLogger(__name__)


def _create_output_directory(base_dir: str = "output/runs") -> Path:
    """
    Create timestamped output directory for actor creation files.

    Args:
        base_dir: Base directory for runs (default: "output/runs")

    Returns:
        Path to created directory: output/runs/<timestamp>/actors/

    Example:
        output/runs/20241103_143022/actors/
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(base_dir) / timestamp / "actors"
    output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"Created output directory: {output_dir}")
    return output_dir


def _save_intermediate_file(
    content: Union[str, dict, BaseModel],
    filepath: Path,
    description: str = "file"
) -> Path:
    """
    Save intermediate output to file (text or JSON).

    Args:
        content: Content to save (str for text, dict/BaseModel for JSON)
        filepath: Full path to save file
        description: Human-readable description for logging

    Returns:
        Path to saved file

    Raises:
        IOError: If file write fails

    Examples:
        # Save raw text
        _save_intermediate_file("Goblin\\nSmall humanoid...",
                               output_dir / "raw_text.txt",
                               "raw stat block text")

        # Save Pydantic model as JSON
        _save_intermediate_file(stat_block,
                               output_dir / "stat_block.json",
                               "StatBlock model")
    """
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)

        if isinstance(content, str):
            # Save as text file
            filepath.write_text(content, encoding='utf-8')
        elif isinstance(content, BaseModel):
            # Save Pydantic model as JSON
            filepath.write_text(content.model_dump_json(indent=2), encoding='utf-8')
        elif isinstance(content, dict):
            # Save dict as JSON
            filepath.write_text(json.dumps(content, indent=2), encoding='utf-8')
        else:
            raise ValueError(f"Unsupported content type: {type(content)}")

        logger.debug(f"Saved {description} to: {filepath}")
        return filepath

    except Exception as e:
        logger.error(f"Failed to save {description} to {filepath}: {e}")
        raise IOError(f"Failed to save {description}: {e}") from e


async def create_actor_from_description(
    description: str,
    challenge_rating: Optional[float] = None,
    model_name: str = "gemini-2.0-flash",
    output_dir_base: str = "output/runs",
    spell_cache: Optional[SpellCache] = None,
    icon_cache: Optional[IconCache] = None,
    foundry_client: Optional[FoundryClient] = None
) -> ActorCreationResult:
    """
    Create a complete D&D 5e actor in FoundryVTT from a natural language description.

    This function orchestrates the full pipeline:
    1. Generate raw stat block text using Gemini
    2. Parse raw text to StatBlock model
    3. Parse StatBlock to detailed ParsedActorData
    4. Generate flavorful biography using Gemini
    5. Convert to FoundryVTT JSON format
    6. Upload to FoundryVTT server

    All intermediate outputs are saved to disk for debugging.

    Args:
        description: Natural language description of the actor
                    Example: "A fierce red dragon wyrmling with fire breath"
        challenge_rating: Optional CR (0.125, 0.25, 0.5, 1-30). If None, Gemini determines it.
        model_name: Gemini model to use (default: "gemini-2.0-flash")
        output_dir_base: Base directory for output (default: "output/runs")
        spell_cache: Optional pre-loaded SpellCache (will create if None)
        foundry_client: Optional FoundryClient (will create if None)

    Returns:
        ActorCreationResult with all intermediate outputs and final FoundryVTT UUID

    Raises:
        ValueError: If any parsing step fails
        RuntimeError: If Gemini API calls fail
        IOError: If file save fails

    Example:
        result = await create_actor_from_description(
            "A cunning goblin assassin with poisoned daggers",
            challenge_rating=2.0
        )
        print(f"Actor created: {result.foundry_uuid}")
        print(f"Saved to: {result.output_dir}")
    """
    timestamp_str = datetime.now().isoformat()

    # Step 0: Create output directory
    logger.info(f"Starting actor creation: {description[:50]}...")
    output_dir = _create_output_directory(output_dir_base)

    try:
        # Step 1: Generate raw stat block text
        logger.info("Step 1/5: Generating stat block text with Gemini...")
        raw_text = await generate_actor_description(
            description=description,
            challenge_rating=challenge_rating,
            model_name=model_name
        )
        raw_text_file = _save_intermediate_file(
            raw_text,
            output_dir / "01_raw_stat_block.txt",
            "raw stat block text"
        )

        # Step 2: Parse to StatBlock model
        logger.info("Step 2/5: Parsing stat block to StatBlock model...")
        stat_block = await parse_raw_text_to_statblock(raw_text, model_name=model_name)
        stat_block_file = _save_intermediate_file(
            stat_block,
            output_dir / "02_stat_block.json",
            "StatBlock model"
        )

        # Step 3: Parse to detailed ParsedActorData
        logger.info("Step 3/6: Parsing to detailed ParsedActorData...")
        parsed_actor = await parse_stat_block_parallel(stat_block)
        parsed_data_file = _save_intermediate_file(
            parsed_actor,
            output_dir / "03_parsed_actor_data.json",
            "ParsedActorData model"
        )

        # Step 4: Generate biography
        logger.info("Step 4/6: Generating actor biography...")
        biography = await generate_actor_biography(parsed_actor, model_name=model_name)
        # Update parsed_actor with biography
        parsed_actor = parsed_actor.model_copy(update={"biography": biography})
        # Re-save with biography included
        _save_intermediate_file(
            parsed_actor,
            output_dir / "03_parsed_actor_data.json",
            "ParsedActorData model (with biography)"
        )

        # Step 5: Convert to FoundryVTT format
        logger.info("Step 5/6: Converting to FoundryVTT format...")
        if spell_cache is None:
            spell_cache = SpellCache()
            spell_cache.load()

        if icon_cache is None:
            logger.info("Loading icon cache...")
            icon_cache = IconCache()
            icon_cache.load()

        actor_json, spell_uuids = await convert_to_foundry(
            parsed_actor,
            spell_cache=spell_cache,
            icon_cache=icon_cache,
            use_ai_icons=True  # Enable AI-powered icon selection
        )
        foundry_json_file = _save_intermediate_file(
            actor_json,
            output_dir / "04_foundry_actor.json",
            "FoundryVTT actor JSON"
        )

        # Step 6: Upload to FoundryVTT
        logger.info("Step 6/6: Uploading to FoundryVTT...")
        if foundry_client is None:
            foundry_client = FoundryClient()

        actor_uuid = foundry_client.actors.create_actor(
            actor_data=actor_json,
            spell_uuids=spell_uuids
        )

        logger.info(f"‚úì Actor created successfully: {actor_uuid}")
        logger.info(f"  Output directory: {output_dir}")

        # Return complete result
        return ActorCreationResult(
            description=description,
            challenge_rating=challenge_rating,
            raw_stat_block_text=raw_text,
            stat_block=stat_block,
            parsed_actor_data=parsed_actor,
            foundry_uuid=actor_uuid,
            output_dir=output_dir,
            raw_text_file=raw_text_file,
            stat_block_file=stat_block_file,
            parsed_data_file=parsed_data_file,
            foundry_json_file=foundry_json_file,
            timestamp=timestamp_str,
            model_used=model_name
        )

    except Exception as e:
        logger.error(f"Actor creation failed: {e}")
        raise


def create_actor_from_description_sync(
    description: str,
    challenge_rating: Optional[float] = None,
    model_name: str = "gemini-2.0-flash",
    output_dir_base: str = "output/runs",
    spell_cache: Optional[SpellCache] = None,
    icon_cache: Optional[IconCache] = None,
    foundry_client: Optional[FoundryClient] = None
) -> ActorCreationResult:
    """
    Synchronous wrapper for create_actor_from_description().

    This is a convenience function that runs the async pipeline in a synchronous context.
    All parameters and return values are identical to the async version.

    Args:
        description: Natural language description of the actor
        challenge_rating: Optional CR (0.125, 0.25, 0.5, 1-30). If None, Gemini determines it.
        model_name: Gemini model to use (default: "gemini-2.0-flash")
        output_dir_base: Base directory for output (default: "output/runs")
        spell_cache: Optional pre-loaded SpellCache
        foundry_client: Optional FoundryClient

    Returns:
        ActorCreationResult with all outputs and FoundryVTT UUID

    Example:
        # Simple synchronous usage
        result = create_actor_from_description_sync(
            "A cunning goblin assassin",
            challenge_rating=2.0
        )
        print(f"Created: {result.foundry_uuid}")
    """
    return asyncio.run(
        create_actor_from_description(
            description=description,
            challenge_rating=challenge_rating,
            model_name=model_name,
            output_dir_base=output_dir_base,
            spell_cache=spell_cache,
            icon_cache=icon_cache,
            foundry_client=foundry_client
        )
    )


async def create_actors_batch(
    descriptions: List[str],
    challenge_ratings: Optional[List[Optional[float]]] = None,
    model_name: str = "gemini-2.0-flash",
    output_dir_base: str = "output/runs",
    spell_cache: Optional[SpellCache] = None,
    icon_cache: Optional[IconCache] = None,
    foundry_client: Optional[FoundryClient] = None
) -> List[Union[ActorCreationResult, Exception]]:
    """
    Create multiple actors in parallel from a list of descriptions.

    This function processes all actors concurrently using asyncio.gather().
    Individual failures are captured and returned in the results list.

    Args:
        descriptions: List of natural language descriptions
        challenge_ratings: Optional list of CRs (same length as descriptions, or None)
        model_name: Gemini model to use (default: "gemini-2.0-flash")
        output_dir_base: Base directory for output (default: "output/runs")
        spell_cache: Optional pre-loaded SpellCache (recommended for batch processing)
        foundry_client: Optional FoundryClient (recommended for batch processing)

    Returns:
        List of ActorCreationResult or Exception objects (one per description)
        Successful results are ActorCreationResult instances
        Failed results are Exception instances

    Example:
        descriptions = [
            "A fierce red dragon wyrmling",
            "A cunning goblin assassin",
            "An ancient treant guardian"
        ]
        crs = [2.0, 1.0, 9.0]

        # Pre-load shared resources for efficiency
        spell_cache = SpellCache()
        spell_cache.load()
        client = FoundryClient()

        results = await create_actors_batch(
            descriptions,
            challenge_ratings=crs,
            spell_cache=spell_cache,
            foundry_client=client
        )

        # Process results
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"Failed: {descriptions[i]} - {result}")
            else:
                print(f"Created: {result.foundry_uuid}")
    """
    # Validate inputs
    if challenge_ratings is not None and len(challenge_ratings) != len(descriptions):
        raise ValueError("challenge_ratings must be same length as descriptions")

    # Default challenge_ratings to all None
    if challenge_ratings is None:
        challenge_ratings = [None] * len(descriptions)

    # Pre-load shared resources if not provided
    if spell_cache is None:
        logger.info("Loading spell cache for batch processing...")
        spell_cache = SpellCache()
        spell_cache.load()

    if icon_cache is None:
        logger.info("Loading icon cache for batch processing...")
        icon_cache = IconCache()
        icon_cache.load()

    if foundry_client is None:
        logger.info("Creating FoundryVTT client for batch processing...")
        foundry_client = FoundryClient()

    logger.info(f"Starting batch creation of {len(descriptions)} actors...")

    # Create tasks for all actors
    tasks = []
    for desc, cr in zip(descriptions, challenge_ratings):
        task = create_actor_from_description(
            description=desc,
            challenge_rating=cr,
            model_name=model_name,
            output_dir_base=output_dir_base,
            spell_cache=spell_cache,
            icon_cache=icon_cache,
            foundry_client=foundry_client
        )
        tasks.append(task)

    # Run all tasks concurrently, capturing exceptions
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Log summary
    successes = sum(1 for r in results if not isinstance(r, Exception))
    failures = len(results) - successes
    logger.info(f"Batch complete: {successes} succeeded, {failures} failed")

    return results


def create_actors_batch_sync(
    descriptions: List[str],
    challenge_ratings: Optional[List[Optional[float]]] = None,
    model_name: str = "gemini-2.0-flash",
    output_dir_base: str = "output/runs",
    spell_cache: Optional[SpellCache] = None,
    icon_cache: Optional[IconCache] = None,
    foundry_client: Optional[FoundryClient] = None
) -> List[Union[ActorCreationResult, Exception]]:
    """
    Synchronous wrapper for create_actors_batch().

    See create_actors_batch() for full documentation.
    """
    return asyncio.run(
        create_actors_batch(
            descriptions=descriptions,
            challenge_ratings=challenge_ratings,
            model_name=model_name,
            output_dir_base=output_dir_base,
            spell_cache=spell_cache,
            icon_cache=icon_cache,
            foundry_client=foundry_client
        )
    )
</file>

<file path="src/actors/process_actors.py">
"""Orchestrate actor processing workflow for a run directory."""

import logging
from pathlib import Path
from typing import Dict, Any, Literal
from foundry.client import FoundryClient
from util.gemini import GeminiAPI
from .extract_stat_blocks import extract_and_parse_stat_blocks
from .extract_npcs import identify_npcs_with_gemini

logger = logging.getLogger(__name__)


def process_actors_for_run(
    run_dir: str,
    target: Literal["local", "forge"] = "local"
) -> Dict[str, Any]:
    """
    Process all actors for a run directory.

    Complete workflow:
    1. Extract and parse stat blocks from XML files
    2. Extract NPCs from XML files
    3. Create/lookup creature actors in FoundryVTT
    4. Create NPC actors with stat block links

    Args:
        run_dir: Path to run directory (contains documents/ folder)
        target: FoundryVTT target environment

    Returns:
        Dict with processing statistics

    Raises:
        FileNotFoundError: If run directory doesn't exist
        RuntimeError: If processing fails
    """
    run_path = Path(run_dir)
    if not run_path.exists():
        raise FileNotFoundError(f"Run directory not found: {run_dir}")

    documents_dir = run_path / "documents"
    if not documents_dir.exists():
        raise FileNotFoundError(f"Documents directory not found: {documents_dir}")

    logger.info(f"Processing actors for run: {run_dir}")

    # Initialize APIs
    gemini_api = GeminiAPI()
    foundry_client = FoundryClient(target=target)

    # Statistics
    stats = {
        "stat_blocks_found": 0,
        "stat_blocks_created": 0,
        "stat_blocks_reused": 0,
        "npcs_found": 0,
        "npcs_created": 0,
        "errors": []
    }

    # Step 1: Extract and parse stat blocks from all XML files
    logger.info("Step 1: Extracting stat blocks from XML files")
    all_stat_blocks = []
    xml_files = list(documents_dir.glob("*.xml"))

    for xml_file in xml_files:
        try:
            stat_blocks = extract_and_parse_stat_blocks(str(xml_file), api=gemini_api)
            all_stat_blocks.extend(stat_blocks)
            logger.info(f"Found {len(stat_blocks)} stat block(s) in {xml_file.name}")
        except Exception as e:
            logger.error(f"Failed to extract stat blocks from {xml_file.name}: {e}")
            stats["errors"].append(f"Stat block extraction failed for {xml_file.name}: {e}")

    stats["stat_blocks_found"] = len(all_stat_blocks)
    logger.info(f"Total stat blocks found: {len(all_stat_blocks)}")

    # Step 2: Extract NPCs from all XML files
    logger.info("Step 2: Extracting NPCs from XML files")
    all_npcs = []

    for xml_file in xml_files:
        try:
            with open(xml_file, 'r') as f:
                xml_content = f.read()

            npcs = identify_npcs_with_gemini(xml_content, api=gemini_api)
            all_npcs.extend(npcs)
            logger.info(f"Found {len(npcs)} NPC(s) in {xml_file.name}")
        except Exception as e:
            logger.error(f"Failed to extract NPCs from {xml_file.name}: {e}")
            stats["errors"].append(f"NPC extraction failed for {xml_file.name}: {e}")

    stats["npcs_found"] = len(all_npcs)
    logger.info(f"Total NPCs found: {len(all_npcs)}")

    # Step 3: Create/lookup creature actors
    logger.info("Step 3: Creating creature actors in FoundryVTT")
    creature_uuid_map = {}  # Map creature name (lowercase) ‚Üí UUID

    for stat_block in all_stat_blocks:
        try:
            # Search compendium first
            existing_uuid = foundry_client.search_actor(stat_block.name)

            if existing_uuid:
                logger.info(f"Found existing actor in compendium: {stat_block.name}")
                creature_uuid_map[stat_block.name.lower()] = existing_uuid
                stats["stat_blocks_reused"] += 1
            else:
                # Create new actor
                logger.info(f"Creating new creature actor: {stat_block.name}")
                new_uuid = foundry_client.create_creature_actor(stat_block)
                creature_uuid_map[stat_block.name.lower()] = new_uuid
                stats["stat_blocks_created"] += 1

        except Exception as e:
            logger.error(f"Failed to process creature actor '{stat_block.name}': {e}")
            stats["errors"].append(f"Creature actor creation failed for {stat_block.name}: {e}")

    # Step 4: Create NPC actors
    logger.info("Step 4: Creating NPC actors in FoundryVTT")

    for npc in all_npcs:
        try:
            # Get stat block UUID if available (case-insensitive lookup)
            stat_block_uuid = creature_uuid_map.get(npc.creature_stat_block_name.lower())

            if not stat_block_uuid:
                # Try searching compendium for the creature type
                stat_block_uuid = foundry_client.search_actor(npc.creature_stat_block_name)

            if not stat_block_uuid:
                logger.warning(
                    f"NPC '{npc.name}' references unknown creature '{npc.creature_stat_block_name}', "
                    f"creating without stat block link"
                )

            # Create NPC actor
            logger.info(f"Creating NPC actor: {npc.name}")
            foundry_client.create_npc_actor(npc, stat_block_uuid=stat_block_uuid)
            stats["npcs_created"] += 1

        except Exception as e:
            logger.error(f"Failed to create NPC actor '{npc.name}': {e}")
            stats["errors"].append(f"NPC actor creation failed for {npc.name}: {e}")

    # Summary
    logger.info("=" * 60)
    logger.info("Actor processing complete!")
    logger.info(f"Stat blocks: {stats['stat_blocks_found']} found, "
                f"{stats['stat_blocks_created']} created, "
                f"{stats['stat_blocks_reused']} reused")
    logger.info(f"NPCs: {stats['npcs_found']} found, {stats['npcs_created']} created")
    if stats["errors"]:
        logger.warning(f"Errors encountered: {len(stats['errors'])}")
    logger.info("=" * 60)

    return stats
</file>

<file path="src/foundry/actors/__init__.py">
"""FoundryVTT actor operations and models."""

from .manager import ActorManager
from .models import (
    DamageFormula,
    SavingThrow,
    Attack,
    Trait,
    Spell,
    SkillProficiency,
    DamageModification,
    ParsedActorData
)
from .spell_cache import SpellCache
from .converter import convert_to_foundry

__all__ = [
    "ActorManager",
    "DamageFormula",
    "SavingThrow",
    "Attack",
    "Trait",
    "Spell",
    "SkillProficiency",
    "DamageModification",
    "ParsedActorData",
    "SpellCache",
    "convert_to_foundry",
]
</file>

<file path="src/foundry/actors/deduplicate.py">
"""
Deduplicate actors by name, prioritizing official sources.

Priority order:
1. Player's Handbook (dnd-players-handbook)
2. D&D 5e 2024 rules (dnd5e.actors24)
3. D&D 5e SRD (dnd5e.monsters)
4. Other sources
"""

import logging
from typing import Dict, List
from collections import defaultdict

logger = logging.getLogger(__name__)


def get_source_priority(uuid: str) -> int:
    """
    Get priority score for an actor's source (lower = higher priority).

    Args:
        uuid: Actor UUID like "Compendium.dnd5e.monsters.abc123"

    Returns:
        Priority score (0 = highest priority)
    """
    if 'dnd-players-handbook' in uuid:
        return 0  # Highest priority - official Player's Handbook
    elif '.actors24' in uuid or 'dnd5e.actors24' in uuid:
        return 1  # 2024 rules update
    elif 'dnd5e.monsters' in uuid or 'dnd5e.' in uuid:
        return 2  # Classic D&D 5e SRD/monsters
    else:
        return 3  # Other sources (homebrew, modules, etc.)


def deduplicate_actors(
    actors: List[Dict],
    dedupe_key: str = 'name',
    verbose: bool = True
) -> List[Dict]:
    """
    Deduplicate actors by a key (typically name), keeping highest priority source.

    Args:
        actors: List of actor dicts with at least 'uuid' and dedupe_key fields
        dedupe_key: Field to use for deduplication (default: 'name')
        verbose: Log duplicate removals

    Returns:
        Deduplicated list of actors sorted by dedupe_key
    """
    # Group by dedupe_key
    actors_by_key = defaultdict(list)
    for actor in actors:
        key = actor.get(dedupe_key, '').strip()
        if key:
            actors_by_key[key].append(actor)

    # For each key, pick the highest priority source
    deduplicated = []
    duplicates_removed = 0

    for key, actor_variants in actors_by_key.items():
        # Sort by priority (lower score = higher priority)
        actor_variants_sorted = sorted(
            actor_variants,
            key=lambda a: get_source_priority(a.get('uuid', ''))
        )

        # Take the first one (highest priority)
        best_actor = actor_variants_sorted[0]
        deduplicated.append(best_actor)

        # Log if we had duplicates
        if len(actor_variants) > 1:
            duplicates_removed += len(actor_variants) - 1

            if verbose:
                sources = [a.get('uuid', '').split('.')[1] if '.' in a.get('uuid', '') else 'unknown'
                          for a in actor_variants_sorted[1:]]
                logger.debug(
                    f"Removed {len(actor_variants) - 1} duplicate(s) of '{key}' "
                    f"(kept {actor_variants_sorted[0].get('uuid', '').split('.')[1]}, "
                    f"removed {', '.join(sources)})"
                )

    if verbose and duplicates_removed > 0:
        logger.info(f"Removed {duplicates_removed} duplicate actors")

    # Sort by dedupe key
    deduplicated_sorted = sorted(deduplicated, key=lambda a: a.get(dedupe_key, '').lower())

    return deduplicated_sorted


def get_source_stats(actors: List[Dict]) -> Dict[str, int]:
    """
    Get statistics about actor sources.

    Args:
        actors: List of actor dicts with 'uuid' field

    Returns:
        Dict mapping source names to counts
    """
    stats = defaultdict(int)

    for actor in actors:
        uuid = actor.get('uuid', '')

        if 'dnd-players-handbook' in uuid:
            stats["Player's Handbook"] += 1
        elif '.actors24' in uuid or 'dnd5e.actors24' in uuid:
            stats["D&D 5e 2024"] += 1
        elif 'dnd5e.monsters' in uuid:
            stats["D&D 5e SRD"] += 1
        else:
            stats["Other"] += 1

    return dict(stats)
</file>

<file path="src/foundry/actors/models.py">
"""Foundry-specific actor models for detailed parsing."""

from typing import List, Optional, Literal, Dict
from pydantic import BaseModel, Field, ConfigDict


class DamageFormula(BaseModel):
    """Dice damage formula (FoundryVTT format)."""

    model_config = ConfigDict(frozen=True)

    number: int  # Dice count (e.g., 3 for 3d6)
    denomination: int  # Dice size (e.g., 6 for d6)
    bonus: str = ""  # Flat modifier (e.g., "+2")
    type: str  # Damage type: "piercing", "slashing", "fire", etc.


class SavingThrow(BaseModel):
    """Saving throw triggered by ability."""

    ability: Literal["str", "dex", "con", "int", "wis", "cha"]
    dc: Optional[int] = None  # Explicit DC if specified
    dc_ability: Optional[str] = None  # Ability for DC calculation (e.g., "cha")
    on_failure: str  # Effect description on failure
    on_success: Optional[str] = None  # Effect on success (if specified)


class Attack(BaseModel):
    """Parsed attack (weapon or spell attack)."""

    name: str
    attack_type: Literal["melee", "ranged", "melee_ranged"]
    attack_bonus: int  # To-hit bonus
    reach: Optional[int] = None  # Melee reach in feet (5, 10, 15, etc.)
    range_short: Optional[int] = None  # Ranged short range
    range_long: Optional[int] = None  # Ranged long range
    damage: List[DamageFormula]  # Primary + additional damage
    additional_effects: Optional[str] = None  # e.g., "target is grappled (escape DC 16)"

    # NEW: Optional attack save (e.g., Pit Fiend Bite poison save)
    attack_save: Optional['AttackSave'] = None

    @property
    def range(self) -> Optional[int]:
        """Alias for range_short for backwards compatibility."""
        return self.range_short


class Trait(BaseModel):
    """Parsed trait/feature."""

    name: str
    description: str
    activation: Literal["action", "bonus", "reaction", "passive", "legendary"] = "passive"
    uses: Optional[int] = None  # Limited uses per day/rest
    recharge: Optional[str] = None  # e.g., "5-6" for recharge on 5 or 6
    saving_throw: Optional[SavingThrow] = None


class Multiattack(BaseModel):
    """Multiattack action."""

    name: str = "Multiattack"
    description: str
    num_attacks: Optional[int] = None
    activation: Literal["action", "bonus", "reaction", "passive"] = "action"

    model_config = ConfigDict(frozen=True)


class Spell(BaseModel):
    """Parsed spell reference with compendium UUID."""

    name: str
    level: int  # 0-9
    uuid: Optional[str] = None  # Compendium UUID (resolved during parsing)
    school: Optional[str] = None  # "evo", "abj", etc.
    casting_time: Optional[str] = None


class InnateSpell(BaseModel):
    """An innate spell with usage frequency."""

    name: str
    frequency: str  # "at will", "3/day", "1/day", etc.
    uses: Optional[int] = None  # Max uses per day
    uuid: Optional[str] = None  # Compendium UUID (resolved during parsing)

    model_config = ConfigDict(frozen=True)


class InnateSpellcasting(BaseModel):
    """Innate spellcasting ability."""

    ability: Optional[str] = None  # "charisma", "intelligence", etc. (optional if not specified)
    save_dc: Optional[int] = None
    attack_bonus: Optional[int] = None
    spells: List[InnateSpell] = Field(default_factory=list)

    model_config = ConfigDict(frozen=True)


class AttackSave(BaseModel):
    """A saving throw associated with an attack."""

    ability: Optional[str] = None  # "con", "dex", "wis", etc. (None for automatic effects)
    dc: Optional[int] = None       # Difficulty Class (None for automatic effects)

    # Damage on failed save
    damage: List[DamageFormula] = Field(default_factory=list)
    on_save: Literal["half", "none", "full", "negates"] = "none"  # Damage on successful save

    # For ongoing effects (e.g., poison damage each turn)
    ongoing_damage: Optional[List[DamageFormula]] = None
    duration_rounds: Optional[int] = None

    effect_description: Optional[str] = None  # e.g., "poisoned condition"

    model_config = ConfigDict(frozen=True)


class SkillProficiency(BaseModel):
    """Skill proficiency entry."""

    skill: str  # "Stealth", "Perception", etc.
    bonus: int  # Total bonus (ability mod + proficiency)
    proficiency_level: Literal[0, 1, 2] = 1  # 0=none, 1=proficient, 2=expertise


class DamageModification(BaseModel):
    """Damage resistance/immunity/vulnerability."""

    types: List[str]  # ["fire", "poison"]
    condition: Optional[str] = None  # "from nonmagical attacks"


class ParsedActorData(BaseModel):
    """Fully parsed stat block ready for FoundryVTT conversion."""

    # Reference to source (for debugging)
    source_statblock_name: str  # Just the name, not full object

    # Core stats (copied from StatBlock)
    name: str
    armor_class: int
    hit_points: int
    hit_dice: Optional[str] = None  # e.g., "27d10 + 189"
    challenge_rating: float

    # Biography/description
    biography: Optional[str] = None  # Full stat block text or generated description

    # Creature type
    size: Optional[str] = None  # "Small", "Medium", "Large", etc.
    creature_type: Optional[str] = None  # "humanoid", "fiend", etc.
    creature_subtype: Optional[str] = None  # "goblinoid", "devil", etc.
    alignment: Optional[str] = None

    # Abilities
    abilities: Dict[str, int]  # STR, DEX, CON, INT, WIS, CHA (required)
    saving_throw_proficiencies: List[str] = []  # ["dex", "wis"]

    # Skills
    skill_proficiencies: List[SkillProficiency] = []

    # Defenses
    damage_resistances: Optional[DamageModification] = None
    damage_immunities: Optional[DamageModification] = None
    damage_vulnerabilities: Optional[DamageModification] = None
    condition_immunities: List[str] = []  # ["poisoned", "charmed"]

    # Movement
    speed_walk: int = 30  # Default 30 ft
    speed_fly: Optional[int] = None
    speed_swim: Optional[int] = None
    speed_burrow: Optional[int] = None
    speed_climb: Optional[int] = None
    speed_hover: bool = False

    # Senses
    darkvision: Optional[int] = None  # Distance in feet
    blindsight: Optional[int] = None
    tremorsense: Optional[int] = None
    truesight: Optional[int] = None
    passive_perception: Optional[int] = None

    # Languages
    languages: List[str] = []  # ["Common", "Goblin"]
    telepathy: Optional[int] = None  # Range in feet

    # Abilities
    traits: List[Trait] = []
    attacks: List[Attack] = []
    reactions: List[Trait] = []  # Reactions use Trait model with activation="reaction"

    # Multiattack
    multiattack: Optional[Multiattack] = None

    # Spellcasting
    spells: List[Spell] = []  # Each spell has UUID pre-resolved
    spellcasting_ability: Optional[Literal["int", "wis", "cha"]] = None
    spell_save_dc: Optional[int] = None
    spell_attack_bonus: Optional[int] = None

    # Innate Spellcasting
    innate_spellcasting: Optional[InnateSpellcasting] = None
</file>

<file path="src/foundry/actors/spell_cache.py">
"""Spell cache for resolving spell names to FoundryVTT compendium UUIDs."""

import logging
import os
from typing import Optional, Dict
from ..items.fetch import fetch_all_spells
from ..items.manager import ItemManager

logger = logging.getLogger(__name__)


class SpellCache:
    """
    Caches spell data from FoundryVTT compendiums for fast UUID lookup.

    Usage:
        cache = SpellCache()
        cache.load()  # Fetch all spells from FoundryVTT
        uuid = cache.get_spell_uuid("Fireball")
    """

    def __init__(self):
        """Initialize empty spell cache."""
        self._spell_by_name: Dict[str, Dict] = {}
        self._full_data_cache: Dict[str, Dict] = {}  # Cache full spell data by UUID
        self._loaded = False
        self._item_manager: Optional[ItemManager] = None

    def load(
        self,
        relay_url: Optional[str] = None,
        api_key: Optional[str] = None,
        client_id: Optional[str] = None
    ) -> None:
        """
        Load all spells from FoundryVTT compendiums.

        Args:
            relay_url: Relay server URL (defaults to env var)
            api_key: API key (defaults to env var)
            client_id: Client ID (defaults to env var)

        Raises:
            ValueError: If required credentials are missing
            RuntimeError: If API request fails
        """
        logger.info("Loading spell cache from FoundryVTT...")

        # Store credentials for lazy fetching
        relay_url = relay_url or os.getenv("FOUNDRY_RELAY_URL")
        api_key = api_key or os.getenv("FOUNDRY_LOCAL_API_KEY")
        client_id = client_id or os.getenv("FOUNDRY_LOCAL_CLIENT_ID")
        foundry_url = os.getenv("FOUNDRY_LOCAL_URL")

        # Initialize ItemManager for lazy fetching full data
        self._item_manager = ItemManager(relay_url, foundry_url, api_key, client_id)

        # Fetch all spells (uses env vars if params not provided)
        kwargs = {}
        if relay_url:
            kwargs['relay_url'] = relay_url
        if api_key:
            kwargs['api_key'] = api_key
        if client_id:
            kwargs['client_id'] = client_id

        spells = fetch_all_spells(**kwargs)

        # Build lookup dict (case-insensitive)
        for spell in spells:
            name = spell.get('name', '').lower()
            if name:
                self._spell_by_name[name] = spell

        self._loaded = True
        logger.info(f"‚úì Loaded {len(self._spell_by_name)} spells into cache")

    def get_spell_uuid(self, spell_name: str) -> Optional[str]:
        """
        Get FoundryVTT compendium UUID for a spell by name.

        Args:
            spell_name: Name of the spell (case-insensitive)

        Returns:
            UUID string if found, None otherwise

        Example:
            >>> cache.get_spell_uuid("Fireball")
            'Compendium.dnd5e.spells.Item.ztgcdrWPshKRpFd0'
        """
        if not self._loaded:
            logger.warning("SpellCache.get_spell_uuid() called before load()")
            return None

        # Case-insensitive lookup
        spell = self._spell_by_name.get(spell_name.lower())

        if spell:
            return spell.get('uuid')

        return None

    def get_spell_data(self, spell_name: str) -> Optional[Dict]:
        """
        Get full spell data by name (including system fields).

        This method lazily fetches full item data from FoundryVTT on first access
        and caches it for subsequent calls.

        Args:
            spell_name: Name of the spell (case-insensitive)

        Returns:
            Full spell dict with system data if found, None otherwise
        """
        if not self._loaded:
            logger.warning("SpellCache.get_spell_data() called before load()")
            return None

        # Get basic spell info from search results
        spell_info = self._spell_by_name.get(spell_name.lower())
        if not spell_info:
            return None

        # Get UUID
        uuid = spell_info.get('uuid')
        if not uuid:
            return None

        # Check if we already have full data cached
        if uuid in self._full_data_cache:
            return self._full_data_cache[uuid]

        # Lazily fetch full data from FoundryVTT
        if self._item_manager:
            try:
                full_data = self._item_manager.get_item(uuid)
                self._full_data_cache[uuid] = full_data
                return full_data
            except Exception as e:
                logger.warning(f"Failed to fetch full data for {spell_name}: {e}")
                return spell_info  # Return basic info as fallback
        else:
            return spell_info  # Return basic info if no ItemManager available

    @property
    def loaded(self) -> bool:
        """Check if cache has been loaded."""
        return self._loaded

    @property
    def spell_count(self) -> int:
        """Get number of spells in cache."""
        return len(self._spell_by_name)
</file>

<file path="src/foundry/items/__init__.py">
"""FoundryVTT item utilities for fetching and managing compendium items."""

from .manager import ItemManager
from .fetch import fetch_items_by_type, fetch_all_spells
from .deduplicate import deduplicate_items, get_source_priority, get_source_stats

__all__ = [
    'ItemManager',
    'fetch_items_by_type',
    'fetch_all_spells',
    'deduplicate_items',
    'get_source_priority',
    'get_source_stats',
]
</file>

<file path="src/foundry/items/deduplicate.py">
"""
Deduplicate items by name, prioritizing official sources.

Priority order:
1. Player's Handbook (dnd-players-handbook)
2. D&D 5e 2024 rules (dnd5e.*.24)
3. D&D 5e SRD (dnd5e.*)
4. Other sources
"""

import logging
from typing import Dict, List
from collections import defaultdict

logger = logging.getLogger(__name__)


def get_source_priority(uuid: str) -> int:
    """
    Get priority score for an item's source (lower = higher priority).

    Args:
        uuid: Item UUID like "Compendium.dnd5e.spells.abc123"

    Returns:
        Priority score (0 = highest priority)
    """
    if 'dnd-players-handbook' in uuid:
        return 0  # Highest priority - official Player's Handbook
    elif '.24' in uuid or 'dnd5e.spells24' in uuid or 'dnd5e.items24' in uuid:
        return 1  # 2024 rules update
    elif 'dnd5e.' in uuid:
        return 2  # Classic D&D 5e SRD
    else:
        return 3  # Other sources (homebrew, modules, etc.)


def deduplicate_items(
    items: List[Dict],
    dedupe_key: str = 'name',
    verbose: bool = True
) -> List[Dict]:
    """
    Deduplicate items by a key (typically name), keeping highest priority source.

    Args:
        items: List of item dicts with at least 'uuid' and dedupe_key fields
        dedupe_key: Field to use for deduplication (default: 'name')
        verbose: Log duplicate removals

    Returns:
        Deduplicated list of items sorted by dedupe_key
    """
    # Group by dedupe_key
    items_by_key = defaultdict(list)
    for item in items:
        key = item.get(dedupe_key, '').strip()
        if key:
            items_by_key[key].append(item)

    # For each key, pick the highest priority source
    deduplicated = []
    duplicates_removed = 0

    for key, item_variants in items_by_key.items():
        # Sort by priority (lower score = higher priority)
        item_variants_sorted = sorted(
            item_variants,
            key=lambda i: get_source_priority(i.get('uuid', ''))
        )

        # Take the first one (highest priority)
        best_item = item_variants_sorted[0]
        deduplicated.append(best_item)

        # Log if we had duplicates
        if len(item_variants) > 1:
            duplicates_removed += len(item_variants) - 1

            if verbose:
                sources = [
                    i.get('uuid', 'unknown').split('.')[1] if '.' in i.get('uuid', '') else 'unknown'
                    for i in item_variants
                ]
                logger.debug(f"  {key}: kept {sources[0]}, removed {', '.join(sources[1:])}")

    # Sort by dedupe_key
    deduplicated_sorted = sorted(deduplicated, key=lambda i: i.get(dedupe_key, ''))

    logger.info(f"‚úì Removed {duplicates_removed} duplicates ({len(deduplicated_sorted)} unique items remain)")

    return deduplicated_sorted


def get_source_stats(items: List[Dict]) -> Dict[str, int]:
    """
    Get statistics on item sources.

    Args:
        items: List of item dicts with 'uuid' field

    Returns:
        Dict mapping source names to counts
    """
    sources = defaultdict(int)

    for item in items:
        uuid = item.get('uuid', '')

        if 'dnd-players-handbook' in uuid:
            sources["Player's Handbook"] += 1
        elif '.24' in uuid or 'dnd5e.spells24' in uuid or 'dnd5e.items24' in uuid:
            sources['D&D 5e 2024'] += 1
        elif 'dnd5e.' in uuid:
            sources['D&D 5e SRD'] += 1
        else:
            sources['Other'] += 1

    return dict(sources)
</file>

<file path="src/foundry/items/manager.py">
"""FoundryVTT Item operations."""

import logging
import requests
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)


class ItemManager:
    """Manages item operations for FoundryVTT."""

    def __init__(self, relay_url: str, foundry_url: str, api_key: str, client_id: str):
        """
        Initialize item manager.

        Args:
            relay_url: URL of the relay server
            foundry_url: URL of the FoundryVTT instance
            api_key: API key for authentication
            client_id: Client ID for the FoundryVTT instance
        """
        self.relay_url = relay_url
        self.foundry_url = foundry_url
        self.api_key = api_key
        self.client_id = client_id

    def get_all_items_by_name(self, name: str) -> List[Dict[str, Any]]:
        """
        Get all items matching the given name across all compendiums.

        Uses the QuickInsert module via the /search endpoint with filter="Item".
        Limited to 200 results maximum (hardcoded in FoundryVTT module).

        Args:
            name: Name of the item to search for

        Returns:
            List of matching item dicts (empty list if none found, max 200 items)

        Raises:
            RuntimeError: If API request fails
        """
        url = f"{self.relay_url}/search"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        params = {
            "clientId": self.client_id,
            "filter": "Item",  # Filters to documentType:Item (via QuickInsert module)
            "query": name
        }

        logger.debug(f"Searching for item: {name}")

        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()

            data = response.json()

            # Search API returns a wrapper with 'results' array
            if isinstance(data, dict) and 'results' in data:
                results = data['results']
                logger.debug(f"Search returned {len(results)} results")
                return results
            # Fallback for unexpected response format
            elif isinstance(data, list):
                logger.debug(f"Search returned {len(data)} results (list format)")
                return data
            else:
                logger.warning(f"Unexpected search result format: {type(data)}")
                return []

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to search for item '{name}': {e}")
            raise RuntimeError(f"Failed to search for item: {e}") from e

    def get_item_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        """
        Get first item matching the given name.

        Args:
            name: Name of the item to find

        Returns:
            Item dict if found (with uuid, name, package, etc.), None otherwise

        Raises:
            RuntimeError: If API request fails
        """
        results = self.get_all_items_by_name(name)

        if not results:
            logger.debug(f"No item found with name: {name}")
            return None

        # Return first match
        item = results[0]
        logger.debug(f"Found item: {item.get('name', 'unknown')} (UUID: {item.get('uuid', 'unknown')})")
        return item

    def get_item(self, item_uuid: str) -> Dict[str, Any]:
        """
        Get an item by UUID.

        Args:
            item_uuid: UUID of the item (e.g., "Item.abc123" or "Compendium.dnd5e.items.Item.abc123")

        Returns:
            Item data dict

        Raises:
            RuntimeError: If API request fails
        """
        url = f"{self.relay_url}/get"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        params = {
            "clientId": self.client_id,
            "uuid": item_uuid
        }

        logger.debug(f"Getting item: {item_uuid}")

        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()

            item_data = response.json()
            logger.debug(f"Retrieved item: {item_data.get('name', 'unknown')}")
            return item_data

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to get item '{item_uuid}': {e}")
            raise RuntimeError(f"Failed to get item: {e}") from e
</file>

<file path="src/foundry/__init__.py">
"""FoundryVTT API integration module."""

from .client import FoundryClient
from .xml_to_journal_html import convert_xml_to_journal_data, convert_xml_directory_to_journals

__all__ = [
    "FoundryClient",
    "convert_xml_to_journal_data",
    "convert_xml_directory_to_journals"
]
</file>

<file path="src/foundry/export_from_foundry.py">
#!/usr/bin/env python3
"""
Export journal entries from FoundryVTT to local files.

Supports exporting to:
- JSON: Full journal data structure
- HTML: Combined HTML file with all pages
- Individual HTML: Separate HTML file per page
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from foundry.client import FoundryClient
from logging_config import setup_logging

logger = setup_logging(__name__)

# Project root is two levels up from this script (foundry -> src -> root)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def export_to_json(journal_data: dict, output_path: str) -> None:
    """
    Export journal data to JSON file.

    Args:
        journal_data: Journal entry data from FoundryVTT API
        output_path: Path to save JSON file
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(journal_data, f, indent=2, ensure_ascii=False)
    logger.info(f"Exported to JSON: {output_path}")


def export_to_html(journal_data: dict, output_path: str, single_file: bool = True) -> None:
    """
    Export journal data to HTML file(s).

    Args:
        journal_data: Journal entry data from FoundryVTT API
        output_path: Path to save HTML file (or directory for multi-file)
        single_file: If True, combine all pages into one file. If False, create separate files.
    """
    # Extract data from response (API wraps journal data in 'data' key)
    data = journal_data.get('data', journal_data)
    journal_name = data.get('name', 'Untitled Journal')
    pages = data.get('pages', [])

    if not pages:
        logger.warning("No pages found in journal")
        return

    # CSS for HTML files
    css_style = """<style>
        body {
            font-family: 'Bookman Old Style', Georgia, serif;
            line-height: 1.6;
            margin: 2em auto;
            max-width: 900px;
            background: #f5f5dc;
            padding: 20px;
        }
        .page-container {
            background: white;
            padding: 40px;
            margin-bottom: 40px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        .page-title {
            font-size: 2em;
            color: #5c3317;
            border-bottom: 3px solid #8b4513;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        nav {
            background-color: #8b4513;
            padding: 1em;
            margin-bottom: 20px;
            border-radius: 5px;
        }
        nav a {
            text-decoration: none;
            color: #f5f5dc;
            padding: 0.5em 1em;
            display: inline-block;
        }
        nav a:hover {
            background-color: #5c3317;
            border-radius: 3px;
        }
        h1 { color: #5c3317; font-size: 1.8em; }
        h2 { color: #5c3317; font-size: 1.5em; }
        h3 { color: #5c3317; font-size: 1.3em; }
        h4 { color: #5c3317; font-size: 1.1em; }
        table {
            border-collapse: collapse;
            margin: 1em 0;
            width: 100%;
            background: white;
        }
        table td {
            padding: 0.5em;
            border: 1px solid #8b4513;
        }
        dl { margin: 1em 0; }
        dt { font-weight: bold; margin-top: 0.5em; color: #5c3317; }
        dd { margin-left: 2em; margin-bottom: 0.5em; }
        .export-info {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
        }
    </style>"""

    if single_file:
        # Combine all pages into one HTML file
        html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{journal_name}</title>
    {css_style}
</head>
<body>
    <nav>
        <strong style="color: #f5f5dc; font-size: 1.2em;">{journal_name}</strong>
"""

        # Add navigation links
        for i, page in enumerate(pages):
            page_name = page.get('name', f'Page {i+1}')
            page_id = f"page-{i}"
            html_content += f'        <a href="#{page_id}">{page_name}</a>\n'

        html_content += "    </nav>\n\n"

        # Add all pages
        for i, page in enumerate(pages):
            page_name = page.get('name', f'Page {i+1}')
            page_id = f"page-{i}"
            page_content = page.get('text', {}).get('content', '<p>No content</p>')

            html_content += f"""    <div class="page-container" id="{page_id}">
        <div class="page-title">{page_name}</div>
        {page_content}
    </div>
"""

        # Add export info
        export_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        html_content += f"""
    <div class="export-info">
        Exported from FoundryVTT on {export_time}<br>
        Journal: {journal_name} | Pages: {len(pages)}
    </div>
</body>
</html>"""

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"Exported to HTML: {output_path} ({len(pages)} pages)")

    else:
        # Create separate HTML file for each page
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)

        for i, page in enumerate(pages):
            page_name = page.get('name', f'Page {i+1}')
            page_content = page.get('text', {}).get('content', '<p>No content</p>')

            # Sanitize filename
            safe_filename = "".join(c for c in page_name if c.isalnum() or c in (' ', '-', '_')).strip()
            page_file = output_dir / f"{i:02d}_{safe_filename}.html"

            html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{page_name} - {journal_name}</title>
    {css_style}
</head>
<body>
    <div class="page-container">
        <div class="page-title">{page_name}</div>
        {page_content}
    </div>
    <div class="export-info">
        Exported from FoundryVTT on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br>
        Journal: {journal_name} | Page {i+1} of {len(pages)}
    </div>
</body>
</html>"""

            with open(page_file, 'w', encoding='utf-8') as f:
                f.write(html_content)

        logger.info(f"Exported to HTML directory: {output_dir} ({len(pages)} pages)")


def export_journal(
    journal_name: str,
    target: str = "local",
    output_format: str = "html",
    output_path: str = None,
    multi_file: bool = False
) -> None:
    """
    Export a journal from FoundryVTT to local file(s).

    Args:
        journal_name: Name of the journal to export
        target: Target environment ('local' or 'forge')
        output_format: Export format ('json', 'html')
        output_path: Path to save exported file(s)
        multi_file: For HTML export, create separate file per page
    """
    # Initialize client
    client = FoundryClient(target=target)

    # Find journal
    logger.info(f"Searching for journal: {journal_name}")
    journal = client.get_journal_by_name(journal_name)

    if not journal:
        logger.error(f"Journal not found: {journal_name}")
        sys.exit(1)

    # Get full journal data with pages
    journal_uuid = journal.get('uuid')
    if not journal_uuid:
        journal_id = journal.get('_id') or journal.get('id')
        if journal_id:
            journal_uuid = f"JournalEntry.{journal_id}"

    if not journal_uuid:
        logger.error(f"Could not determine UUID for journal: {journal_name}")
        sys.exit(1)

    logger.info(f"Retrieving journal data: {journal_uuid}")
    journal_data = client.get_journal(journal_uuid)

    # Determine output path
    if not output_path:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = Path(PROJECT_ROOT) / "output" / "exports" / timestamp
        output_dir.mkdir(parents=True, exist_ok=True)

        if output_format == "json":
            output_path = output_dir / f"{journal_name}.json"
        elif output_format == "html":
            if multi_file:
                output_path = output_dir / journal_name
            else:
                output_path = output_dir / f"{journal_name}.html"
    else:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Export
    if output_format == "json":
        export_to_json(journal_data, str(output_path))
    elif output_format == "html":
        export_to_html(journal_data, str(output_path), single_file=not multi_file)
    else:
        logger.error(f"Unknown format: {output_format}")
        sys.exit(1)

    logger.info(f"Export complete!")


def main():
    """Main entry point for journal export script."""
    parser = argparse.ArgumentParser(
        description="Export journal entries from FoundryVTT to local files"
    )
    parser.add_argument(
        "journal_name",
        help="Name of the journal to export"
    )
    parser.add_argument(
        "--target",
        choices=["local", "forge"],
        default="local",
        help="Target FoundryVTT environment (default: local)"
    )
    parser.add_argument(
        "--format",
        choices=["json", "html"],
        default="html",
        help="Export format (default: html)"
    )
    parser.add_argument(
        "--output",
        help="Output path (default: output/exports/<timestamp>/)"
    )
    parser.add_argument(
        "--multi-file",
        action="store_true",
        help="For HTML: create separate file per page (default: single file)"
    )

    args = parser.parse_args()

    # Load environment variables
    load_dotenv()

    export_journal(
        journal_name=args.journal_name,
        target=args.target,
        output_format=args.format,
        output_path=args.output,
        multi_file=args.multi_file
    )


if __name__ == "__main__":
    main()
</file>

<file path="src/foundry/xml_to_journal_html.py">
"""Convert XML documents to FoundryVTT journal-ready HTML.

This module reuses the core XML to HTML conversion from pdf_processing/xml_to_html.py
and adds only FoundryVTT-specific modifications if needed.
"""

import os
import sys
import re
from pathlib import Path
from typing import Dict, Any, List

# Add parent to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the shared XML to HTML conversion function
from pdf_processing.xml_to_html import xml_to_html_content


def convert_xml_to_journal_data(xml_file_path: str) -> Dict[str, Any]:
    """
    Convert an XML file to journal-ready data structure.

    Uses the shared xml_to_html_content() function from pdf_processing module.
    FoundryVTT journals don't need navigation or full page structure, just content.

    Args:
        xml_file_path: Path to XML file

    Returns:
        Dictionary with journal entry data:
        {
            "name": "Chapter_Name",
            "html": "<h1>...</h1><p>...</p>",
            "metadata": {
                "source_file": "/path/to/file.xml",
                "chapter_number": "01",
                ...
            }
        }
    """
    xml_path = Path(xml_file_path)

    # Convert XML to HTML using shared function
    # include_footers=False to exclude headers/footers from journal pages
    html_content = xml_to_html_content(str(xml_path), include_footers=False)

    # Extract metadata from filename
    filename = xml_path.stem  # Without extension

    # Parse chapter number if present (e.g., "01_Chapter_Name" -> "01")
    parts = filename.split("_", 1)
    chapter_number = parts[0] if parts[0].isdigit() or (len(parts[0]) == 2 and parts[0][0].isdigit()) else None

    metadata = {
        "source_file": str(xml_path),
        "chapter_number": chapter_number,
        "filename": filename,
    }

    return {
        "name": filename,
        "html": html_content,
        "metadata": metadata,
    }


def convert_xml_directory_to_journals(xml_dir_path: str) -> List[Dict[str, Any]]:
    """
    Convert all XML files in a directory to journal data.

    Args:
        xml_dir_path: Path to directory containing XML files

    Returns:
        List of journal data dictionaries
    """
    xml_dir = Path(xml_dir_path)

    if not xml_dir.exists():
        raise ValueError(f"Directory does not exist: {xml_dir_path}")

    journals = []

    for xml_file in sorted(xml_dir.glob("*.xml")):
        journal_data = convert_xml_to_journal_data(str(xml_file))
        journals.append(journal_data)

    return journals


def add_uuid_links(html: str, entity_refs: Dict[str, str]) -> str:
    """
    Replace entity mentions in HTML with @UUID links to FoundryVTT entities.

    This function searches for entity names in the HTML and replaces them with
    clickable @UUID links. Works for ANY FoundryVTT entity type: Items, Actors,
    Scenes, Journals, RollTables, etc.

    Args:
        html: Journal HTML content
        entity_refs: Dictionary mapping entity names to their UUIDs
                     Works for any entity type:
                     - Items: {"Hat of Disguise": "Compendium.dnd5e.items.abc123"}
                     - Actors: {"Klarg": "Actor.xyz789"}
                     - Scenes: {"Cragmaw Cave": "Scene.map456"}
                     - Journals: {"Chapter 1": "JournalEntry.abc"}

    Returns:
        HTML with entity mentions replaced by @UUID[uuid]{name} links

    Example:
        >>> html = "<p>You find a Hat of Disguise and meet Klarg.</p>"
        >>> entity_refs = {
        ...     "Hat of Disguise": "Compendium.dnd5e.items.abc123",
        ...     "Klarg": "Actor.xyz789"
        ... }
        >>> add_uuid_links(html, entity_refs)
        '<p>You find a @UUID[Compendium.dnd5e.items.abc123]{Hat of Disguise} and meet @UUID[Actor.xyz789]{Klarg}.</p>'

    Notes:
        - Links are case-sensitive
        - Longer entity names are processed first to avoid partial matches
        - Entities are only linked once per occurrence (no double-linking)
        - Existing @UUID links are preserved
        - Works for all FoundryVTT entity types (Items, Actors, Scenes, etc.)
    """
    if not entity_refs:
        return html

    # Sort entities by length (longest first) to avoid partial matches
    # e.g., "Longsword +1" before "Longsword"
    sorted_entities = sorted(entity_refs.items(), key=lambda x: len(x[0]), reverse=True)

    modified_html = html

    for entity_name, uuid in sorted_entities:
        # Escape special regex characters in entity name
        escaped_name = re.escape(entity_name)

        # Pattern to match entity name NOT already inside @UUID[...]
        # Negative lookbehind: not preceded by @UUID[...
        # Negative lookahead: not followed by ...] or inside {...}
        pattern = rf'(?<!@UUID\[)(?<!\{{)\b({escaped_name})\b(?!\}})'

        # Replace with @UUID link
        replacement = rf'@UUID[{uuid}]{{\1}}'

        modified_html = re.sub(pattern, replacement, modified_html)

    return modified_html
</file>

<file path="src/pdf_processing/image_asset_processing/__init__.py">
"""Image asset extraction from D&D PDFs."""
</file>

<file path="src/pdf_processing/image_asset_processing/detect_maps.py">
"""Gemini Vision-based map detection."""
import asyncio
import logging
import os
import fitz
import io
from google import genai
from google.genai import types
from typing import List
from dotenv import load_dotenv
from src.pdf_processing.image_asset_processing.models import MapDetectionResult

load_dotenv()
logger = logging.getLogger(__name__)

GEMINI_MODEL = "gemini-2.0-flash"
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds


async def detect_single_page(client: genai.Client, page_image: bytes, page_num: int) -> MapDetectionResult:
    """Detect map on single PDF page using Gemini Vision.

    Args:
        client: Gemini client instance
        page_image: PDF page rendered as PNG bytes
        page_num: Page number (for logging)

    Returns:
        MapDetectionResult with detection results
    """
    prompt = """Analyze this D&D module page. Does it contain a FUNCTIONAL navigation map (dungeon/wilderness overview)
or battle map (tactical grid/encounter area)?

FUNCTIONAL MAP = The primary content is a usable map for gameplay (floor plans, terrain, tactical grids)

NOT A MAP:
- Maps shown as props in artwork (character holding a map, map on a table)
- Maps as decorative elements in scene illustrations
- Character portraits, item illustrations, decorative art, page decorations

If yes, respond with JSON:
{
  "has_map": true,
  "type": "navigation_map" or "battle_map",
  "name": "Descriptive 3-word max name"
}

If no map, respond with JSON:
{
  "has_map": false,
  "type": null,
  "name": null
}"""

    for attempt in range(MAX_RETRIES):
        try:
            # Wrap blocking Gemini call in asyncio.to_thread() for true parallelization
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=GEMINI_MODEL,
                contents=[
                    types.Part.from_bytes(data=page_image, mime_type="image/png"),
                    prompt
                ]
            )

            # Parse JSON response
            import json
            response_text = response.text.strip()

            # Remove markdown code blocks if present
            if response_text.startswith("```"):
                response_text = response_text.split("```")[1]
                if response_text.startswith("json"):
                    response_text = response_text[4:]
                response_text = response_text.strip()

            result_data = json.loads(response_text)
            result = MapDetectionResult(**result_data)

            logger.debug(f"Page {page_num}: has_map={result.has_map}, type={result.type}, name={result.name}")
            return result

        except Exception as e:
            logger.warning(f"Page {page_num} attempt {attempt + 1}/{MAX_RETRIES} failed: {e}")
            if attempt < MAX_RETRIES - 1:
                await asyncio.sleep(RETRY_DELAY * (2 ** attempt))
            else:
                logger.error(f"Page {page_num} detection failed after {MAX_RETRIES} attempts")
                return MapDetectionResult(has_map=False, type=None, name=None)


async def is_map_image_async(client: genai.Client, image_bytes: bytes, width: int, height: int) -> bool:
    """Asynchronously check if an image is a map using Gemini Vision.

    This is a simpler version of detect_single_page for classifying
    individual extracted images from PyMuPDF.

    Args:
        client: Gemini client instance
        image_bytes: Image as bytes (PNG/JPEG format)
        width: Image width in pixels
        height: Image height in pixels

    Returns:
        True if the image is a navigation or battle map, False otherwise
    """
    prompt = """Is this image a FUNCTIONAL D&D navigation map or battle map?

FUNCTIONAL MAP = The image is a usable map for gameplay:
- Navigation maps: dungeon layouts, wilderness areas, floor plans, geographical features
- Battle maps: tactical grids, encounter areas, combat spaces

NOT A MAP:
- Maps shown as props in artwork (character holding map, map on table)
- Maps as decorative elements in illustrations
- Background textures, decorative borders, character portraits, item art

Respond with JSON: {"is_map": true} or {"is_map": false}"""

    try:
        # Wrap blocking Gemini call in asyncio.to_thread() for true parallelization
        response = await asyncio.to_thread(
            client.models.generate_content,
            model=GEMINI_MODEL,
            contents=[
                types.Part.from_bytes(data=image_bytes, mime_type="image/png"),
                prompt
            ]
        )

        import json
        response_text = response.text.strip()

        # Remove markdown code blocks if present
        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
            response_text = response_text.strip()

        result = json.loads(response_text)
        is_map = result.get("is_map", False)

        logger.debug(f"Image classification ({width}x{height}): is_map={is_map}")
        return is_map

    except Exception as e:
        logger.warning(f"Image classification failed: {e}")
        return False


def _render_single_page(pdf_path: str, page_num: int) -> tuple[int, bytes]:
    """Render a single PDF page to PNG bytes (blocking operation for thread pool)."""
    doc = fitz.open(pdf_path)
    page = doc[page_num]
    pix = page.get_pixmap(dpi=150)
    img_bytes = pix.pil_tobytes(format="PNG")
    doc.close()
    return (page_num + 1, img_bytes)  # Return 1-indexed page number


async def detect_maps_async(pdf_path: str) -> List[MapDetectionResult]:
    """Detect maps in all pages of PDF using async Gemini Vision calls.

    Args:
        pdf_path: Path to PDF file

    Returns:
        List of MapDetectionResult, one per page
    """
    api_key = os.getenv("GeminiImageAPI")
    if not api_key:
        raise ValueError("GeminiImageAPI environment variable not set")

    client = genai.Client(api_key=api_key)

    # Get page count
    doc = await asyncio.to_thread(fitz.open, pdf_path)
    page_count = len(doc)
    await asyncio.to_thread(doc.close)

    # Render all pages in parallel using thread pool
    logger.debug(f"Rendering {page_count} pages in parallel...")
    import time
    start_render = time.time()
    render_tasks = [
        asyncio.to_thread(_render_single_page, pdf_path, page_num)
        for page_num in range(page_count)
    ]
    page_images = await asyncio.gather(*render_tasks)
    render_time = time.time() - start_render
    logger.debug(f"Rendered {page_count} pages in {render_time:.1f}s")

    logger.info(f"Detecting maps in {len(page_images)} pages...")

    # Process all pages in parallel
    start_detect = time.time()
    tasks = [detect_single_page(client, img_bytes, page_num)
             for page_num, img_bytes in page_images]
    results = await asyncio.gather(*tasks)
    detect_time = time.time() - start_detect
    logger.debug(f"Gemini detection calls completed in {detect_time:.1f}s")

    maps_found = sum(1 for r in results if r.has_map)
    logger.info(f"Detection complete: {maps_found}/{len(results)} pages have maps")

    return results
</file>

<file path="src/pdf_processing/image_asset_processing/extract_map_assets.py">
"""Main orchestration script for extracting map assets from D&D PDFs.

This script combines three extraction approaches:
1. PyMuPDF extraction with AI classification (for embedded images)
2. Gemini Vision detection (identifies pages with maps)
3. Gemini Imagen segmentation (for baked-in maps)

Usage:
    python extract_map_assets.py --pdf path/to/module.pdf --output output/maps/
"""

import argparse
import asyncio
import logging
import os
import sys
from pathlib import Path
from datetime import datetime
from google import genai
from dotenv import load_dotenv
import fitz

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from logging_config import setup_logging

from src.pdf_processing.image_asset_processing.detect_maps import detect_maps_async
from src.pdf_processing.image_asset_processing.extract_maps import extract_image_with_pymupdf_async
from src.pdf_processing.image_asset_processing.segment_maps import segment_with_imagen
from src.pdf_processing.image_asset_processing.models import MapMetadata

load_dotenv()
logger = setup_logging(__name__)

# Project root (3 levels up: image_asset_processing/ -> pdf_processing/ -> src/ -> root)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))


def _check_if_flattened(pdf_path: str, page_num: int) -> tuple[bool, str]:
    """Check if page is flattened (blocking operation, run in thread pool).

    Returns:
        Tuple of (is_flattened, log_message)
    """
    doc = fitz.open(pdf_path)
    page = doc[page_num - 1]  # 0-indexed

    images = page.get_images()
    page_area = page.rect.width * page.rect.height
    is_flattened = False
    log_msg = ""

    if len(images) == 1:
        try:
            xref = images[0][0]
            img_info = doc.extract_image(xref)
            img_area = img_info['width'] * img_info['height']
            if img_area / page_area > 0.8:
                is_flattened = True
                log_msg = f"Detected flattened page (single image {img_info['width']}x{img_info['height']} covers {100*img_area/page_area:.0f}% of page)"
        except:
            pass

    doc.close()
    return is_flattened, log_msg


def _render_page_to_image(pdf_path: str, page_num: int) -> bytes:
    """Render PDF page to PNG image (blocking operation, run in thread pool)."""
    doc = fitz.open(pdf_path)
    page = doc[page_num - 1]
    pix = page.get_pixmap(dpi=150)
    page_image = pix.pil_tobytes(format="PNG")
    doc.close()
    return page_image


async def extract_single_page(pdf_path: str, page_num: int, detection, output_dir: str, chapter_name: str = None) -> MapMetadata | None:
    """Extract map from a single PDF page.

    Args:
        pdf_path: Path to PDF file
        page_num: Page number (1-indexed)
        detection: MapDetectionResult for this page
        output_dir: Directory to save extracted map
        chapter_name: Optional chapter name for metadata

    Returns:
        MapMetadata if extraction succeeded, None otherwise
    """
    logger.info(f"Processing page {page_num}: {detection.type} - {detection.name}")

    output_path = os.path.join(
        output_dir,
        f"page_{page_num:03d}_{detection.name.replace(' ', '_').lower()}.png"
    )

    # Check if page is flattened (run in thread pool to avoid blocking)
    is_flattened, log_msg = await asyncio.to_thread(_check_if_flattened, pdf_path, page_num)
    if log_msg:
        logger.info(f"  Page {page_num}: {log_msg}")

    # Try PyMuPDF extraction first (faster for embedded images)
    # Skip if page is flattened - PyMuPDF would just extract the whole page
    if not is_flattened:
        logger.info(f"  Page {page_num}: Attempting PyMuPDF extraction...")

        # Open PDF in thread pool (blocking operation)
        doc = await asyncio.to_thread(fitz.open, pdf_path)
        page = doc[page_num - 1]

        success = await extract_image_with_pymupdf_async(
            page, output_path, use_ai_classification=True
        )

        # Close in thread pool
        await asyncio.to_thread(doc.close)
    else:
        logger.info(f"  Page {page_num}: Skipping PyMuPDF (flattened page), using Imagen segmentation...")
        success = False

    metadata = None

    if success:
        logger.info(f"  Page {page_num}: ‚úì Extracted with PyMuPDF -> {output_path}")
        metadata = MapMetadata(
            name=detection.name,
            chapter=chapter_name,
            page_num=page_num,
            type=detection.type,
            source="extracted"
        )
    else:
        # Fallback to Imagen segmentation for baked-in maps
        logger.info(f"  Page {page_num}: PyMuPDF failed, attempting Imagen segmentation...")

        # Render page to image (run in thread pool to avoid blocking)
        page_image = await asyncio.to_thread(_render_page_to_image, pdf_path, page_num)

        try:
            # segment_with_imagen is synchronous, run in thread pool
            await asyncio.to_thread(segment_with_imagen, page_image, detection.type, output_path)
            logger.info(f"  Page {page_num}: ‚úì Segmented with Imagen -> {output_path}")
            metadata = MapMetadata(
                name=detection.name,
                chapter=chapter_name,
                page_num=page_num,
                type=detection.type,
                source="segmented"
            )
        except Exception as e:
            logger.warning(f"  Page {page_num}: ‚úó Failed to segment map: {e}")

    return metadata


async def extract_maps_from_pdf(pdf_path: str, output_dir: str, chapter_name: str = None) -> list[MapMetadata]:
    """Extract all maps from PDF using hybrid approach.

    Args:
        pdf_path: Path to PDF file
        output_dir: Directory to save extracted maps
        chapter_name: Optional chapter name for metadata

    Returns:
        List of MapMetadata for all extracted maps
    """
    api_key = os.getenv("GeminiImageAPI")
    if not api_key:
        raise ValueError("GeminiImageAPI environment variable not set")

    client = genai.Client(api_key=api_key)

    # Step 1: Detect which pages have maps
    logger.info(f"Step 1: Detecting maps in {pdf_path}...")
    detection_results = await detect_maps_async(pdf_path)

    pages_with_maps = [
        (i + 1, result) for i, result in enumerate(detection_results)
        if result.has_map
    ]

    if not pages_with_maps:
        logger.warning("No maps detected in PDF")
        return []

    logger.info(f"Found {len(pages_with_maps)} page(s) with maps")

    # Step 2: Extract maps from all pages in parallel
    logger.info(f"Step 2: Extracting maps from {len(pages_with_maps)} pages in parallel...")

    extraction_tasks = [
        extract_single_page(pdf_path, page_num, detection, output_dir, chapter_name)
        for page_num, detection in pages_with_maps
    ]

    results = await asyncio.gather(*extraction_tasks)

    # Filter out None results (failed extractions)
    extracted_maps = [m for m in results if m is not None]

    return extracted_maps


def save_metadata(maps: list[MapMetadata], output_dir: str):
    """Save extraction metadata as JSON."""
    import json

    metadata_path = os.path.join(output_dir, "maps_metadata.json")
    data = {
        "extracted_at": datetime.now().isoformat(),
        "total_maps": len(maps),
        "maps": [m.model_dump() for m in maps]
    }

    with open(metadata_path, "w") as f:
        json.dump(data, f, indent=2)

    logger.info(f"Saved metadata to {metadata_path}")


async def main():
    parser = argparse.ArgumentParser(
        description="Extract map assets from D&D PDFs using hybrid AI approach"
    )
    parser.add_argument(
        "--pdf",
        required=True,
        help="Path to PDF file"
    )
    parser.add_argument(
        "--output",
        default=None,
        help="Output directory (default: output/runs/<timestamp>/map_assets)"
    )
    parser.add_argument(
        "--chapter",
        default=None,
        help="Chapter name for metadata"
    )

    args = parser.parse_args()

    # Resolve paths
    pdf_path = os.path.abspath(args.pdf)
    if not os.path.exists(pdf_path):
        logger.error(f"PDF not found: {pdf_path}")
        sys.exit(1)

    # Create output directory
    if args.output:
        output_dir = os.path.abspath(args.output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(
            PROJECT_ROOT,
            "output",
            "runs",
            timestamp,
            "map_assets"
        )

    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Output directory: {output_dir}")

    # Extract maps
    try:
        maps = await extract_maps_from_pdf(pdf_path, output_dir, args.chapter)

        if maps:
            logger.info(f"\n{'='*60}")
            logger.info(f"EXTRACTION COMPLETE")
            logger.info(f"{'='*60}")
            logger.info(f"Total maps extracted: {len(maps)}")
            logger.info(f"  PyMuPDF extraction: {sum(1 for m in maps if m.source == 'extracted')}")
            logger.info(f"  Imagen segmentation: {sum(1 for m in maps if m.source == 'segmented')}")

            # Save metadata
            save_metadata(maps, output_dir)

            logger.info(f"\nMaps saved to: {output_dir}")
        else:
            logger.warning("No maps were extracted")

    except Exception as e:
        logger.error(f"Extraction failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="src/pdf_processing/image_asset_processing/extract_maps.py">
"""PyMuPDF-based image extraction with AI classification."""
import logging
import fitz
import asyncio
import os
from google import genai
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# Size thresholds
MIN_IMAGE_SIZE = 200  # Minimum width/height in pixels to avoid tiny decorative elements
PAGE_AREA_THRESHOLD = 0.10  # 10% of page area (lowered from 25% to catch more maps)


async def extract_image_with_pymupdf_async(page: fitz.Page, output_path: str, use_ai_classification: bool = True) -> bool:
    """Extract large images from PDF page using PyMuPDF with async AI classification.

    Searches for images that meet size thresholds (>10% page area AND >200x200px).
    Uses async Gemini Vision to verify images are actually maps, not decorative elements.

    Args:
        page: PyMuPDF page object
        output_path: Path to save extracted image (PNG format)
        use_ai_classification: If True, use Gemini to classify images as maps.
                               If False, use old behavior (largest image wins).

    Returns:
        True if extraction succeeded and image is a map, False otherwise
    """
    # Import here to avoid circular dependency
    from src.pdf_processing.image_asset_processing.detect_maps import is_map_image_async

    try:
        images = page.get_images()
        if not images:
            logger.debug(f"No images found on page")
            return False

        # Calculate page area threshold
        page_area = page.rect.width * page.rect.height
        area_threshold = page_area * PAGE_AREA_THRESHOLD

        # Extract all images meeting size thresholds
        doc = page.parent
        candidates = []

        for img_ref in images:
            xref = img_ref[0]
            try:
                img_info = doc.extract_image(xref)
                img_width = img_info['width']
                img_height = img_info['height']
                img_area = img_width * img_height

                # Filter 1: Must be above minimum size
                if img_width < MIN_IMAGE_SIZE or img_height < MIN_IMAGE_SIZE:
                    logger.debug(f"Skipping tiny image: {img_width}x{img_height}")
                    continue

                # Filter 2: Must occupy enough page area
                if img_area > area_threshold:
                    candidates.append((img_info, img_area, img_width, img_height))
                    logger.debug(f"Found large image: {img_width}x{img_height} ({img_area} px¬≤, {100*img_area/page_area:.1f}% of page)")
            except Exception as e:
                logger.warning(f"Failed to extract image xref {xref}: {e}")
                continue

        if not candidates:
            logger.debug(f"No images above size thresholds ({PAGE_AREA_THRESHOLD*100:.0f}% page area, {MIN_IMAGE_SIZE}x{MIN_IMAGE_SIZE}px)")
            return False

        # Sort by area (largest first)
        candidates.sort(key=lambda x: x[1], reverse=True)

        # If AI classification enabled, classify all candidates in parallel
        if use_ai_classification:
            api_key = os.getenv("GeminiImageAPI")
            if not api_key:
                logger.warning("GeminiImageAPI not set, falling back to largest image")
                use_ai_classification = False
            else:
                client = genai.Client(api_key=api_key)
                logger.info(f"Classifying {len(candidates)} large image(s) with Gemini Vision (async)...")

                # Classify all in parallel
                classification_tasks = [
                    is_map_image_async(client, img_info['image'], width, height)
                    for img_info, _, width, height in candidates
                ]
                results = await asyncio.gather(*classification_tasks)

                # Find first image that's classified as a map
                for (img_info, img_area, width, height), is_map in zip(candidates, results):
                    if is_map:
                        # This is a map! Save it.
                        with open(output_path, "wb") as f:
                            f.write(img_info['image'])
                        logger.info(f"‚úì Extracted map: {width}x{height} -> {output_path}")
                        return True
                    else:
                        logger.debug(f"‚úó Not a map: {width}x{height} (likely background/decoration)")

                logger.info(f"No maps found among {len(candidates)} large image(s)")
                return False

        # Fallback: just use largest image
        if not use_ai_classification:
            img_info, img_area, width, height = candidates[0]
            with open(output_path, "wb") as f:
                f.write(img_info['image'])
            logger.info(f"Extracted largest image: {width}x{height} -> {output_path}")
            return True

        return False

    except Exception as e:
        logger.error(f"Image extraction failed: {e}")
        return False
</file>

<file path="src/pdf_processing/image_asset_processing/models.py">
"""Data models for image asset extraction."""
from pydantic import BaseModel
from typing import Optional


class MapDetectionResult(BaseModel):
    """Result from Gemini Vision map detection.

    Attributes:
        has_map: Whether the page contains a map
        type: Map type ("navigation_map" or "battle_map")
        name: Descriptive name (3 words max)
    """
    has_map: bool
    type: Optional[str] = None
    name: Optional[str] = None


class MapMetadata(BaseModel):
    """Metadata for extracted map asset.

    Attributes:
        name: Descriptive map name
        chapter: Chapter name (None if unknown)
        page_num: PDF page number (1-indexed)
        type: Map type ("navigation_map" or "battle_map")
        source: Extraction method ("extracted" or "segmented")
    """
    name: str
    chapter: Optional[str] = None
    page_num: int
    type: str
    source: str
</file>

<file path="src/pdf_processing/image_asset_processing/preprocess_image.py">
"""Preprocessing utilities for image segmentation."""
import numpy as np
from PIL import Image
import io
import logging

logger = logging.getLogger(__name__)


def remove_existing_red_pixels(image_bytes: bytes) -> bytes:
    """Replace existing red pixels with black to avoid confusion with Gemini's red border.

    Detects pixels that are reddish (R > 150, R > G+50, R > B+50) and replaces them
    with black to ensure only Gemini's generated red border will be detected.

    Args:
        image_bytes: Input image as bytes

    Returns:
        Preprocessed image as bytes with red pixels replaced by black
    """
    # Load image
    img = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    img_array = np.array(img)

    # Detect existing red pixels using THE SAME threshold as our final red detection
    # This ensures we only remove pixels that would interfere with border detection
    red_mask = (
        (img_array[:, :, 0] > 200) &  # R > 200
        (img_array[:, :, 1] < 50) &   # G < 50
        (img_array[:, :, 2] < 50)     # B < 50
    )

    red_pixel_count = red_mask.sum()

    if red_pixel_count > 0:
        logger.info(f"Preprocessing: Replacing {red_pixel_count} existing red pixels with black")
        # Replace red pixels with black
        img_array[red_mask] = [0, 0, 0]
    else:
        logger.debug("Preprocessing: No existing red pixels found")

    # Convert back to bytes
    processed_img = Image.fromarray(img_array)
    output_buffer = io.BytesIO()
    processed_img.save(output_buffer, format='PNG')
    return output_buffer.getvalue()
</file>

<file path="src/pdf_processing/image_asset_processing/segment_maps.py">
"""Gemini Imagen-based map segmentation."""
import logging
import os
import numpy as np
from PIL import Image
import io
from google import genai
from google.genai import types
from dotenv import load_dotenv
import pytesseract

load_dotenv()
logger = logging.getLogger(__name__)

IMAGEN_MODEL = "gemini-2.5-flash-image"
MAX_RETRIES = 5


class SegmentationError(Exception):
    """Raised when segmentation validation fails."""
    pass


def check_word_count(image_bytes: bytes, max_words: int = 200) -> tuple[int, bool]:
    """Check word count in image using OCR.

    Args:
        image_bytes: Image as bytes
        max_words: Maximum allowed word count

    Returns:
        Tuple of (word_count, is_valid)
    """
    try:
        img = Image.open(io.BytesIO(image_bytes))
        text = pytesseract.image_to_string(img)
        word_count = len(text.split())
        is_valid = word_count <= max_words
        return word_count, is_valid
    except Exception as e:
        logger.warning(f"OCR word count check failed: {e}, assuming valid")
        return 0, True


def detect_red_pixels(image_bytes: bytes) -> np.ndarray:
    """Detect red pixels in image (lenient matching for AI-generated borders).

    Detects pixels that are "very red" - high R channel, low G and B channels.
    This is more lenient than exact RGB(255,0,0) to account for compression artifacts.

    Args:
        image_bytes: PNG image as bytes

    Returns:
        Numpy array of (y, x) coordinates of red pixels
    """
    img = Image.open(io.BytesIO(image_bytes))
    img_array = np.array(img)

    # Check for grayscale
    if len(img_array.shape) == 2:  # Grayscale
        return np.array([[], []])

    # Lenient red detection: R > 200, G < 50, B < 50
    # This catches RGB(255,0,0) and similar "very red" pixels
    red_mask = (img_array[:,:,0] > 200) & (img_array[:,:,1] < 50) & (img_array[:,:,2] < 50)
    red_pixels = np.where(red_mask)

    return red_pixels


def find_rectangular_regions(red_pixels: np.ndarray) -> list:
    """Find rectangular regions from red pixels using connected components.

    Uses connected components to find disconnected red regions, then returns
    the bounding box of each region.

    Args:
        red_pixels: Numpy array from detect_red_pixels

    Returns:
        List of rectangles as (x_min, y_min, x_max, y_max, area) tuples, sorted by area (largest first)
    """
    if len(red_pixels[0]) == 0:
        return []

    import cv2

    # Create binary mask from red pixels
    y_coords, x_coords = red_pixels
    height = y_coords.max() + 1
    width = x_coords.max() + 1
    mask = np.zeros((height, width), dtype=np.uint8)
    mask[y_coords, x_coords] = 255

    # Morphological closing to connect thin borders into solid regions
    # This connects the 4 edges of a rectangular border into one region
    # Use a very large kernel to ensure border edges are connected
    kernel = np.ones((50, 50), np.uint8)
    mask_closed = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

    # Find connected components
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask_closed, connectivity=8)

    rectangles = []
    for label_id in range(1, num_labels):  # Skip background (label 0)
        x = stats[label_id, cv2.CC_STAT_LEFT]
        y = stats[label_id, cv2.CC_STAT_TOP]
        w = stats[label_id, cv2.CC_STAT_WIDTH]
        h = stats[label_id, cv2.CC_STAT_HEIGHT]
        area = w * h

        # Filter out very small regions (noise/artifacts)
        if area < 10000:
            continue

        rectangles.append((x, y, x + w, y + h, area))

    # Sort by area (largest first)
    rectangles.sort(key=lambda r: r[4], reverse=True)

    logger.debug(f"Found {len(rectangles)} connected region(s)")
    for i, (x_min, y_min, x_max, y_max, area) in enumerate(rectangles[:3]):  # Show top 3
        logger.debug(f"  Region {i+1}: {x_max-x_min}x{y_max-y_min} (area: {area}px¬≤)")

    return rectangles


def calculate_bounding_box(red_pixels: np.ndarray) -> tuple:
    """Calculate bounding box from red pixel coordinates.

    For a rectangular border (4 thin edges), the bounding box of all red pixels
    gives us the enclosed rectangle.

    Args:
        red_pixels: Numpy array from detect_red_pixels

    Returns:
        Tuple of (x_min, y_min, x_max, y_max)
    """
    if len(red_pixels[0]) == 0:
        return None

    # Simple bounding box approach
    # When Gemini draws a rectangular border (4 edges), the min/max of all red pixels
    # gives us the corners of the rectangle
    y_coords, x_coords = red_pixels
    x_min = x_coords.min()
    y_min = y_coords.min()
    x_max = x_coords.max()
    y_max = y_coords.max()

    width = x_max - x_min
    height = y_max - y_min

    logger.debug(f"Bounding box: {width}x{height}")

    return (x_min, y_min, x_max, y_max)


def segment_with_imagen(page_image: bytes, map_type: str, output_path: str, temperature: float = 0.5) -> None:
    """Segment baked-in map using Gemini Imagen red perimeter technique.

    Args:
        page_image: PDF page rendered as PNG bytes
        map_type: "navigation_map" or "battle_map"
        output_path: Path to save cropped image
        temperature: Model temperature for generation (0-1, default 0.2)

    Raises:
        SegmentationError: If output validation fails
    """
    from src.pdf_processing.image_asset_processing.preprocess_image import remove_existing_red_pixels

    api_key = os.getenv("GeminiImageAPI")
    if not api_key:
        raise ValueError("GeminiImageAPI environment variable not set")

    client = genai.Client(api_key=api_key)

    # Create temp directory for debug files
    # If output_path is already in a "temp" directory, use it; otherwise create temp/
    output_dir = os.path.dirname(output_path)
    if os.path.basename(output_dir) == "temp":
        # Already in temp directory, use it
        temp_dir = output_dir
    else:
        # Create temp subdirectory
        temp_dir = os.path.join(output_dir, "temp")
        os.makedirs(temp_dir, exist_ok=True)

    # Preprocess: Remove any existing red pixels ONLY for the Gemini request
    # (we'll crop from the original image later)
    logger.info("Preprocessing: Removing existing red pixels for boundary detection...")
    preprocessed_image = remove_existing_red_pixels(page_image)

    # Debug: Save preprocessed image to temp/
    output_filename = os.path.basename(output_path)
    preprocessed_debug_path = os.path.join(temp_dir, output_filename.replace(".png", "_preprocessed.png"))
    Image.open(io.BytesIO(preprocessed_image)).save(preprocessed_debug_path)
    logger.debug(f"Saved preprocessed image to {preprocessed_debug_path}")

    # Construct detailed prompt for red perimeter
    map_type_readable = map_type.replace("_", " ")

    if map_type == "navigation_map":
        map_description = """a dungeon or wilderness map showing:
- Rooms, corridors, caves, or outdoor terrain
- Walls, doors, pathways, or geographical features
- Often includes a compass rose
- May show tactical grid or scale
- The actual geographic/floor plan content, NOT decorative page borders or text headers"""
    else:  # battle_map
        map_description = """a tactical battle map showing:
- Combat grid or encounter area
- Room layouts or terrain features
- Tactical positioning spaces
- The actual combat map content, NOT decorative page borders or text"""

    prompt = "draw a tight bright red RGB(255,0,0) perimeter around the dnd map in this image. Do NOT include paragraphs. No padding"

    for attempt in range(MAX_RETRIES):
        try:
            logger.debug(f"Attempt {attempt + 1}/{MAX_RETRIES}: Generating red perimeter with Gemini Flash Image")

            # Step 1: Generate image with red perimeter using generate_content
            # Use preprocessed image (with red pixels removed) for boundary detection
            preprocessed_pil = Image.open(io.BytesIO(preprocessed_image))

            # Use specified temperature for border placement
            config = types.GenerateContentConfig(
                temperature=temperature,
                response_modalities=["IMAGE"]
            )

            response = client.models.generate_content(
                model=IMAGEN_MODEL,
                contents=[preprocessed_pil, prompt],
                config=config
            )

            # Extract generated image from response
            generated_image_bytes = None
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    generated_image_bytes = part.inline_data.data
                    break

            if generated_image_bytes is None:
                raise SegmentationError("No image data in response")

            # Debug: Save the generated image with red perimeter to temp/
            debug_path = os.path.join(temp_dir, output_filename.replace(".png", "_with_red_perimeter.png"))
            generated_img = Image.open(io.BytesIO(generated_image_bytes))
            generated_img.save(debug_path)
            logger.debug(f"Saved image with red perimeter to {debug_path}")

            # Step 2: Detect red pixels
            red_pixels = detect_red_pixels(generated_image_bytes)
            red_pixel_count = len(red_pixels[0])

            logger.debug(f"Detected {red_pixel_count} red pixels")

            # Step 3: Calculate bounding box
            bbox = calculate_bounding_box(red_pixels)

            if bbox is None:
                raise SegmentationError(f"No bounding box found (red pixel count: {red_pixel_count})")

            # Step 4: Validate
            if red_pixel_count < 100:
                raise SegmentationError(f"Insufficient red pixels: {red_pixel_count} (need >= 100)")

            bbox_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            if bbox_area < 1000:
                raise SegmentationError(f"Bounding box too small: {bbox_area}px¬≤ (need >= 1000)")

            logger.info(f"Validation passed: {red_pixel_count} red pixels, bbox area {bbox_area}px¬≤")

            # Step 5: Scale bounding box back to original resolution
            # Gemini downscales images, so we need to scale coordinates back up
            original_img = Image.open(io.BytesIO(page_image))
            generated_img_pil = Image.open(io.BytesIO(generated_image_bytes))

            scale_x = original_img.width / generated_img_pil.width
            scale_y = original_img.height / generated_img_pil.height

            logger.debug(f"Scaling bbox from {generated_img_pil.width}x{generated_img_pil.height} to {original_img.width}x{original_img.height}")
            logger.debug(f"Scale factors: x={scale_x:.2f}, y={scale_y:.2f}")

            x_min, y_min, x_max, y_max = bbox

            # Scale to original resolution
            x_min = int(x_min * scale_x)
            y_min = int(y_min * scale_y)
            x_max = int(x_max * scale_x)
            y_max = int(y_max * scale_y)

            logger.debug(f"Scaled bbox: ({x_min}, {y_min}) to ({x_max}, {y_max})")

            # Inset by 5 pixels (scaled)
            inset = int(5 * max(scale_x, scale_y))
            x_min = max(0, x_min + inset)
            y_min = max(0, y_min + inset)
            x_max = min(original_img.width, x_max - inset)
            y_max = min(original_img.height, y_max - inset)

            cropped = original_img.crop((x_min, y_min, x_max, y_max))

            # Step 6: Quality check - verify word count
            cropped_buffer = io.BytesIO()
            cropped.save(cropped_buffer, format='PNG')
            cropped_bytes = cropped_buffer.getvalue()

            word_count, is_valid = check_word_count(cropped_bytes, max_words=100)
            if not is_valid:
                raise SegmentationError(f"Extracted region contains {word_count} words (max 100), likely included text paragraphs")

            logger.info(f"Quality check passed: {word_count} words (threshold: 100)")

            # Step 7: Save
            cropped.save(output_path, "PNG")
            logger.info(f"Segmented map saved to {output_path} ({x_max - x_min}x{y_max - y_min})")
            return

        except SegmentationError as e:
            logger.warning(f"Attempt {attempt + 1}/{MAX_RETRIES} validation failed: {e}")
            if attempt >= MAX_RETRIES - 1:
                raise
        except Exception as e:
            logger.error(f"Attempt {attempt + 1}/{MAX_RETRIES} failed: {e}")
            if attempt >= MAX_RETRIES - 1:
                raise SegmentationError(f"Segmentation failed after {MAX_RETRIES} attempts: {e}")
</file>

<file path="src/pdf_processing/image_asset_processing/validate_segmentation.py">
#!/usr/bin/env python3
"""Standalone tool to validate Gemini Imagen red perimeter segmentation technique.

Usage:
    uv run python src/pdf_processing/image_asset_processing/validate_segmentation.py \
        --pdf data/pdfs/test.pdf --pages 5 12 18
"""
import argparse
import logging
import fitz
from dataclasses import dataclass
from segment_maps import detect_red_pixels, calculate_bounding_box

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """Result from testing segmentation on a page."""
    page_num: int
    success: bool
    red_pixel_count: int
    bbox: tuple
    error: str = None


def test_segmentation_on_page(page: fitz.Page, page_num: int) -> ValidationResult:
    """Test red perimeter segmentation technique on a single page.

    TODO: This function needs to:
    1. Render page to image
    2. Call Gemini Imagen to add red perimeter
    3. Detect red pixels in result
    4. Validate bounding box

    For now, returns placeholder result.
    """
    logger.info(f"Testing page {page_num}...")

    # TODO: Implement Gemini Imagen call
    # For now, return placeholder
    return ValidationResult(
        page_num=page_num,
        success=False,
        red_pixel_count=0,
        bbox=None,
        error="Not implemented - need to test Gemini Imagen red perimeter generation"
    )


def main():
    parser = argparse.ArgumentParser(
        description="Validate Gemini Imagen red perimeter segmentation technique"
    )
    parser.add_argument("--pdf", required=True, help="Path to PDF file")
    parser.add_argument("--pages", nargs="+", type=int, required=True,
                       help="Page numbers to test (space-separated)")
    args = parser.parse_args()

    doc = fitz.open(args.pdf)

    print(f"\nTesting segmentation on {len(args.pages)} pages from {args.pdf}\n")
    print("=" * 70)

    results = []
    for page_num in args.pages:
        if page_num < 1 or page_num > len(doc):
            logger.error(f"Page {page_num} out of range (1-{len(doc)})")
            continue

        page = doc[page_num - 1]  # Convert to 0-indexed
        result = test_segmentation_on_page(page, page_num)
        results.append(result)

        # Print result
        status = "‚úì PASSED" if result.success else "‚úó FAILED"
        print(f"\nPage {page_num}: {status}")
        print(f"  Red pixels detected: {result.red_pixel_count}")
        print(f"  Bounding box: {result.bbox}")
        if result.error:
            print(f"  Error: {result.error}")

    print("\n" + "=" * 70)

    # Summary
    passed = sum(1 for r in results if r.success)
    print(f"\nSummary: {passed}/{len(results)} pages passed validation")

    if passed < len(results):
        print("\n‚ö†Ô∏è  Some validations failed. Do not use segmentation in production until this works.")
        return 1
    else:
        print("\n‚úì All validations passed! Segmentation technique is ready to use.")
        return 0


if __name__ == "__main__":
    exit(main())
</file>

<file path="src/pdf_processing/__init__.py">
"""
PDF Processing Module

This module contains scripts for processing D&D module PDFs:
- split_pdf: Splits source PDFs into chapter-level PDFs
- pdf_to_xml: Converts PDF chapters to structured XML using Gemini AI
- get_toc: Extracts table of contents from PDFs
"""
</file>

<file path="src/pdf_processing/get_toc.py">
import os
import fitz  # PyMuPDF
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from logging_config import setup_logging

# Project root is three levels up from the script's directory (pdf_processing -> src -> root)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logger = setup_logging(__name__)

def get_toc():
    """
    This script extracts the table of contents from the PDF file.
    """
    pdf_file_path = os.path.join(PROJECT_ROOT, "data", "pdfs", "Lost_Mine_of_Phandelver.pdf")
    doc = fitz.open(pdf_file_path)
    toc = doc.get_toc()
    doc.close()

    if toc:
        logger.info("Table of Contents:")
        for item in toc:
            level, title, page = item
            logger.info(f"  Level {level}: {title} (Page {page})")
    else:
        logger.info("No table of contents found in the PDF")

if __name__ == "__main__":
    get_toc()
</file>

<file path="src/pdf_processing/split_pdf.py">
import fitz  # PyMuPDF
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from logging_config import setup_logging

# Project root is three levels up from the script's directory (pdf_processing -> src -> root)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logger = setup_logging(__name__)

def split_pdf_by_chapters():
    """
    This script splits the PDF file into chapters based on the table of contents,
    and saves them in a directory named after the PDF.
    """
    pdf_file_path = os.path.join(PROJECT_ROOT, "data", "pdfs", "Lost_Mine_of_Phandelver.pdf")
    base_output_dir = os.path.join(PROJECT_ROOT, "data", "pdf_sections")
    
    # Get the PDF name for the subdirectory
    pdf_filename = os.path.splitext(os.path.basename(pdf_file_path))[0]
    output_dir = os.path.join(base_output_dir, pdf_filename)
    
    # Create the output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    doc = fitz.open(pdf_file_path)

    # Define the sections based on the extracted TOC
    sections = [
        (2, 5, "01_Introduction"),
        (6, 13, "02_Part_1_Goblin_Arrows"),
        (14, 26, "03_Part_2_Phandalin"),
        (27, 41, "04_Part_3_The_Spiders_Web"),
        (42, 50, "05_Part_4_Wave_Echo_Cave"),
        (51, 51, "06_Conclusion"),
        (52, 53, "07_Appendix_A_Magic_Items"),
        (54, 63, "08_Appendix_B_Monsters"),
        (64, len(doc), "09_Rules_Index"),
    ]

    logger.info(f"Splitting PDF into {len(sections)} sections in {output_dir}")

    for start_page, end_page, title in sections:
        start_page_0_indexed = start_page - 1
        end_page_0_indexed = end_page - 1

        if end_page_0_indexed >= len(doc):
            end_page_0_indexed = len(doc) - 1

        output_pdf_path = os.path.join(output_dir, f"{title}.pdf")
        new_doc = fitz.open()  # Create a new empty PDF
        new_doc.insert_pdf(doc, from_page=start_page_0_indexed, to_page=end_page_0_indexed)
        new_doc.save(output_pdf_path)
        new_doc.close()
        logger.debug(f"Created: {output_pdf_path}")

    doc.close()
    logger.info("PDF splitting complete")

if __name__ == "__main__":
    split_pdf_by_chapters()
</file>

<file path="src/pdf_processing/xml_to_html.py">
import os
import re
import xml.etree.ElementTree as ET
from datetime import datetime
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from logging_config import setup_logging

# Project root is three levels up from the script's directory (pdf_processing -> src -> root)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logger = setup_logging(__name__)

def convert_markdown_to_html(text):
    """
    Converts Markdown formatting to HTML tags.
    - **text** ‚Üí <strong>text</strong>
    - *text* ‚Üí <em>text</em>
    """
    if not text:
        return text

    # Convert bold first (to avoid conflicts with italic)
    text = re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', text)
    # Then convert italic
    text = re.sub(r'\*(.+?)\*', r'<em>\1</em>', text)

    return text

def xml_to_html_content(xml_path, include_footers=False):
    """
    Converts the content of an XML file to an HTML string.

    Args:
        xml_path: Path to the XML file
        include_footers: If False, excludes <footer> and <header> tags from output (default: False)
    """
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        html_content = ""

        # Process elements recursively to preserve structure
        def process_element(elem, depth=0):
            result = ""

            # Skip footer and header if include_footers is False
            if not include_footers and elem.tag in ['footer', 'header']:
                return ""

            # Handle different element types
            # Semantic heading tags (Chapter > Section > Subsection hierarchy)
            if elem.tag == 'chapter_title':
                # Chapter title -> h1
                text = convert_markdown_to_html(elem.text if elem.text else "")
                result += f"<h1>{text}</h1>\n"
            elif elem.tag == 'title':
                # General title -> h1
                text = convert_markdown_to_html(elem.text if elem.text else "")
                result += f"<h1>{text}</h1>\n"
            elif elem.tag == 'section':
                # Section is a container - process children
                # If it has direct text, output as h2
                if elem.text and elem.text.strip():
                    text = convert_markdown_to_html(elem.text.strip())
                    result += f"<h2>{text}</h2>\n"
                # Process all children
                for child in elem:
                    result += process_element(child, depth + 1)
            elif elem.tag == 'subsection':
                # Subsection is a container - process children
                # If it has direct text, output as h3
                if elem.text and elem.text.strip():
                    text = convert_markdown_to_html(elem.text.strip())
                    result += f"<h3>{text}</h3>\n"
                # Process all children
                for child in elem:
                    result += process_element(child, depth + 1)
            elif elem.tag == 'subsubsection':
                # Sub-subsection is a container - process children
                # If it has direct text, output as h4
                if elem.text and elem.text.strip():
                    text = convert_markdown_to_html(elem.text.strip())
                    result += f"<h4>{text}</h4>\n"
                # Process all children
                for child in elem:
                    result += process_element(child, depth + 1)
            elif elem.tag == 'p':
                text = convert_markdown_to_html(elem.text if elem.text else "")
                result += f"<p>{text}</p>\n"
            elif elem.tag == 'list':
                result += "<ul>\n"
                for item in elem.findall('item'):
                    item_text = convert_markdown_to_html(item.text if item.text else "")
                    result += f"  <li>{item_text}</li>\n"
                result += "</ul>\n"
            elif elem.tag == 'boxed_text':
                result += '<aside style="position: relative; background: #fef9e7; padding: 20px 50px; margin: 20px 0; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);">\n'
                # Left decorative bar
                result += '    <span style="position: absolute; left: 15px; top: 10px; bottom: 10px; width: 4px; background: linear-gradient(to bottom, #5c3317 0%, #8b4513 20%, #5c3317 50%, #8b4513 80%, #5c3317 100%); border-radius: 2px;"></span>\n'
                # Right decorative bar
                result += '    <span style="position: absolute; right: 15px; top: 10px; bottom: 10px; width: 4px; background: linear-gradient(to bottom, #5c3317 0%, #8b4513 20%, #5c3317 50%, #8b4513 80%, #5c3317 100%); border-radius: 2px;"></span>\n'
                # Content
                result += '    \n'
                for child in elem:
                    result += '    ' + process_element(child, depth + 1).replace('\n', '\n    ')
                result += '</aside>\n'
            elif elem.tag == 'table':
                result += '<table border="1">\n'
                for child in elem:
                    result += process_element(child, depth + 1)
                result += '</table>\n'
            elif elem.tag == 'table_row':
                result += '  <tr>\n'
                for child in elem:
                    result += process_element(child, depth + 1)
                result += '  </tr>\n'
            elif elem.tag == 'table_cell':
                text = convert_markdown_to_html(elem.text if elem.text else "")
                result += f'    <td>{text}</td>\n'
            elif elem.tag == 'definition_list':
                result += '<dl>\n'
                for child in elem:
                    result += process_element(child, depth + 1)
                result += '</dl>\n'
            elif elem.tag == 'definition_item':
                for child in elem:
                    result += process_element(child, depth + 1)
            elif elem.tag == 'term':
                text = convert_markdown_to_html(elem.text if elem.text else "")
                result += f'  <dt>{text}</dt>\n'
            elif elem.tag == 'definition':
                result += '  <dd>\n'
                # Handle bare text content before first child element (if present)
                if elem.text and elem.text.strip():
                    text = convert_markdown_to_html(elem.text.strip())
                    result += f'    <p>{text}</p>\n'
                # Process child elements and their tail text
                for child in elem:
                    result += '    ' + process_element(child, depth + 1).replace('\n', '\n    ')
                    # Handle tail text after child elements
                    if child.tail and child.tail.strip():
                        tail_text = convert_markdown_to_html(child.tail.strip())
                        result += f'    <p>{tail_text}</p>\n'
                result += '  </dd>\n'
            # Skip structural/metadata tags that shouldn't be rendered
            elif elem.tag in ['page', 'footer', 'header', 'page_number']:
                for child in elem:
                    result += process_element(child, depth + 1)

            return result

        # Process all children of root
        for child in root:
            html_content += process_element(child)

        return html_content
    except ET.ParseError as e:
        logger.error(f"Error parsing {xml_path}: {e}")
        return f"<h2>Error parsing {os.path.basename(xml_path)}</h2><p>{e}</p>"

def generate_html_page(xml_path, nav_links, output_path, include_footers=False):
    """
    Generates a full HTML page from an XML file.

    Args:
        xml_path: Path to the XML file
        nav_links: List of (text, href) tuples for navigation
        output_path: Path to write the HTML file
        include_footers: If False, excludes <footer> and <header> tags from output (default: False)
    """
    html_content = xml_to_html_content(xml_path, include_footers=include_footers)
    
    nav_html = "<nav>\n"
    for link_text, link_href in nav_links:
        nav_html += f'  <a href="{link_href}">{link_text}</a>\n'
    nav_html += "</nav>\n"

    # Basic CSS for a clean look
    css_style = """<style>
        body { font-family: sans-serif; line-height: 1.6; margin: 2em; }
        nav { background-color: #f2f2f2; padding: 1em; }
        nav a { text-decoration: none; color: #333; padding: 0.5em 1em; }
        nav a:hover { background-color: #ddd; }
        h1, h2, h3, h4 { color: #333; }
        .boxed-text { background-color: #f9f9f9; border: 2px solid #ddd; padding: 1em; margin: 1em 0; }
        table { border-collapse: collapse; margin: 1em 0; }
        table td { padding: 0.5em; }
        dl { margin: 1em 0; }
        dt { font-weight: bold; margin-top: 0.5em; }
        dd { margin-left: 2em; margin-bottom: 0.5em; }
    </style>"""

    full_html = f"""<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{os.path.splitext(os.path.basename(xml_path))[0]}</title>
        {css_style}
    </head>
    <body>
        {nav_html}
        <main>
            {html_content}
        </main>
    </body>
    </html>"""
    
    with open(output_path, "w") as f:
        f.write(full_html)
    logger.debug(f"Successfully created HTML file: {output_path}")

def main(input_dir, output_dir):
    """Main function to convert all XML files in a directory to HTML."""
    xml_files = sorted([f for f in os.listdir(input_dir) if f.endswith(".xml")])
    
    os.makedirs(output_dir, exist_ok=True)

    nav_links = []
    for xml_file in xml_files:
        link_text = os.path.splitext(xml_file)[0]
        link_href = f"{link_text}.html"
        nav_links.append((link_text, link_href))

    for xml_file in xml_files:
        xml_path = os.path.join(input_dir, xml_file)
        logger.info(f"Processing {xml_path}")
        html_filename = f"{os.path.splitext(xml_file)[0]}.html"
        output_path = os.path.join(output_dir, html_filename)
        generate_html_page(xml_path, nav_links, output_path)

if __name__ == "__main__":
    runs_dir = os.path.join(PROJECT_ROOT, "output", "runs")
    latest_run = sorted(os.listdir(runs_dir))[-1]
    latest_run_docs_dir = os.path.join(runs_dir, latest_run, "documents")
    
    html_output_dir = os.path.join(latest_run_docs_dir, "html")

    main(latest_run_docs_dir, html_output_dir)
</file>

<file path="src/scene_extraction/__init__.py">
"""Scene extraction and artwork generation."""

from .models import Scene, ChapterContext
from .extract_context import extract_chapter_context
from .identify_scenes import identify_scene_locations
from .generate_artwork import generate_scene_image, save_scene_image
from .create_gallery import create_scene_gallery_html

__all__ = [
    'Scene',
    'ChapterContext',
    'extract_chapter_context',
    'identify_scene_locations',
    'generate_scene_image',
    'save_scene_image',
    'create_scene_gallery_html'
]
</file>

<file path="src/scene_extraction/create_gallery.py">
"""Create FoundryVTT journal page HTML for scene gallery."""

import logging
from typing import List, Dict

from .models import Scene

logger = logging.getLogger(__name__)


def create_scene_gallery_html(scenes: List[Scene], image_paths: Dict[str, str], prompts: Dict[str, str] = None) -> str:
    """
    Create HTML for a FoundryVTT journal page with scene gallery.

    Args:
        scenes: List of Scene objects
        image_paths: Dict mapping scene name to image file path (relative to FoundryVTT)
        prompts: Optional dict mapping scene name to full Gemini prompt used for image generation

    Returns:
        HTML string for journal page

    Example image_paths:
        {
            "Cave Entrance": "worlds/my-world/images/scene_001_cave_entrance.png",
            "Town Square": "worlds/my-world/images/scene_002_town_square.png"
        }
    """
    logger.info(f"Creating scene gallery HTML for {len(scenes)} scenes")

    if not scenes:
        return """
<h1>Scene Gallery</h1>
<p>This chapter contains no scene artwork.</p>
"""

    html_parts = [
        "<h1>Scene Gallery</h1>",
        # Add CSS for collapsible details
        """<style>
        details {
            margin-top: 1em;
            padding: 0.5em;
            background-color: #1a1a1a;
            border: 1px solid #333;
            border-radius: 4px;
        }
        summary {
            cursor: pointer;
            font-weight: bold;
            color: #888;
            padding: 0.5em;
            user-select: none;
        }
        summary:hover {
            color: #aaa;
        }
        .prompt-content {
            margin-top: 0.5em;
            padding: 0.5em;
            background-color: #0d0d0d;
            border-left: 3px solid #444;
            font-family: monospace;
            font-size: 0.85em;
            white-space: pre-wrap;
            color: #ccc;
        }
        </style>"""
    ]

    for scene in scenes:
        # Section header
        html_parts.append(f'<h2 style="margin-top: 2em; border-bottom: 2px solid #444;">{scene.name}</h2>')

        # Section path (breadcrumb)
        html_parts.append(f'<p style="color: #888; font-size: 0.9em; margin-bottom: 0.5em;">{scene.section_path}</p>')

        # Image (if available)
        image_path = image_paths.get(scene.name)
        if image_path:
            html_parts.append(f'<img src="{image_path}" alt="{scene.name}" style="max-width: 100%; height: auto; border: 1px solid #333; margin: 1em 0;" />')
        else:
            html_parts.append('<p style="color: #666; font-style: italic;">No image available for this scene.</p>')

        # Scene description
        html_parts.append(f'<p style="margin-top: 1em;">{scene.description}</p>')

        # Collapsible prompt (if available)
        if prompts and scene.name in prompts:
            prompt_text = prompts[scene.name].replace('<', '&lt;').replace('>', '&gt;')
            html_parts.append(f'''<details>
    <summary>View Full Gemini Prompt</summary>
    <div class="prompt-content">{prompt_text}</div>
</details>''')

        # Divider
        html_parts.append('<hr style="margin: 2em 0; border: none; border-top: 1px solid #333;" />')

    return "\n".join(html_parts)
</file>

<file path="src/scene_extraction/generate_artwork.py">
"""Generate scene artwork using Gemini Imagen."""

import logging
import os
import time
from typing import Optional
from dotenv import load_dotenv
from google import genai
from google.genai import types
from io import BytesIO

from .models import Scene, ChapterContext

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

# Configure Gemini API
GEMINI_API_KEY = os.getenv("GeminiImageAPI")

IMAGEN_MODEL_NAME = "imagen-3.0-generate-002"
DEFAULT_STYLE_PROMPT = "fantasy illustration, D&D 5e art style, detailed environment, high quality"


def construct_image_prompt(
    scene: Scene,
    chapter_context: ChapterContext,
    style_prompt: Optional[str] = None
) -> str:
    """
    Construct the full prompt for image generation.

    Args:
        scene: Scene object with description and location_type
        chapter_context: Chapter environmental context
        style_prompt: Optional custom style prompt

    Returns:
        Complete prompt string for Gemini Imagen
    """
    style = style_prompt or DEFAULT_STYLE_PROMPT

    return f"""
{scene.description}

Location Type: {scene.location_type}
Lighting: {chapter_context.lighting or 'natural'}
Terrain: {chapter_context.terrain or 'varied'}
Atmosphere: {chapter_context.atmosphere or 'neutral'}

Style: {style}

IMPORTANT CONSTRAINTS:
- Do NOT include any text, words, letters, signs, or writing in the image
- Do NOT depict specific named creatures or characters (generic mobs or background townsfolk are acceptable if needed for atmosphere)
- Focus on the physical environment and location details only
- If location_type is "underground", ensure the scene clearly shows it is inside a cave/dungeon with stone walls, dim lighting, and enclosed space
"""


def generate_scene_image(
    scene: Scene,
    chapter_context: ChapterContext,
    style_prompt: Optional[str] = None
) -> tuple[bytes, str]:
    """
    Generate artwork for a scene using Gemini Imagen.

    Args:
        scene: Scene object with description
        chapter_context: Chapter environmental context
        style_prompt: Optional custom style prompt (uses default if not provided)

    Returns:
        Tuple of (image_bytes, prompt_text)
        - image_bytes: Image data as bytes (PNG format)
        - prompt_text: Full prompt sent to Gemini

    Raises:
        RuntimeError: If image generation fails
    """
    logger.info(f"Generating artwork for scene: {scene.name}")

    # Construct comprehensive prompt
    prompt = construct_image_prompt(scene, chapter_context, style_prompt)

    logger.debug(f"Image generation prompt: {prompt}")

    try:
        # Create client with API key
        client = genai.Client(api_key=GEMINI_API_KEY)

        # Generate image using new Imagen API
        response = client.models.generate_images(
            model=IMAGEN_MODEL_NAME,
            prompt=prompt,
            config=types.GenerateImagesConfig(
                number_of_images=1,
            )
        )

        # Extract image from response
        generated_image = response.generated_images[0]

        # The image object has a _pil_image attribute with the actual PIL Image
        # Convert to bytes (PNG format)
        image_buffer = BytesIO()
        generated_image.image._pil_image.save(image_buffer, format='PNG')
        image_data = image_buffer.getvalue()

        logger.info(f"Generated image for '{scene.name}' ({len(image_data)} bytes)")
        return (image_data, prompt)

    except Exception as e:
        logger.error(f"Failed to generate image for scene '{scene.name}': {e}")
        raise RuntimeError(f"Failed to generate scene image: {e}") from e


def save_scene_image(image_bytes: bytes, output_path: str) -> None:
    """
    Save image bytes to file.

    Args:
        image_bytes: Image data as bytes
        output_path: Path to save image file

    Raises:
        IOError: If file write fails
    """
    from pathlib import Path

    try:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'wb') as f:
            f.write(image_bytes)
        logger.info(f"Saved image to {output_path}")
    except IOError as e:
        logger.error(f"Failed to save image to '{output_path}': {e}")
        raise
</file>

<file path="src/scene_extraction/models.py">
"""Data models for scene extraction."""

from typing import Optional
from pydantic import BaseModel, field_validator


class Scene(BaseModel):
    """Represents a physical location/scene from a D&D module."""

    section_path: str  # e.g., "Chapter 2 ‚Üí The Cragmaw Hideout ‚Üí Area 1"
    name: str
    description: str  # Physical environment description only (no NPCs/monsters)
    location_type: str  # e.g., "underground", "outdoor", "interior", "underwater"
    xml_section_id: Optional[str] = None  # Reference to XML section element

    @field_validator('name')
    @classmethod
    def validate_name_not_empty(cls, v: str) -> str:
        """Ensure name is not empty."""
        if not v or not v.strip():
            raise ValueError("Scene name cannot be empty")
        return v

    @field_validator('description')
    @classmethod
    def validate_description_not_empty(cls, v: str) -> str:
        """Ensure description is not empty."""
        if not v or not v.strip():
            raise ValueError("Scene description cannot be empty")
        return v


class ChapterContext(BaseModel):
    """Environmental context for a chapter (inferred by Gemini)."""

    environment_type: str  # e.g., "underground", "forest", "urban", "coastal"
    weather: Optional[str] = None  # e.g., "rainy", "foggy", "clear"
    atmosphere: Optional[str] = None  # e.g., "oppressive", "peaceful", "tense"
    lighting: Optional[str] = None  # e.g., "dim torchlight", "bright sunlight", "darkness"
    terrain: Optional[str] = None  # e.g., "rocky caverns", "dense forest", "cobblestone streets"
    additional_notes: Optional[str] = None  # Any other relevant context
</file>

<file path="src/util/__init__.py">
"""Utility modules for D&D Module Converter."""
</file>

<file path="src/util/parallel_image_gen.py">
"""Parallel image generation utility using Gemini Imagen."""

import asyncio
import logging
from typing import List, Optional, Union
from io import BytesIO
from pathlib import Path
from datetime import datetime
import os
from PIL import Image
from google import genai
from google.genai import types
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)


async def generate_images_parallel(
    prompts: List[str],
    max_concurrent: int = 15,
    save_dir: Optional[Path] = None,
    make_run: bool = False,
    reference_image: Optional[Union[str, Path, Image.Image]] = None,
    temperature: float = 1.0,
    model: str = "gemini-2.5-flash-image"
) -> List[Optional[bytes]]:
    """
    Generate multiple images in parallel from prompts.

    Args:
        prompts: List of image generation prompts
        max_concurrent: Maximum concurrent API calls (default 15)
        save_dir: Optional directory to save images (with auto-generated names)
        make_run: If True, create a timestamped subfolder inside save_dir (default False)
        reference_image: Optional reference image (file path, Path, or PIL Image) to condition generation
        temperature: Sampling temperature 0.0-2.0 (default 1.0). Higher = more creative/random
        model: Gemini model to use (default: gemini-2.5-flash-image)

    Returns:
        List of image bytes (PNG format), None for failed generations

    Example:
        # Basic usage
        images = await generate_images_parallel(prompts)

        # With reference image and low temperature (more consistent)
        images = await generate_images_parallel(
            prompts,
            reference_image="/path/to/reference.png",
            temperature=0.5
        )

        # High temperature for more creative variation
        images = await generate_images_parallel(
            prompts,
            temperature=1.5,
            save_dir=Path("output/images"),
            make_run=True
        )
    """
    if not prompts:
        return []

    # Store reference image info for thread-safe access
    # Each thread will load its own copy to avoid PIL threading issues
    ref_image_path = None
    if reference_image is not None:
        if isinstance(reference_image, Image.Image):
            # Save PIL Image to temp file and pass path
            import tempfile
            temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
            reference_image.save(temp_file.name, 'PNG')
            ref_image_path = temp_file.name
            logger.info(f"Saved reference PIL Image to temp file: {ref_image_path}")
        else:
            # Convert string or Path to string path
            ref_image_path = str(reference_image)
            logger.info(f"Using reference image: {ref_image_path}")

    # Create timestamped run directory if requested
    actual_save_dir = None
    if save_dir:
        if make_run:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            actual_save_dir = save_dir / timestamp
            logger.info(f"Creating timestamped run directory: {actual_save_dir}")
        else:
            actual_save_dir = save_dir

    semaphore = asyncio.Semaphore(max_concurrent)

    async def generate_one(prompt: str, index: int) -> Optional[bytes]:
        async with semaphore:
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(
                    None,
                    _generate_image_sync,
                    prompt,
                    model,
                    actual_save_dir,
                    index if actual_save_dir else None,
                    ref_image_path,  # Pass path, not PIL Image object
                    temperature
                )
            except Exception as e:
                logger.error(f"Failed to generate image {index} ('{prompt[:50]}...'): {e}")
                return None

    logger.info(f"Generating {len(prompts)} images with max_concurrent={max_concurrent}")
    results = await asyncio.gather(*[generate_one(p, i) for i, p in enumerate(prompts)])

    success_count = sum(1 for r in results if r is not None)
    logger.info(f"Generated {success_count}/{len(prompts)} images successfully")
    if actual_save_dir:
        logger.info(f"Images saved to: {actual_save_dir}")

    return results


def _generate_image_sync(
    prompt: str,
    model: str,
    save_dir: Optional[Path] = None,
    index: Optional[int] = None,
    reference_image_path: Optional[str] = None,
    temperature: float = 1.0
) -> bytes:
    """
    Blocking image generation call.

    This runs in a thread pool executor to avoid blocking the event loop.

    Args:
        reference_image_path: Path to reference image file (each thread loads its own copy)
        temperature: Sampling temperature 0.0-2.0
    """
    api_key = os.getenv("GeminiImageAPI")
    if not api_key:
        raise ValueError("GeminiImageAPI environment variable not set")

    client = genai.Client(api_key=api_key)

    # Build contents list: [prompt] or [prompt, image]
    contents = [prompt]
    if reference_image_path is not None:
        # Each thread loads its own copy of the image for thread safety
        ref_image = Image.open(reference_image_path)
        contents.append(ref_image)

    response = client.models.generate_content(
        model=model,
        contents=contents,
        config=types.GenerateContentConfig(
            temperature=temperature,
            response_modalities=["IMAGE"]
        )
    )

    # Extract image bytes from response
    generated_image_bytes = None
    for part in response.candidates[0].content.parts:
        if part.inline_data is not None:
            generated_image_bytes = part.inline_data.data
            break

    if generated_image_bytes is None:
        raise RuntimeError("No image data in response")

    # Optionally save to disk
    if save_dir and index is not None:
        save_dir.mkdir(parents=True, exist_ok=True)
        filepath = save_dir / f"image_{index:03d}.png"
        with open(filepath, 'wb') as f:
            f.write(generated_image_bytes)
        logger.debug(f"Saved image to {filepath}")

    return generated_image_bytes


async def generate_variations(
    prompt: str,
    count: int = 4,
    max_concurrent: int = 15,
    save_dir: Optional[Path] = None,
    make_run: bool = False,
    reference_image: Optional[Union[str, Path, Image.Image]] = None,
    temperature: float = 1.0,
    model: str = "gemini-2.5-flash-image"
) -> List[Optional[bytes]]:
    """
    Generate multiple variations of the same prompt.

    Args:
        prompt: Single prompt to generate variations of
        count: Number of variations to generate
        max_concurrent: Maximum concurrent API calls
        save_dir: Optional directory to save images
        make_run: If True, create a timestamped subfolder inside save_dir
        reference_image: Optional reference image to condition generation
        temperature: Sampling temperature 0.0-2.0 (default 1.0)
        model: Gemini model to use (default: gemini-2.5-flash-image)

    Returns:
        List of image bytes (PNG format)

    Example:
        # Just get bytes
        variations = await generate_variations("A goblin hideout", count=4)

        # With reference image and high temperature for more variety
        variations = await generate_variations(
            "A goblin hideout",
            count=4,
            reference_image="path/to/reference.png",
            temperature=1.5
        )

        # Low temperature for more consistency
        variations = await generate_variations(
            "A goblin hideout",
            count=4,
            temperature=0.3,
            save_dir=Path("output/variations"),
            make_run=True
        )
    """
    return await generate_images_parallel(
        [prompt] * count,
        max_concurrent=max_concurrent,
        save_dir=save_dir,
        make_run=make_run,
        reference_image=reference_image,
        temperature=temperature,
        model=model
    )
</file>

<file path="src/wall_detection/polygonize.py">
#!/usr/bin/env python3
"""
Auto-extract VTT-ready wall lines from a red-traced map.

Pipeline (grid-agnostic):
1) Load ‚Üí HSV red threshold ‚Üí clean (close+open).
2) Dilate (~2 px) to seal micro gaps.
3) Skeletonize ‚Üí trace 8-connected polylines.
4) Simplify (RDP).
5) Connect: snap endpoint‚Üîendpoint and endpoint‚Üípolyline (T-junctions).
6) Drop short stubs ‚Üí tiny cleanup RDP.
7) Save: overlays (blue on original) + "blue-only" (transparent & white).
8) Optional: export polylines as JSON (pixels).

Requires: opencv-python, numpy, scikit-image
"""
import argparse, json, math, os
from pathlib import Path
import cv2, numpy as np
from skimage.morphology import thin

# ---------- Utilities ----------
def red_mask_hsv(img_rgb, s_min=110, v_min=60):
    hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)
    m1 = cv2.inRange(hsv, (0, s_min, v_min), (10, 255, 255))
    m2 = cv2.inRange(hsv, (170, s_min, v_min), (179, 255, 255))
    return cv2.bitwise_or(m1, m2)

def draw_overlay(img_bgr, polys, color=(255,120,0), thick=2):
    """Draw polylines on BGR image (for overlay output)."""
    result = img_bgr.copy()
    for poly in polys:
        if len(poly)<2: continue
        cv2.polylines(result, [np.int32(poly).reshape(-1,1,2)], False, color, thick, cv2.LINE_AA)
    return result

def clean_mask(mask, close_k=5, open_k=3):
    if close_k: mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((close_k,close_k),np.uint8))
    if open_k:  mask = cv2.morphologyEx(mask,  cv2.MORPH_OPEN,  np.ones((open_k,open_k),np.uint8))
    return mask

def rdp(points, eps):
    """Ramer‚ÄìDouglas‚ÄìPeucker on list[(x,y)]."""
    if len(points) < 3: return points
    def dpt(p,a,b):
        ax,ay=a; bx,by=b; px,py=p
        if ax==bx and ay==by: return math.hypot(px-ax, py-ay)
        t=max(0,min(1,((px-ax)*(bx-ax)+(py-ay)*(by-ay))/((bx-ax)**2+(by-ay)**2)))
        q=(ax+t*(bx-ax), ay+t*(by-ay)); return math.hypot(px-q[0], py-q[1])
    imax, dmax = 0, 0.0
    for i in range(1,len(points)-1):
        d = dpt(points[i], points[0], points[-1])
        if d>dmax: imax, dmax = i, d
    if dmax>eps:
        a = rdp(points[:imax+1], eps)
        b = rdp(points[imax:],   eps)
        return a[:-1]+b
    return [points[0], points[-1]]

def simplify_collinear(points, tolerance=1.0):
    """Remove points that are nearly collinear with their neighbors.
    
    For each point, check if it lies on (or very close to) the line 
    between its predecessor and successor. If so, remove it.
    Iterates until no more points can be removed.
    """
    if len(points) <= 2:
        return points
    
    changed = True
    result = list(points)
    
    while changed:
        changed = False
        simplified = [result[0]]  # Always keep first point
        
        for i in range(1, len(result) - 1):
            prev = simplified[-1]
            curr = result[i]
            next_pt = result[i + 1]
            
            # Calculate perpendicular distance from curr to line prev->next_pt
            px, py = curr
            ax, ay = prev
            bx, by = next_pt
            
            # Vector from a to b
            dx, dy = bx - ax, by - ay
            len_sq = dx * dx + dy * dy
            
            if len_sq < 1e-10:  # prev and next are essentially the same point
                continue
            
            # Perpendicular distance
            t = ((px - ax) * dx + (py - ay) * dy) / len_sq
            t = max(0, min(1, t))  # Clamp to segment
            closest_x = ax + t * dx
            closest_y = ay + t * dy
            dist = math.hypot(px - closest_x, py - closest_y)
            
            # Only keep the point if it's far enough from the line
            if dist > tolerance:
                simplified.append(curr)
            else:
                changed = True  # We removed a point, so iterate again
        
        simplified.append(result[-1])  # Always keep last point
        result = simplified
    
    return result

def lines_only_rgba(polys, size, thick=2, color_bgr=(255,120,0), white_bg=False, vertices_only=False, lines_and_vertices=False):
    H,W = size; bgr = np.zeros((H,W,3), np.uint8)
    for p in polys:
        if len(p)<2: continue
        if vertices_only:
            # Draw only the vertex points
            for point in p:
                x, y = int(point[0]), int(point[1])
                cv2.circle(bgr, (x, y), max(2, thick), color_bgr, -1, cv2.LINE_AA)
        elif lines_and_vertices:
            # Draw lines first
            cv2.polylines(bgr, [np.int32(p).reshape(-1,1,2)], False, color_bgr, thick, cv2.LINE_AA)
            # Then draw vertices on top
            for point in p:
                x, y = int(point[0]), int(point[1])
                cv2.circle(bgr, (x, y), max(3, thick+1), color_bgr, -1, cv2.LINE_AA)
        else:
            # Just draw solid lines
            cv2.polylines(bgr, [np.int32(p).reshape(-1,1,2)], False, color_bgr, thick, cv2.LINE_AA)
    if white_bg:
        bgra = np.dstack([bgr, np.full((H,W),255,np.uint8)])
    else:
        a = (bgr.sum(2)>0).astype(np.uint8)*255
        bgra = np.dstack([bgr, a])
    return bgra

# ---------- Skeleton ‚Üí polylines ----------
def skeleton_polylines(mask, dilate_px=2):
    if dilate_px>0:
        k = 2*dilate_px+1
        mask = cv2.dilate(mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(k,k)), 1)
    skel = thin(mask>0).astype(np.uint8)  # 1-px strokes

    idx = np.argwhere(skel>0)
    pix = {(int(y),int(x)) for y,x in idx}       # store as (row,col)
    
    def nbrs(p):
        r,c=p
        # 8-connected neighborhood
        return [(r+dr,c+dc) for dr in (-1,0,1) for dc in (-1,0,1)
                if (dr or dc) and (r+dr,c+dc) in pix]
    
    deg = {p:len(nbrs(p)) for p in pix}
    endpoints = [p for p,d in deg.items() if d==1]
    vis=set()

    def trace(start):
        path=[start]; vis.add(start)
        n=[q for q in nbrs(start) if q not in vis]
        if not n: return path
        cur=n[0]; prev=start; path.append(cur); vis.add(cur)
        while True:
            n=[q for q in nbrs(cur) if q!=prev and q not in vis]
            if len(nbrs(cur))!=2 or not n: break
            nxt=n[0]; path.append(nxt); vis.add(nxt); prev,cur=cur,nxt
        return path

    lines=[]
    for ep in endpoints:
        if ep in vis: continue
        p=[(q[1],q[0]) for q in trace(ep)];  # (x,y)
        if len(p)>=2: lines.append(p)
    # cycles
    for p0 in list(pix):
        if p0 in vis: continue
        if deg.get(p0,0)==2:
            p=[(q[1],q[0]) for q in trace(p0)]
            if len(p)>=2: lines.append(p)
    return lines

# ---------- Connect gaps (vector) ----------
def poly_length(poly): return sum(math.hypot(poly[i+1][0]-poly[i][0], poly[i+1][1]-poly[i][1]) for i in range(len(poly)-1))
def nearest_on_seg(p,a,b):
    ax,ay=a; bx,by=b; px,py=p; dx,dy=bx-ax,by-ay; den=dx*dx+dy*dy
    if den==0: return a, math.hypot(px-ax,py-ay), 0.0
    t=max(0.0,min(1.0,((px-ax)*dx+(py-ay)*dy)/den)); q=(ax+t*dx, ay+t*dy)
    return q, math.hypot(px-q[0],py-q[1]), t
def insert_point(poly, i, t):
    ax,ay=poly[i]; bx,by=poly[i+1]; q=(ax+t*(bx-ax), ay+t*(by-ay))
    return poly[:i+1]+[q]+poly[i+1:]

def connect_polylines(polys, snap_dist=6.0, min_len=12.0):
    # 1) endpoint‚Üîendpoint snap/merge
    changed=True
    while changed:
        changed=False; n=len(polys); used=[False]*n
        for i in range(n):
            if used[i]: continue
            e1=(polys[i][0], polys[i][-1])
            for j in range(i+1,n):
                if used[j]: continue
                e2=(polys[j][0], polys[j][-1])
                for a in (0,1):
                    for b in (0,1):
                        d=math.hypot(e1[a][0]-e2[b][0], e1[a][1]-e2[b][1])
                        if d<=snap_dist:
                            s1 = polys[i] if a==1 else polys[i][::-1]
                            s2 = polys[j] if b==0 else polys[j][::-1]
                            if d>0.5: s1 = s1+[e2[b]]      # small bridge
                            polys[j]=s1+s2; used[i]=True; changed=True; break
                    if used[i]: break
                if used[i]: break
        polys=[polys[k] for k in range(n) if not used[k]]

    # 2) endpoint ‚Üí nearest point on other poly (insert vertex; T-join)
    for i in range(len(polys)):
        for end_idx in (0,-1):
            p = polys[i][end_idx]; best=(None,None,1e9,None)
            for j in range(len(polys)):
                if j==i: continue
                for k in range(len(polys[j])-1):
                    q, dist, t = nearest_on_seg(p, polys[j][k], polys[j][k+1])
                    if dist<best[2]: best=(j,k,dist,t)
            j,k,dist,t = best
            if dist<=snap_dist:
                polys[j] = insert_point(polys[j], k, t)
                if end_idx==0: polys[i] = [polys[j][k+1]] + polys[i]
                else:          polys[i] = polys[i] + [polys[j][k+1]]

    # 3) drop stubs
    polys=[p for p in polys if poly_length(p)>=min_len]
    return polys

def remove_parallel_duplicates(polys, parallel_dist=8.0):
    """Remove polylines that run parallel and very close to other polylines."""
    if not polys:
        return polys
    
    def avg_dist_to_poly(p1, p2):
        """Average minimum distance from points on p1 to polyline p2"""
        if len(p1) < 2 or len(p2) < 2:
            return float('inf')
        total_dist = 0
        for pt in p1:
            min_dist = float('inf')
            for k in range(len(p2) - 1):
                _, dist, _ = nearest_on_seg(pt, p2[k], p2[k+1])
                min_dist = min(min_dist, dist)
            total_dist += min_dist
        return total_dist / len(p1)
    
    keep = [True] * len(polys)
    lengths = [poly_length(p) for p in polys]
    indices = sorted(range(len(polys)), key=lambda i: lengths[i], reverse=True)
    
    for i in indices:
        if not keep[i]:
            continue
        for j in indices:
            if i == j or not keep[j]:
                continue
            dist_j_to_i = avg_dist_to_poly(polys[j], polys[i])
            dist_i_to_j = avg_dist_to_poly(polys[i], polys[j])
            avg_dist = (dist_j_to_i + dist_i_to_j) / 2
            if avg_dist < parallel_dist:
                keep[j] = False
    
    return [polys[i] for i in range(len(polys)) if keep[i]]

# ---------- Main ----------
def main():
    ap = argparse.ArgumentParser(description="Red-traced map ‚Üí simplified blue wall lines")
    ap.add_argument("image", help="Input map image (PNG/JPG)")
    ap.add_argument("--close", type=int, default=5, help="close kernel (px)")
    ap.add_argument("--open",  type=int, default=3, help="open kernel (px)")
    ap.add_argument("--dilate",type=int, default=6, help="skeleton dilation radius (px)")
    ap.add_argument("--eps",   type=float, default=5.0, help="RDP epsilon (px)")
    ap.add_argument("--snap",  type=float, default=6.0, help="snap/bridge distance (px)")
    ap.add_argument("--minlen",type=float, default=12.0, help="min polyline length to keep (px)")
    ap.add_argument("--collinear", type=float, default=0.5, help="collinear tolerance (px) - removes intermediate points on straight lines")
    ap.add_argument("--parallel", type=float, default=0, help="remove parallel duplicate lines closer than this distance (0=disable)")
    ap.add_argument("--outdir", default="out", help="output directory")
    ap.add_argument("--json", action="store_true", help="export polylines as JSON")
    args = ap.parse_args()

    outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)
    img = cv2.imread(str(args.image)); H,W = img.shape[:2]
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # 1) Red mask ‚Üí clean
    mask = red_mask_hsv(img_rgb); mask = clean_mask(mask, args.close, args.open)
    cv2.imwrite(str(outdir/"mask_debug.png"), mask)

    # 2‚Äì3) Dilate ‚Üí skeleton ‚Üí polylines
    polylines = skeleton_polylines(mask, args.dilate)

    # 4) Simplify
    polylines = [rdp(p, args.eps) for p in polylines if len(p)>=2]

    # 5‚Äì6) Connect & prune
    polylines = connect_polylines(polylines, args.snap, args.minlen)
    points_before_collinear = sum(len(p) for p in polylines)
    
    # 6b) Additional collinear simplification
    if args.collinear > 0:
        polylines = [simplify_collinear(p, args.collinear) for p in polylines]
        polylines = [p for p in polylines if len(p) >= 2]  # Remove any that became too short

    # 6c) Remove parallel duplicates (for thick walls)
    lines_before_parallel = len(polylines)
    points_before_parallel = sum(len(p) for p in polylines)
    if args.parallel > 0:
        polylines = remove_parallel_duplicates(polylines, args.parallel)

    # 7) Save images
    overlay = draw_overlay(img, polylines, color=(255,120,0), thick=2)
    cv2.imwrite(str(outdir/"overlay_blue.png"), overlay)
    cv2.imwrite(str(outdir/"lines_only_transparent.png"),
                lines_only_rgba(polylines, (H,W), thick=2, color_bgr=(255,120,0), white_bg=False))
    cv2.imwrite(str(outdir/"lines_only_white.png"),
                lines_only_rgba(polylines, (H,W), thick=2, color_bgr=(255,120,0), white_bg=True))
    
    # 7b) Save lines + vertices versions (lines with vertex points marked)
    cv2.imwrite(str(outdir/"lines_and_vertices_transparent.png"),
                lines_only_rgba(polylines, (H,W), thick=2, color_bgr=(255,120,0), white_bg=False, lines_and_vertices=True))
    cv2.imwrite(str(outdir/"lines_and_vertices_white.png"),
                lines_only_rgba(polylines, (H,W), thick=2, color_bgr=(255,120,0), white_bg=True, lines_and_vertices=True))

    # 8) (Optional) JSON export
    if args.json:
        data = {"width":W, "height":H, "polylines":[[(float(x),float(y)) for x,y in p] for p in polylines]}
        (outdir/"polylines.json").write_text(json.dumps(data, indent=2))

    # Stats
    npts = sum(len(p) for p in polylines)
    print(f"Done. Polylines={len(polylines)}, points={npts}")
    if args.collinear > 0:
        print(f"Collinear simplification removed {points_before_collinear - npts} points ({points_before_collinear} ‚Üí {npts})")
    if args.parallel > 0:
        print(f"Parallel duplicate removal: {lines_before_parallel} ‚Üí {len(polylines)} polylines, {points_before_parallel} ‚Üí {npts} points")
    print(f"Saved solid lines: {outdir/'overlay_blue.png'}, {outdir/'lines_only_transparent.png'}, {outdir/'lines_only_white.png'}")
    print(f"Saved lines+vertices: {outdir/'lines_and_vertices_transparent.png'}, {outdir/'lines_and_vertices_white.png'}")
    if args.json: print(f"Saved: {outdir/'polylines.json'}")

if __name__ == "__main__":
    main()
</file>

<file path="src/wall_detection/redline_walls.py">
"""Generate red-lined wall images from battle maps using Gemini."""

import sys
from pathlib import Path
from typing import List, Optional, Union
from PIL import Image

# Add parent to path for util import
sys.path.insert(0, str(Path(__file__).parent.parent))

from util.parallel_image_gen import generate_images_parallel


REDLINE_PROMPT = "Draw red lines for walls in this battle map. Draw straight lines only. Avoid stairs. Do not outline the frame."


async def redline_walls(
    reference_image: Union[str, Path, Image.Image],
    save_dir: Path,
    make_run: bool = True,
    max_concurrent: int = 15,
    temperature: float = 0.5,
    model: str = "gemini-2.5-flash-image"
) -> List[Optional[bytes]]:
    """
    Generate red-lined wall images from battle map.

    Args:
        reference_image: Battle map image to redline
        save_dir: Directory to save redlined images
        make_run: Create timestamped subfolder (default True)
        max_concurrent: Max parallel API calls (default 15)
        temperature: Sampling temperature (default 0.5 for consistency)
        model: Gemini model (default gemini-2.5-flash-image)

    Returns:
        List of image bytes (PNG), None for failures

    Example:
        results = await redline_walls(
            "path/to/battle_map.png",
            save_dir=Path("output/redlined")
        )
    """
    return await generate_images_parallel(
        [REDLINE_PROMPT],
        reference_image=reference_image,
        save_dir=save_dir,
        make_run=make_run,
        max_concurrent=max_concurrent,
        temperature=temperature,
        model=model
    )
</file>

<file path="src/logging_config.py">
"""
Centralized logging configuration for dnd_module_gen project.

This module provides consistent logging setup across all project scripts.
Log levels:
    DEBUG: Detailed processing steps, API calls, page-level operations
    INFO: Normal workflow progress (chapter/page processing, file creation)
    WARNING: Non-fatal issues (OCR fallback, word count mismatches, retries)
    ERROR: Processing failures, exceptions, unrecoverable errors

Usage:
    from logging_config import setup_logging

    logger = setup_logging(__name__)
    logger.info("Processing started")
"""

import logging
import sys
from pathlib import Path
from typing import Optional


def setup_logging(
    name: str,
    level: int = logging.INFO,
    log_file: Optional[Path] = None,
    console_output: bool = True
) -> logging.Logger:
    """
    Configure and return a logger instance.

    Args:
        name: Logger name (typically __name__ from calling module)
        level: Logging level (default: INFO)
        log_file: Optional path to write logs to file
        console_output: Whether to output to console (default: True)

    Returns:
        Configured logger instance
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Remove existing handlers to avoid duplicates
    logger.handlers.clear()

    # Create formatter
    formatter = logging.Formatter(
        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # Console handler
    if console_output:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    # File handler
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger


def get_run_logger(script_name: str, run_dir: Path, level: int = logging.INFO) -> logging.Logger:
    """
    Get a logger configured for a specific run directory.

    This is useful for scripts that create timestamped run directories
    and want logs written to those directories.

    Args:
        script_name: Name of the script (e.g., 'pdf_to_xml')
        run_dir: Run directory path
        level: Logging level (default: INFO)

    Returns:
        Configured logger instance with both console and file output
    """
    log_file = run_dir / f"{script_name}.log"
    return setup_logging(script_name, level=level, log_file=log_file, console_output=True)
</file>

<file path="tests/actors/__init__.py">
"""Tests for actor/NPC extraction modules."""
</file>

<file path="tests/actors/test_extract_npcs.py">
"""Tests for NPC extraction with Gemini."""

import pytest
from pathlib import Path
from actors.extract_npcs import identify_npcs_with_gemini
from actors.models import NPC


@pytest.mark.integration
@pytest.mark.requires_api
class TestNPCExtraction:
    """Test NPC extraction with real Gemini API calls."""

    def test_identify_npcs_from_xml(self, check_api_key):
        """Test Gemini identifies named NPCs from XML."""
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_npcs.xml"

        with open(fixture_path, 'r') as f:
            xml_content = f.read()

        npcs = identify_npcs_with_gemini(xml_content)

        # Should find at least Klarg and Sildar
        assert len(npcs) >= 2

        npc_names = [npc.name for npc in npcs]
        assert "Klarg" in npc_names
        assert "Sildar Hallwinter" in npc_names

        # Check Klarg details
        klarg = next(npc for npc in npcs if npc.name == "Klarg")
        assert klarg.creature_stat_block_name == "Bugbear"
        assert "goblin" in klarg.description.lower() or "cragmaw" in klarg.description.lower()
        assert klarg.location is not None

        # Check Sildar details
        sildar = next(npc for npc in npcs if npc.name == "Sildar Hallwinter")
        assert sildar.creature_stat_block_name == "Human Fighter"
        assert "lords" in sildar.description.lower() or "alliance" in sildar.description.lower()

    def test_identify_npcs_no_npcs(self, check_api_key):
        """Test extraction from XML with no named NPCs."""
        xml_content = """
        <Chapter>
            <page>
                <section>Empty Room</section>
                <p>This room contains nothing of interest.</p>
            </page>
        </Chapter>
        """

        npcs = identify_npcs_with_gemini(xml_content)

        assert len(npcs) == 0


@pytest.mark.unit
class TestNPCExtractionUnit:
    """Unit tests for NPC extraction."""

    def test_function_exists(self):
        """Verify function exists with correct signature."""
        from actors.extract_npcs import identify_npcs_with_gemini
        import inspect

        sig = inspect.signature(identify_npcs_with_gemini)
        assert 'xml_content' in sig.parameters
</file>

<file path="tests/actors/test_integration_full_workflow.py">
"""
Full integration tests for actor extraction workflow.

These tests use REAL Gemini API calls to verify the complete workflow:
- XML stat block extraction and parsing
- NPC identification and linking
- Actor creation (mocked FoundryVTT API)
"""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch
from actors.extract_stat_blocks import extract_and_parse_stat_blocks
from actors.extract_npcs import identify_npcs_with_gemini
from actors.process_actors import process_actors_for_run
from actors.models import StatBlock, NPC


@pytest.mark.integration
@pytest.mark.requires_api
class TestFullActorExtractionWorkflow:
    """Integration tests for complete actor extraction workflow with real Gemini API."""

    def test_complete_workflow_with_sample_chapter(self, check_api_key):
        """
        Test complete workflow: XML ‚Üí stat blocks ‚Üí NPCs ‚Üí actor creation.

        Uses real Gemini API calls for parsing and extraction.
        Mocks only FoundryVTT API calls.
        """
        # Use the sample XML fixture with stat blocks and NPCs
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_npcs.xml"

        # Step 1: Extract and parse stat blocks with real Gemini API
        stat_blocks = extract_and_parse_stat_blocks(str(fixture_path))

        # Verify stat blocks were extracted and parsed
        assert len(stat_blocks) >= 2, "Should extract at least 2 stat blocks (Bugbear, Human Fighter)"

        # Find the stat blocks by name
        stat_block_names = [sb.name.upper() for sb in stat_blocks]
        assert "BUGBEAR" in stat_block_names, "Should extract Bugbear stat block"
        assert "HUMAN FIGHTER" in stat_block_names or "HUMAN" in stat_block_names, \
            "Should extract Human Fighter stat block"

        # Verify stat blocks have required fields
        for stat_block in stat_blocks:
            assert isinstance(stat_block, StatBlock)
            assert stat_block.armor_class > 0
            assert stat_block.hit_points > 0
            assert stat_block.challenge_rating >= 0
            assert stat_block.raw_text  # Original text preserved

        # Step 2: Extract NPCs with real Gemini API
        with open(fixture_path, 'r') as f:
            xml_content = f.read()

        npcs = identify_npcs_with_gemini(xml_content)

        # Verify NPCs were extracted
        assert len(npcs) >= 2, "Should extract at least 2 NPCs (Klarg, Sildar)"

        # Find NPCs by name
        npc_names = [npc.name for npc in npcs]
        assert "Klarg" in npc_names, "Should extract Klarg"
        assert "Sildar Hallwinter" in npc_names or "Sildar" in npc_names, \
            "Should extract Sildar Hallwinter"

        # Verify NPCs have required fields and stat block links
        for npc in npcs:
            assert isinstance(npc, NPC)
            assert npc.creature_stat_block_name  # Linked to creature
            assert npc.description
            assert npc.plot_relevance

        # Step 3: Verify NPCs are linked to correct stat blocks
        klarg = next((npc for npc in npcs if npc.name == "Klarg"), None)
        assert klarg is not None
        assert klarg.creature_stat_block_name == "Bugbear", \
            "Klarg should be linked to Bugbear stat block"

        sildar = next((npc for npc in npcs if "Sildar" in npc.name), None)
        assert sildar is not None
        assert "Human" in sildar.creature_stat_block_name or \
               "Fighter" in sildar.creature_stat_block_name, \
            "Sildar should be linked to Human Fighter stat block"

    def test_workflow_with_run_directory(self, check_api_key, tmp_path):
        """
        Test process_actors_for_run with real Gemini API calls.

        Simulates a run directory with XML files and processes actors.
        Mocks FoundryVTT API calls.
        """
        # Create mock run directory structure
        run_dir = tmp_path / "test_run"
        documents_dir = run_dir / "documents"
        documents_dir.mkdir(parents=True)

        # Copy sample XML to run directory
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_npcs.xml"
        test_xml = documents_dir / "chapter_01.xml"
        test_xml.write_text(fixture_path.read_text())

        # Mock FoundryVTT API calls
        with patch('actors.process_actors.FoundryClient') as mock_client_class:
            mock_client = Mock()
            mock_client.search_actor.return_value = None  # No existing actors
            mock_client.create_creature_actor.return_value = "Actor.creature123"
            mock_client.create_npc_actor.return_value = "Actor.npc456"
            mock_client_class.return_value = mock_client

            # Run the full workflow
            stats = process_actors_for_run(str(run_dir), target="local")

        # Verify statistics
        assert stats["stat_blocks_found"] >= 2, "Should find at least 2 stat blocks"
        assert stats["stat_blocks_created"] >= 2, "Should create at least 2 creature actors"
        assert stats["stat_blocks_reused"] == 0, "No actors should be reused (all new)"
        assert stats["npcs_found"] >= 2, "Should find at least 2 NPCs"
        assert stats["npcs_created"] >= 2, "Should create at least 2 NPC actors"
        assert len(stats["errors"]) == 0, f"No errors expected, got: {stats['errors']}"

        # Verify FoundryVTT API was called
        assert mock_client.search_actor.call_count >= 2, \
            "Should search for each creature type"
        assert mock_client.create_creature_actor.call_count >= 2, \
            "Should create at least 2 creature actors"
        assert mock_client.create_npc_actor.call_count >= 2, \
            "Should create at least 2 NPC actors"

    def test_workflow_with_compendium_reuse(self, check_api_key, tmp_path):
        """
        Test workflow reuses existing actors from compendium.

        Uses real Gemini API for parsing/extraction.
        Mocks FoundryVTT to simulate existing actors in compendium.
        """
        # Create mock run directory
        run_dir = tmp_path / "test_run_reuse"
        documents_dir = run_dir / "documents"
        documents_dir.mkdir(parents=True)

        # Copy sample XML
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_npcs.xml"
        test_xml = documents_dir / "chapter_01.xml"
        test_xml.write_text(fixture_path.read_text())

        # Mock FoundryVTT with existing actors in compendium
        with patch('actors.process_actors.FoundryClient') as mock_client_class:
            mock_client = Mock()

            # Simulate Bugbear exists in compendium, Human Fighter does not
            def mock_search(name):
                if "BUGBEAR" in name.upper():
                    return "Actor.existing_bugbear"
                return None

            mock_client.search_actor.side_effect = mock_search
            mock_client.create_creature_actor.return_value = "Actor.new_human"
            mock_client.create_npc_actor.return_value = "Actor.npc_created"
            mock_client_class.return_value = mock_client

            # Run workflow
            stats = process_actors_for_run(str(run_dir), target="local")

        # Verify compendium reuse
        assert stats["stat_blocks_reused"] >= 1, \
            "Should reuse at least 1 actor (Bugbear) from compendium"
        assert stats["stat_blocks_created"] >= 1, \
            "Should create at least 1 new actor (Human Fighter)"

        # Verify NPCs were linked to both existing and new actors
        assert stats["npcs_created"] >= 2, "Should create NPCs"

    def test_workflow_handles_no_stat_blocks(self, check_api_key, tmp_path):
        """
        Test workflow gracefully handles XML with no stat blocks.

        Uses real Gemini API - should identify NPCs but no stat blocks.
        """
        # Create run directory
        run_dir = tmp_path / "test_run_no_stats"
        documents_dir = run_dir / "documents"
        documents_dir.mkdir(parents=True)

        # Create XML with NPCs but no stat blocks
        xml_content = """
        <Chapter_01>
            <page number="1">
                <section>Town of Phandalin</section>
                <p>The mayor greets you warmly.</p>
            </page>
        </Chapter_01>
        """
        test_xml = documents_dir / "chapter_01.xml"
        test_xml.write_text(xml_content)

        # Mock FoundryVTT
        with patch('actors.process_actors.FoundryClient') as mock_client_class:
            mock_client = Mock()
            mock_client.search_actor.return_value = None
            mock_client_class.return_value = mock_client

            # Run workflow
            stats = process_actors_for_run(str(run_dir), target="local")

        # Verify graceful handling
        assert stats["stat_blocks_found"] == 0, "No stat blocks in XML"
        assert stats["stat_blocks_created"] == 0, "No creatures created"
        assert len(stats["errors"]) == 0, "No errors - empty results are valid"

    def test_stat_block_parsing_accuracy(self, check_api_key):
        """
        Test Gemini parsing accuracy for various stat block formats.

        Verifies Gemini correctly extracts all D&D 5e fields.
        """
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_stat_blocks.xml"

        # Extract and parse with real Gemini API
        stat_blocks = extract_and_parse_stat_blocks(str(fixture_path))

        assert len(stat_blocks) >= 2, "Should parse multiple stat blocks"

        # Verify Goblin stat block accuracy
        goblin = next((sb for sb in stat_blocks if "GOBLIN" in sb.name.upper() and "BOSS" not in sb.name.upper()), None)
        if goblin:
            assert goblin.armor_class == 15, "Goblin AC should be 15"
            assert goblin.hit_points == 7, "Goblin HP should be 7"
            assert goblin.challenge_rating == 0.25, "Goblin CR should be 1/4"

            # Verify optional fields if parsed
            if goblin.abilities:
                assert "DEX" in goblin.abilities or "dex" in goblin.abilities, \
                    "Goblin should have DEX ability score"

    def test_npc_extraction_accuracy(self, check_api_key):
        """
        Test Gemini NPC extraction accuracy.

        Verifies NPCs are correctly identified and linked to stat blocks.
        """
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_npcs.xml"

        with open(fixture_path, 'r') as f:
            xml_content = f.read()

        # Extract NPCs with real Gemini API
        npcs = identify_npcs_with_gemini(xml_content)

        assert len(npcs) >= 2, "Should identify multiple NPCs"

        # Verify Klarg details
        klarg = next((npc for npc in npcs if npc.name == "Klarg"), None)
        if klarg:
            assert klarg.creature_stat_block_name == "Bugbear", \
                "Klarg should be linked to Bugbear"
            assert "goblin" in klarg.description.lower() or \
                   "cragmaw" in klarg.description.lower(), \
                "Klarg description should mention goblins or Cragmaw"
            assert klarg.location is not None, "Klarg should have a location"

        # Verify Sildar details
        sildar = next((npc for npc in npcs if "Sildar" in npc.name), None)
        if sildar:
            assert "Human" in sildar.creature_stat_block_name or \
                   "Fighter" in sildar.creature_stat_block_name, \
                "Sildar should be linked to Human Fighter"
            assert "lords" in sildar.description.lower() or \
                   "alliance" in sildar.description.lower(), \
                "Sildar description should mention Lords' Alliance"


@pytest.mark.integration
@pytest.mark.requires_api
@pytest.mark.slow
class TestEndToEndWithRealPDF:
    """
    End-to-end tests that process real PDFs with Gemini API.

    These tests are marked as slow and require both Gemini API and test PDFs.
    Uses the Appendix B: Monsters PDF which contains 20+ stat blocks.
    """

    def test_xml_has_stat_block_tags(self, check_api_key, tmp_path):
        """
        Test that XML generation includes stat block tags.

        Uses Appendix B: Monsters PDF which contains stat blocks for:
        - Goblin, Ghoul, Giant Spider, Grick, Hobgoblin
        - Owlbear, Ogre, Spectator, Stirge, Wolf
        - Young Green Dragon, Zombie, Cultist, Doppelganger
        - And more (20+ total stat blocks)

        This verifies:
        1. pdf_to_xml.py processes the PDF successfully
        2. <stat_block name="..."> tags are present in output XML
        3. Raw stat block text is preserved inside tags
        4. Multiple stat blocks are detected
        """
        import sys
        import os
        import re
        import glob
        from pathlib import Path

        # Path to monsters PDF directory
        monsters_pdf_dir = Path("/Users/ethanotto/Documents/Projects/dnd_module_gen/data/pdf_sections/Lost_Mine_of_Phandelver")
        monsters_pdf_name = "08_Appendix_B_Monsters.pdf"
        monsters_pdf = monsters_pdf_dir / monsters_pdf_name

        if not monsters_pdf.exists():
            pytest.skip(f"Monsters PDF not found: {monsters_pdf}")

        # Add src to path for importing pdf_to_xml
        project_root = Path(__file__).parent.parent.parent
        sys.path.insert(0, str(project_root / "src"))

        # Import the PDF processing main function
        from pdf_processing.pdf_to_xml import main, configure_gemini

        # Create output directory
        output_dir = tmp_path / "monsters_output"
        output_dir.mkdir(parents=True)

        # Process the monsters PDF
        print(f"\nProcessing monsters PDF: {monsters_pdf}")
        print(f"Output directory: {output_dir}")

        try:
            # Configure Gemini API
            configure_gemini()

            # Process the PDF (this will make real Gemini API calls)
            # main() expects:
            #   - input_dir: directory containing PDFs
            #   - base_output_dir: where to create timestamped run
            #   - single_file: name of the PDF file to process
            main(
                input_dir=str(monsters_pdf_dir),
                base_output_dir=str(output_dir),
                single_file=monsters_pdf_name
            )

            # Find the generated XML in the timestamped run directory
            # The structure is: output_dir/<timestamp>/documents/08_Appendix_B_Monsters.xml
            run_dirs = sorted(glob.glob(str(output_dir / "*")))
            assert len(run_dirs) > 0, "No run directory created"

            latest_run = run_dirs[-1]
            xml_output_path = Path(latest_run) / "documents" / "08_Appendix_B_Monsters.xml"

            assert xml_output_path.exists(), f"XML output not created: {xml_output_path}"

            # Read the generated XML
            with open(xml_output_path, 'r') as f:
                xml_content = f.read()

            # Verify stat blocks are tagged
            stat_block_pattern = r'<stat_block name="([^"]+)">(.*?)</stat_block>'
            stat_blocks = re.findall(stat_block_pattern, xml_content, re.DOTALL)

            print(f"\nFound {len(stat_blocks)} stat blocks in generated XML:")
            for name, _ in stat_blocks[:5]:  # Show first 5
                print(f"  - {name}")
            if len(stat_blocks) > 5:
                print(f"  ... and {len(stat_blocks) - 5} more")

            # Assertions
            assert len(stat_blocks) >= 5, \
                f"Expected at least 5 stat blocks, found {len(stat_blocks)}"

            # Check for some expected creatures
            stat_block_names = [name.upper() for name, _ in stat_blocks]
            expected_creatures = ["GOBLIN", "GHOUL", "GIANT SPIDER", "OGRE", "WOLF"]

            found_expected = []
            for expected in expected_creatures:
                if any(expected in name for name in stat_block_names):
                    found_expected.append(expected)

            print(f"\nExpected creatures found: {found_expected}")

            assert len(found_expected) >= 2, \
                f"Expected to find at least 2 of {expected_creatures}, found {found_expected}"

            # Verify raw text is preserved (check one stat block)
            if stat_blocks:
                first_name, first_content = stat_blocks[0]
                assert len(first_content.strip()) > 50, \
                    f"Stat block '{first_name}' content too short: {len(first_content)} chars"

                # Should contain typical stat block elements
                typical_elements = ["Armor Class", "Hit Points", "Speed", "STR", "DEX"]
                found_elements = [elem for elem in typical_elements if elem in first_content]

                assert len(found_elements) >= 2, \
                    f"Stat block should contain typical elements, found {found_elements}"

            print("\n‚úÖ XML stat block tagging verification passed!")

        except Exception as e:
            pytest.fail(f"Failed to process PDF or verify output: {e}")
</file>

<file path="tests/actors/test_models.py">
"""Tests for actor Pydantic models."""

import pytest
from datetime import datetime
from pathlib import Path
from pydantic import ValidationError
from src.actors.models import StatBlock, NPC, ActorCreationResult


@pytest.mark.unit
class TestStatBlockModel:
    """Test StatBlock Pydantic model."""

    def test_stat_block_valid_minimal(self):
        """Test StatBlock with minimal required fields."""
        stat_block = StatBlock(
            name="Goblin",
            raw_text="Goblin stat block text...",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25
        )
        assert stat_block.name == "Goblin"
        assert stat_block.armor_class == 15
        assert stat_block.hit_points == 7
        assert stat_block.challenge_rating == 0.25
        assert stat_block.raw_text == "Goblin stat block text..."

    def test_stat_block_valid_complete(self):
        """Test StatBlock with all optional fields."""
        stat_block = StatBlock(
            name="Goblin Boss",
            raw_text="Goblin Boss stat block...",
            armor_class=17,
            hit_points=21,
            challenge_rating=1.0,
            size="Small",
            type="humanoid",
            alignment="neutral evil",
            abilities={"STR": 10, "DEX": 14, "CON": 10, "INT": 10, "WIS": 8, "CHA": 10},
            speed="30 ft.",
            senses="darkvision 60 ft.",
            languages="Common, Goblin"
        )
        assert stat_block.size == "Small"
        assert stat_block.type == "humanoid"
        assert stat_block.abilities["DEX"] == 14

    def test_stat_block_invalid_ac(self):
        """Test StatBlock rejects invalid armor class."""
        with pytest.raises(ValidationError):
            StatBlock(
                name="Invalid",
                raw_text="text",
                armor_class=50,  # Too high
                hit_points=10,
                challenge_rating=1.0
            )

    def test_stat_block_missing_required(self):
        """Test StatBlock requires all required fields."""
        with pytest.raises(ValidationError):
            StatBlock(
                name="Missing Fields",
                raw_text="text"
                # Missing AC, HP, CR
            )


@pytest.mark.unit
class TestNPCModel:
    """Test NPC Pydantic model."""

    def test_npc_valid_minimal(self):
        """Test NPC with minimal required fields."""
        npc = NPC(
            name="Klarg",
            creature_stat_block_name="Goblin Boss",
            description="Leader of the Cragmaw goblins",
            plot_relevance="Guards the stolen supplies"
        )
        assert npc.name == "Klarg"
        assert npc.creature_stat_block_name == "Goblin Boss"
        assert npc.location is None

    def test_npc_valid_complete(self):
        """Test NPC with all optional fields."""
        npc = NPC(
            name="Sildar Hallwinter",
            creature_stat_block_name="Human Fighter",
            description="Member of the Lords' Alliance",
            plot_relevance="Captured by goblins, needs rescue",
            location="Cragmaw Hideout",
            first_appearance_section="Chapter 1 ‚Üí Goblin Ambush"
        )
        assert npc.location == "Cragmaw Hideout"
        assert npc.first_appearance_section == "Chapter 1 ‚Üí Goblin Ambush"

    def test_npc_missing_required(self):
        """Test NPC requires all required fields."""
        with pytest.raises(ValidationError):
            NPC(
                name="Incomplete",
                description="Missing creature type"
                # Missing creature_stat_block_name and plot_relevance
            )


@pytest.mark.unit
class TestActorCreationResult:
    """Test ActorCreationResult dataclass."""

    def test_actor_creation_result_basic(self):
        """Test ActorCreationResult can be instantiated."""
        stat_block = StatBlock(
            name="Test Goblin",
            raw_text="Goblin\nSmall humanoid...",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25
        )

        result = ActorCreationResult(
            description="A sneaky goblin",
            challenge_rating=0.25,
            raw_stat_block_text="Goblin\nSmall humanoid...",
            stat_block=stat_block,
            parsed_actor_data=None,  # We'll use a real one later
            foundry_uuid="Actor.abc123",
            output_dir=Path("/tmp/test"),
            timestamp=datetime.now().isoformat(),
            model_used="gemini-2.0-flash"
        )

        assert result.description == "A sneaky goblin"
        assert result.foundry_uuid == "Actor.abc123"
        assert result.stat_block.name == "Test Goblin"
        assert result.challenge_rating == 0.25
        assert result.model_used == "gemini-2.0-flash"

    def test_actor_creation_result_with_file_paths(self):
        """Test ActorCreationResult with optional file paths."""
        stat_block = StatBlock(
            name="Test Goblin",
            raw_text="Goblin\nSmall humanoid...",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25
        )

        output_dir = Path("/tmp/test/actor_output")
        result = ActorCreationResult(
            description="A sneaky goblin",
            challenge_rating=0.25,
            raw_stat_block_text="Goblin\nSmall humanoid...",
            stat_block=stat_block,
            parsed_actor_data=None,
            foundry_uuid="Actor.abc123",
            output_dir=output_dir,
            raw_text_file=output_dir / "raw_text.txt",
            stat_block_file=output_dir / "stat_block.json",
            parsed_data_file=output_dir / "parsed_data.json",
            foundry_json_file=output_dir / "foundry_actor.json",
            timestamp=datetime.now().isoformat(),
            model_used="gemini-2.0-flash"
        )

        assert result.raw_text_file == output_dir / "raw_text.txt"
        assert result.stat_block_file == output_dir / "stat_block.json"
        assert result.parsed_data_file == output_dir / "parsed_data.json"
        assert result.foundry_json_file == output_dir / "foundry_actor.json"
</file>

<file path="tests/actors/test_process_actors.py">
"""Tests for actor processing workflow."""

import pytest
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path
from actors.process_actors import process_actors_for_run


@pytest.mark.unit
class TestActorProcessingWorkflow:
    """Test complete actor processing workflow (mocked)."""

    def test_process_actors_workflow(self):
        """Test complete workflow: extract stat blocks ‚Üí parse ‚Üí extract NPCs ‚Üí create actors."""

        # Mock XML file
        run_dir = "/tmp/test_run"
        xml_file = f"{run_dir}/documents/chapter_01.xml"

        # Mock dependencies
        with patch('actors.process_actors.extract_and_parse_stat_blocks') as mock_extract_sb, \
             patch('actors.process_actors.identify_npcs_with_gemini') as mock_extract_npcs, \
             patch('actors.process_actors.FoundryClient') as mock_client_class, \
             patch('actors.process_actors.Path') as mock_path_class, \
             patch('actors.process_actors.GeminiAPI') as mock_gemini_api, \
             patch('builtins.open', create=True) as mock_open:

            # Setup mocks for Path
            mock_run_path = MagicMock()
            mock_run_path.exists.return_value = True
            mock_documents_dir = MagicMock()
            mock_documents_dir.exists.return_value = True
            mock_documents_dir.glob.return_value = [Path(xml_file)]
            mock_run_path.__truediv__.return_value = mock_documents_dir
            mock_path_class.return_value = mock_run_path

            # Mock file read
            mock_file = MagicMock()
            mock_file.__enter__.return_value.read.return_value = "<xml>test</xml>"
            mock_open.return_value = mock_file

            # Mock stat blocks
            from actors.models import StatBlock
            mock_stat_block = StatBlock(
                name="Goblin",
                raw_text="Goblin text",
                armor_class=15,
                hit_points=7,
                challenge_rating=0.25
            )
            mock_extract_sb.return_value = [mock_stat_block]

            # Mock NPCs
            from actors.models import NPC
            mock_npc = NPC(
                name="Klarg",
                creature_stat_block_name="Goblin Boss",
                description="Leader",
                plot_relevance="Guards supplies"
            )
            mock_extract_npcs.return_value = [mock_npc]

            # Mock FoundryClient
            mock_client = MagicMock()
            mock_client.search_actor.return_value = None  # Not found in compendium
            mock_client.create_creature_actor.return_value = "Actor.creature123"
            mock_client.create_npc_actor.return_value = "Actor.npc456"
            mock_client_class.return_value = mock_client

            # Run workflow
            result = process_actors_for_run(run_dir, target="local")

            # Verify calls
            mock_extract_sb.assert_called_once()
            mock_extract_npcs.assert_called_once()
            mock_client.search_actor.assert_called()
            mock_client.create_creature_actor.assert_called_once_with(mock_stat_block)
            mock_client.create_npc_actor.assert_called_once()

            # Verify result
            assert result["stat_blocks_found"] == 1
            assert result["stat_blocks_created"] == 1
            assert result["npcs_found"] == 1
            assert result["npcs_created"] == 1

    def test_process_actors_reuses_compendium(self):
        """Test workflow reuses existing compendium actors."""

        run_dir = "/tmp/test_run"
        xml_file = f"{run_dir}/documents/chapter_01.xml"

        with patch('actors.process_actors.extract_and_parse_stat_blocks') as mock_extract_sb, \
             patch('actors.process_actors.identify_npcs_with_gemini') as mock_extract_npcs, \
             patch('actors.process_actors.FoundryClient') as mock_client_class, \
             patch('actors.process_actors.Path') as mock_path_class, \
             patch('actors.process_actors.GeminiAPI') as mock_gemini_api, \
             patch('builtins.open', create=True) as mock_open:

            # Setup mocks for Path
            mock_run_path = MagicMock()
            mock_run_path.exists.return_value = True
            mock_documents_dir = MagicMock()
            mock_documents_dir.exists.return_value = True
            mock_documents_dir.glob.return_value = [Path(xml_file)]
            mock_run_path.__truediv__.return_value = mock_documents_dir
            mock_path_class.return_value = mock_run_path

            # Mock file read
            mock_file = MagicMock()
            mock_file.__enter__.return_value.read.return_value = "<xml>test</xml>"
            mock_open.return_value = mock_file

            from actors.models import StatBlock, NPC
            mock_stat_block = StatBlock(
                name="Goblin",
                raw_text="text",
                armor_class=15,
                hit_points=7,
                challenge_rating=0.25
            )
            mock_extract_sb.return_value = [mock_stat_block]

            mock_npc = NPC(
                name="Snarf",
                creature_stat_block_name="Goblin",
                description="Scout",
                plot_relevance="Ambushes party"
            )
            mock_extract_npcs.return_value = [mock_npc]

            # Mock client finds Goblin in compendium
            mock_client = MagicMock()
            mock_client.search_actor.return_value = "Actor.existing_goblin"
            mock_client.create_npc_actor.return_value = "Actor.npc789"
            mock_client_class.return_value = mock_client

            result = process_actors_for_run(run_dir, target="local")

            # Verify Goblin NOT created (reused from compendium)
            mock_client.create_creature_actor.assert_not_called()

            # Verify NPC created with existing Goblin UUID
            call_args = mock_client.create_npc_actor.call_args
            # call_args is (args, kwargs) - we want kwargs['stat_block_uuid']
            assert call_args.kwargs['stat_block_uuid'] == "Actor.existing_goblin"

            assert result["stat_blocks_reused"] == 1
</file>

<file path="tests/foundry/actors/__init__.py">
# Empty __init__.py for tests/foundry/actors
</file>

<file path="tests/foundry/actors/test_attack_save.py">
"""Tests for AttackSave model."""

import pytest
from foundry.actors.models import AttackSave, DamageFormula, Attack


class TestAttackSaveModel:
    """Tests for AttackSave model."""

    def test_basic_attack_save(self):
        """Should create basic attack save."""
        save = AttackSave(
            ability="con",
            dc=13,
            damage=[DamageFormula(number=2, denomination=6, bonus="", type="poison")],
            on_save="half"
        )

        assert save.ability == "con"
        assert save.dc == 13
        assert len(save.damage) == 1
        assert save.on_save == "half"

    def test_ongoing_damage_attack_save(self):
        """Should support ongoing damage effects."""
        save = AttackSave(
            ability="con",
            dc=21,
            damage=[],  # No immediate damage
            ongoing_damage=[DamageFormula(number=6, denomination=6, bonus="", type="poison")],
            duration_rounds=10,
            effect_description="Poisoned - can't regain HP"
        )

        assert save.ongoing_damage is not None
        assert len(save.ongoing_damage) == 1
        assert save.duration_rounds == 10

    def test_attack_save_on_save_validation(self):
        """Should validate on_save literal values."""
        # Valid values should work
        save1 = AttackSave(ability="dex", dc=15, on_save="half")
        save2 = AttackSave(ability="dex", dc=15, on_save="none")
        save3 = AttackSave(ability="dex", dc=15, on_save="full")

        assert save1.on_save == "half"
        assert save2.on_save == "none"
        assert save3.on_save == "full"

    def test_attack_save_frozen(self):
        """Should be immutable."""
        save = AttackSave(ability="wis", dc=14)

        with pytest.raises(Exception):  # Pydantic frozen error
            save.dc = 15


class TestAttackWithAttackSave:
    """Tests for Attack model with attack_save field."""

    def test_attack_with_attack_save(self):
        """Should include optional attack_save field."""
        attack = Attack(
            name="Poison Bite",
            attack_type="melee",
            attack_bonus=4,
            reach=5,
            damage=[DamageFormula(number=1, denomination=6, bonus="+2", type="piercing")],
            attack_save=AttackSave(
                ability="con",
                dc=13,
                damage=[DamageFormula(number=2, denomination=6, bonus="", type="poison")],
                on_save="half"
            )
        )

        assert attack.attack_save is not None
        assert attack.attack_save.ability == "con"
        assert attack.attack_save.dc == 13

    def test_attack_without_attack_save(self):
        """Should work without attack_save (defaults to None)."""
        attack = Attack(
            name="Longsword",
            attack_type="melee",
            attack_bonus=5,
            reach=5,
            damage=[DamageFormula(number=1, denomination=8, bonus="+3", type="slashing")]
        )

        assert attack.attack_save is None
</file>

<file path="tests/foundry/actors/test_converter.py">
"""Tests for foundry.actors.converter."""

import pytest
from foundry.actors.converter import (
    convert_to_foundry,
    _generate_activity_id,
    _base_activity_structure,
    _create_attack_activity,
    _create_save_activity,
    _create_ongoing_damage_activity
)
from foundry.actors.models import ParsedActorData, Attack, Trait, DamageFormula, AttackSave


class TestConverter:
    """Tests for convert_to_foundry implementation."""

    async def test_converts_basic_actor(self):
        """Should convert minimal actor to FoundryVTT format."""
        goblin = ParsedActorData(
            source_statblock_name="Goblin",
            name="Goblin",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25,
            abilities={
                "STR": 8,
                "DEX": 14,
                "CON": 10,
                "INT": 10,
                "WIS": 8,
                "CHA": 8
            }
        )

        result, spell_uuids = await convert_to_foundry(goblin)

        # Check top-level structure
        assert result["name"] == "Goblin"
        assert result["type"] == "npc"
        assert "system" in result
        assert "items" in result

        # Check spell UUIDs (should be empty for basic actor)
        assert spell_uuids == []

        # Check abilities
        assert result["system"]["abilities"]["dex"]["value"] == 14
        assert result["system"]["abilities"]["str"]["value"] == 8

        # Check attributes
        assert result["system"]["attributes"]["ac"]["value"] == 15
        assert result["system"]["attributes"]["hp"]["max"] == 7

        # Check details
        assert result["system"]["details"]["cr"] == 0.25

    async def test_converts_actor_with_attacks(self):
        """Should convert attacks to weapon items with activities."""
        goblin = ParsedActorData(
            source_statblock_name="Goblin",
            name="Goblin",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25,
            abilities={"STR": 8, "DEX": 14, "CON": 10, "INT": 10, "WIS": 8, "CHA": 8},
            attacks=[
                Attack(
                    name="Scimitar",
                    attack_type="melee",
                    attack_bonus=4,
                    reach=5,
                    damage=[DamageFormula(number=1, denomination=6, bonus="+2", type="slashing")]
                )
            ]
        )

        result, spell_uuids = await convert_to_foundry(goblin)
        weapon = result["items"][0]

        # Check spell UUIDs (should be empty)
        assert spell_uuids == []

        # NEW v10+ structure checks
        assert "activities" in weapon["system"]
        assert len(weapon["system"]["activities"]) == 1

        # Verify attack activity
        activity = list(weapon["system"]["activities"].values())[0]
        assert activity["type"] == "attack"
        assert activity["attack"]["bonus"] == "4"
        assert activity["attack"]["flat"] == True

        # OLD v9 fields should be removed
        assert "attackBonus" not in weapon["system"]
        assert "parts" not in weapon["system"].get("damage", {})

        # NEW damage.base structure
        assert "base" in weapon["system"]["damage"]
        assert weapon["system"]["damage"]["base"]["number"] == 1
        assert weapon["system"]["damage"]["base"]["denomination"] == 6

    async def test_converts_actor_with_traits(self):
        """Should convert traits to feat items."""
        goblin = ParsedActorData(
            source_statblock_name="Goblin",
            name="Goblin",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25,
            abilities={"STR": 8, "DEX": 14, "CON": 10, "INT": 10, "WIS": 8, "CHA": 8},
            traits=[
                Trait(
                    name="Nimble Escape",
                    description="The goblin can take the Disengage or Hide action as a bonus action.",
                    activation="bonus"
                )
            ]
        )

        result, spell_uuids = await convert_to_foundry(goblin)

        # Check spell UUIDs (should be empty)
        assert spell_uuids == []

        # Check items array has feat
        assert len(result["items"]) == 1
        feat = result["items"][0]
        assert feat["name"] == "Nimble Escape"
        assert feat["type"] == "feat"

        # Check activation in v10+ activities structure
        assert "activities" in feat["system"]
        assert len(feat["system"]["activities"]) == 1
        activity = list(feat["system"]["activities"].values())[0]
        assert activity["activation"]["type"] == "bonus"

    def test_importable(self):
        """Converter should be importable from foundry.actors."""
        from foundry.actors import convert_to_foundry as imported_converter

        assert imported_converter is convert_to_foundry


class TestActivityHelpers:
    """Tests for activity generation helper functions."""

    def test_generate_activity_id(self):
        """Should generate unique 16-character IDs."""
        id1 = _generate_activity_id()
        id2 = _generate_activity_id()

        assert len(id1) == 16
        assert len(id2) == 16
        assert id1 != id2  # Should be unique

    def test_base_activity_structure(self):
        """Should return dict with all required base fields."""
        base = _base_activity_structure()

        assert "activation" in base
        assert "consumption" in base
        assert "description" in base
        assert "duration" in base
        assert "effects" in base
        assert "range" in base
        assert "target" in base
        assert "uses" in base

    def test_create_attack_activity(self):
        """Should create attack-type activity."""
        attack = Attack(
            name="Longsword",
            attack_type="melee",
            attack_bonus=5,
            reach=5,
            damage=[DamageFormula(number=1, denomination=8, bonus="+3", type="slashing")]
        )

        activity = _create_attack_activity(attack, "test123")

        assert activity["type"] == "attack"
        assert activity["_id"] == "test123"
        assert activity["attack"]["bonus"] == "5"
        assert activity["attack"]["flat"] == True
        assert activity["damage"]["includeBase"] == True

    def test_create_save_activity(self):
        """Should create save-type activity."""
        save = AttackSave(
            ability="con",
            dc=13,
            damage=[DamageFormula(number=2, denomination=6, bonus="", type="poison")],
            on_save="half"
        )

        activity = _create_save_activity(save, "save456")

        assert activity["type"] == "save"
        assert activity["_id"] == "save456"
        assert activity["save"]["ability"] == ["con"]
        assert activity["save"]["dc"]["formula"] == "13"
        assert activity["damage"]["onSave"] == "half"
        assert len(activity["damage"]["parts"]) == 1
        # Check object structure (v10+)
        part = activity["damage"]["parts"][0]
        assert part["number"] == 2
        assert part["denomination"] == 6
        assert part["bonus"] == ""
        assert part["types"] == ["poison"]

    def test_create_ongoing_damage_activity(self):
        """Should create damage-type activity for ongoing effects."""
        save = AttackSave(
            ability="con",
            dc=21,
            ongoing_damage=[DamageFormula(number=6, denomination=6, bonus="", type="poison")]
        )

        activity = _create_ongoing_damage_activity(save, "dmg789")

        assert activity["type"] == "damage"
        assert activity["_id"] == "dmg789"
        assert activity["activation"]["type"] == "turnStart"
        assert len(activity["damage"]["parts"]) == 1
        # Check object structure (v10+)
        part = activity["damage"]["parts"][0]
        assert part["number"] == 6
        assert part["denomination"] == 6
        assert part["bonus"] == ""
        assert part["types"] == ["poison"]


class TestWeaponActivities:
    """Tests for weapon conversion with v10+ activities structure."""

    async def test_converts_attack_with_save(self):
        """Should create weapon with attack + save activities."""
        actor = ParsedActorData(
            source_statblock_name="Test",
            name="Test",
            armor_class=15,
            hit_points=50,
            challenge_rating=2,
            abilities={"STR": 14, "DEX": 12, "CON": 13, "INT": 10, "WIS": 11, "CHA": 8},
            attacks=[
                Attack(
                    name="Poison Bite",
                    attack_type="melee",
                    attack_bonus=4,
                    reach=5,
                    damage=[DamageFormula(number=1, denomination=6, bonus="+2", type="piercing")],
                    attack_save=AttackSave(
                        ability="con",
                        dc=13,
                        damage=[DamageFormula(number=2, denomination=6, bonus="", type="poison")],
                        on_save="half"
                    )
                )
            ]
        )

        result, spell_uuids = await convert_to_foundry(actor)
        weapon = result["items"][0]

        # Check spell UUIDs (should be empty)
        assert spell_uuids == []

        # Should have 2 activities: attack + save
        assert len(weapon["system"]["activities"]) == 2

        activities = list(weapon["system"]["activities"].values())
        assert any(a["type"] == "attack" for a in activities)
        assert any(a["type"] == "save" for a in activities)

    async def test_converts_attack_with_ongoing_damage(self):
        """Should create weapon with attack + save + ongoing damage activities."""
        pit_fiend = ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend",
            armor_class=19,
            hit_points=300,
            challenge_rating=20,
            abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
            attacks=[
                Attack(
                    name="Bite",
                    attack_type="melee",
                    attack_bonus=14,
                    reach=5,
                    damage=[DamageFormula(number=4, denomination=6, bonus="+8", type="piercing")],
                    attack_save=AttackSave(
                        ability="con",
                        dc=21,
                        ongoing_damage=[DamageFormula(number=6, denomination=6, bonus="", type="poison")],
                        duration_rounds=10
                    )
                )
            ]
        )

        result, spell_uuids = await convert_to_foundry(pit_fiend)
        bite = result["items"][0]

        # Check spell UUIDs (should be empty)
        assert spell_uuids == []

        # Should have 3 activities: attack + save + ongoing damage
        assert len(bite["system"]["activities"]) == 3

        activities = list(bite["system"]["activities"].values())
        assert any(a["type"] == "attack" for a in activities)
        assert any(a["type"] == "save" for a in activities)
        assert any(a["type"] == "damage" for a in activities)

        # Verify ongoing damage has correct activation
        dmg_activity = [a for a in activities if a["type"] == "damage"][0]
        assert dmg_activity["activation"]["type"] == "turnStart"


class TestIconCacheIntegration:
    """Tests for icon cache integration with converter."""

    async def test_convert_to_foundry_uses_icon_cache(self):
        """Test converter selects appropriate icons from cache."""
        from foundry.actors.converter import convert_to_foundry
        from foundry.actors.models import ParsedActorData, Attack, DamageFormula
        from foundry.icon_cache import IconCache

        # Setup mock icon cache
        icon_cache = IconCache()
        icon_cache._all_icons = [
            "icons/weapons/swords/scimitar-guard-purple.webp",
            "icons/magic/fire/explosion-fireball.webp"
        ]
        icon_cache._icons_by_category = {
            "weapons": ["icons/weapons/swords/scimitar-guard-purple.webp"],
            "magic": ["icons/magic/fire/explosion-fireball.webp"]
        }
        icon_cache._loaded = True

        # Create test actor with scimitar attack
        parsed_actor = ParsedActorData(
            source_statblock_name="Goblin",
            name="Goblin",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25,
            abilities={"str": 8, "dex": 14, "con": 10, "int": 10, "wis": 8, "cha": 8},
            attacks=[
                Attack(
                    name="Scimitar",
                    attack_type="melee",
                    attack_bonus=4,
                    reach=5,
                    damage=[DamageFormula(number=1, denomination=6, bonus="+2", type="slashing")]
                )
            ]
        )

        foundry_json, _ = await convert_to_foundry(parsed_actor, icon_cache=icon_cache, use_ai_icons=False)

        # Find scimitar item
        scimitar = next(item for item in foundry_json['items'] if item['name'] == 'Scimitar')

        assert scimitar['img'] == "icons/weapons/swords/scimitar-guard-purple.webp"
</file>

<file path="tests/foundry/actors/test_multiattack.py">
"""Tests for multiattack parsing."""

import pytest
from foundry.actors.models import ParsedActorData, Multiattack
from foundry.actors.converter import convert_to_foundry


class TestMultiattackModel:
    """Tests for Multiattack model."""

    def test_basic_multiattack(self):
        """Should create basic multiattack."""
        multiattack = Multiattack(
            name="Multiattack",
            description="The pit fiend makes four attacks: one with its bite, one with its claw, one with its mace, and one with its tail.",
            num_attacks=4
        )

        assert multiattack.name == "Multiattack"
        assert multiattack.num_attacks == 4
        assert "four attacks" in multiattack.description

    def test_multiattack_with_options(self):
        """Should handle multiattack with options."""
        multiattack = Multiattack(
            name="Multiattack",
            description="The dragon can use its Frightful Presence. It then makes three attacks: one with its bite and two with its claws.",
            num_attacks=3
        )

        assert multiattack.num_attacks == 3

    def test_parsed_actor_with_multiattack(self):
        """Should include multiattack in ParsedActorData."""
        actor = ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend",
            armor_class=19,
            hit_points=300,
            challenge_rating=20,
            abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
            multiattack=Multiattack(
                name="Multiattack",
                description="Makes four attacks.",
                num_attacks=4
            )
        )

        assert actor.multiattack is not None
        assert actor.multiattack.num_attacks == 4


class TestMultiattackConversion:
    """Tests for converting multiattack to FoundryVTT format."""

    async def test_converts_multiattack_to_feat(self):
        """Should convert multiattack to feat item."""
        actor = ParsedActorData(
            source_statblock_name="Test",
            name="Test Creature",
            armor_class=15,
            hit_points=100,
            challenge_rating=5,
            abilities={"STR": 18, "DEX": 14, "CON": 16, "INT": 10, "WIS": 12, "CHA": 10},
            multiattack=Multiattack(
                name="Multiattack",
                description="The creature makes two attacks: one with its bite and one with its claws.",
                num_attacks=2
            )
        )

        result, spell_uuids = await convert_to_foundry(actor)

        # Check spell UUIDs (should be empty)
        assert spell_uuids == []

        # Should have multiattack feat in items
        feats = [item for item in result["items"] if item["type"] == "feat"]
        multiattack_feat = next((f for f in feats if f["name"] == "Multiattack"), None)

        assert multiattack_feat is not None
        assert multiattack_feat["system"]["activation"]["type"] == "action"
        assert "two attacks" in multiattack_feat["system"]["description"]["value"]
</file>

<file path="tests/foundry/actors/test_roundtrip_integration.py">
"""Integration tests for full ParsedActorData ‚Üí FoundryVTT ‚Üí verify round-trip."""

import pytest
import json
import os
from pathlib import Path
from dotenv import load_dotenv
from foundry.actors.models import ParsedActorData, Attack, DamageFormula, AttackSave
from foundry.actors.converter import convert_to_foundry
from foundry.client import FoundryClient

# Load environment variables from .env file
load_dotenv()


@pytest.mark.integration
@pytest.mark.requires_foundry
class TestActorRoundTrip:
    """
    Integration tests for complete actor upload/download cycle.

    These tests require:
    - FoundryVTT running locally
    - REST API relay server running
    - Valid API credentials in .env
    """

    @pytest.fixture
    def foundry_client(self):
        """Create FoundryVTT client for testing."""
        try:
            client = FoundryClient(target="local")
            # Test connection
            client.journals.get_all_journals_by_name("__test__")
            return client
        except Exception as e:
            pytest.skip(f"FoundryVTT not available: {e}")

    @pytest.fixture
    def goblin_data(self):
        """Load Goblin ParsedActorData from fixture."""
        fixture_path = Path(__file__).parent / "fixtures" / "goblin_parsed.json"
        with open(fixture_path) as f:
            data = json.load(f)
        return ParsedActorData(**data)

    @pytest.fixture
    def mage_data(self):
        """Load Mage ParsedActorData from fixture."""
        fixture_path = Path(__file__).parent / "fixtures" / "mage_parsed.json"
        with open(fixture_path) as f:
            data = json.load(f)
        return ParsedActorData(**data)

    async def test_upload_and_download_goblin(self, foundry_client, goblin_data):
        """
        Test complete cycle: Goblin ParsedActorData ‚Üí FoundryVTT ‚Üí verify.

        Steps:
        1. Convert ParsedActorData to FoundryVTT JSON
        2. Upload to FoundryVTT
        3. Download the actor back
        4. Verify all critical fields are preserved
        """
        # Step 1: Convert to FoundryVTT format
        foundry_json, spell_uuids = await convert_to_foundry(goblin_data)

        # Verify conversion produced valid structure
        assert foundry_json["name"] == "Goblin"
        assert foundry_json["type"] == "npc"
        assert len(foundry_json["items"]) == 3  # 2 attacks + 1 trait

        # Step 2: Upload to FoundryVTT
        actor_uuid = foundry_client.actors.create_actor(foundry_json)
        assert actor_uuid is not None
        assert actor_uuid.startswith("Actor.")

        # Step 3: Download back
        downloaded_actor = foundry_client.actors.get_actor(actor_uuid)

        # Step 4: Verify fields
        assert downloaded_actor["name"] == "Goblin"
        assert downloaded_actor["system"]["abilities"]["dex"]["value"] == 14
        assert downloaded_actor["system"]["attributes"]["ac"]["flat"] == 15
        assert downloaded_actor["system"]["attributes"]["hp"]["max"] == 7
        assert downloaded_actor["system"]["details"]["cr"] == 0.25

        # Verify items were preserved
        items = downloaded_actor["items"]
        assert len(items) == 3

        weapon_names = [item["name"] for item in items if item["type"] == "weapon"]
        assert "Scimitar" in weapon_names
        assert "Shortbow" in weapon_names

        trait_names = [item["name"] for item in items if item["type"] == "feat"]
        assert "Nimble Escape" in trait_names

    async def test_upload_and_download_mage(self, foundry_client, mage_data):
        """
        Test complete cycle: Mage ParsedActorData ‚Üí FoundryVTT ‚Üí verify.

        This tests a more complex actor with:
        - Spellcasting
        - Multiple spell levels
        - Saving throw proficiencies
        """
        # Step 1: Convert to FoundryVTT format
        # Use backward compatibility mode since mage_data has fake spell UUIDs
        foundry_json, spell_uuids = await convert_to_foundry(mage_data, include_spells_in_payload=True)

        # Verify conversion
        assert foundry_json["name"] == "Mage"
        assert foundry_json["type"] == "npc"
        # Backward compat mode: Spells IN payload
        assert len(foundry_json["items"]) == 17  # 1 attack + 16 spells

        # Verify spellcasting attributes
        assert foundry_json["system"]["attributes"]["spellcasting"] == "int"
        assert foundry_json["system"]["attributes"]["spelldc"] == 14

        # Step 2: Upload to FoundryVTT (no spell_uuids needed - already in payload)
        actor_uuid = foundry_client.actors.create_actor(foundry_json)
        assert actor_uuid is not None
        assert actor_uuid.startswith("Actor.")

        # Step 3: Download and verify
        downloaded_actor = foundry_client.actors.get_actor(actor_uuid)

        # FoundryVTT may transform spellcasting attributes on storage
        # Just verify the actor was created and items preserved
        assert downloaded_actor["name"] == "Mage"
        assert downloaded_actor["type"] == "npc"

        spells = [item for item in downloaded_actor["items"] if item["type"] == "spell"]
        assert len(spells) == 16

        # Verify Fireball spell exists (UUID may be transformed by FoundryVTT)
        fireball = next(s for s in spells if s["name"] == "Fireball")
        assert fireball is not None

    async def test_conversion_preserves_all_attacks(self, goblin_data):
        """Verify attacks are correctly converted to weapon items with activities."""
        foundry_json, spell_uuids = await convert_to_foundry(goblin_data)

        weapons = [item for item in foundry_json["items"] if item["type"] == "weapon"]
        assert len(weapons) == 2

        # Check Scimitar (v10+ structure)
        scimitar = next(w for w in weapons if w["name"] == "Scimitar")
        assert "activities" in scimitar["system"]
        assert len(scimitar["system"]["activities"]) == 1

        # Verify attack activity
        scimitar_activity = list(scimitar["system"]["activities"].values())[0]
        assert scimitar_activity["type"] == "attack"
        assert scimitar_activity["attack"]["bonus"] == "4"
        assert scimitar_activity["attack"]["type"]["value"] == "melee"

        # Check damage.base structure (v10+)
        assert "base" in scimitar["system"]["damage"]
        assert scimitar["system"]["damage"]["base"]["number"] == 1
        assert scimitar["system"]["damage"]["base"]["denomination"] == 6
        assert scimitar["system"]["damage"]["base"]["types"] == ["slashing"]

        # Check Shortbow
        shortbow = next(w for w in weapons if w["name"] == "Shortbow")
        assert "activities" in shortbow["system"]
        shortbow_activity = list(shortbow["system"]["activities"].values())[0]
        assert shortbow_activity["attack"]["type"]["value"] == "ranged"
        assert shortbow["system"]["range"]["value"] == 80
        assert shortbow["system"]["range"]["long"] == 320

    async def test_conversion_preserves_all_traits(self, goblin_data):
        """Verify traits are correctly converted to feat items."""
        foundry_json, spell_uuids = await convert_to_foundry(goblin_data)

        feats = [item for item in foundry_json["items"] if item["type"] == "feat"]
        assert len(feats) == 1

        nimble_escape = feats[0]
        assert nimble_escape["name"] == "Nimble Escape"
        assert nimble_escape["system"]["activation"]["type"] == "bonus"
        assert "Disengage or Hide" in nimble_escape["system"]["description"]["value"]

    async def test_conversion_preserves_spells(self, mage_data):
        """Verify spells are correctly returned as UUIDs (not in payload)."""
        foundry_json, spell_uuids = await convert_to_foundry(mage_data)

        # NEW behavior: Spells NOT in payload
        spells = [item for item in foundry_json["items"] if item["type"] == "spell"]
        assert len(spells) == 0, "Spells should NOT be in CREATE payload"

        # Spells should be in spell_uuids
        assert len(spell_uuids) == 16
        # All should be compendium UUIDs
        assert all(uuid.startswith("Compendium.") for uuid in spell_uuids)

    async def test_conversion_structure_matches_foundry_format(self, goblin_data):
        """Verify converted JSON matches FoundryVTT expected structure."""
        foundry_json, spell_uuids = await convert_to_foundry(goblin_data)

        # Check required top-level fields
        assert "name" in foundry_json
        assert "type" in foundry_json
        assert "system" in foundry_json
        assert "items" in foundry_json
        assert "effects" in foundry_json
        assert "flags" in foundry_json

        # Check system structure
        system = foundry_json["system"]
        assert "abilities" in system
        assert "attributes" in system
        assert "details" in system
        assert "traits" in system

        # Check abilities have correct structure
        for ability in ["str", "dex", "con", "int", "wis", "cha"]:
            assert ability in system["abilities"]
            assert "value" in system["abilities"][ability]
            assert "proficient" in system["abilities"][ability]

        # Check attributes structure
        assert "ac" in system["attributes"]
        assert "hp" in system["attributes"]
        assert "movement" in system["attributes"]

    async def test_weapon_activities_functional_in_foundry(self, foundry_client):
        """Weapons should have working attack buttons in FoundryVTT."""
        goblin = ParsedActorData(
            source_statblock_name="Goblin",
            name="Goblin Activities Test",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25,
            abilities={"STR": 8, "DEX": 14, "CON": 10, "INT": 10, "WIS": 8, "CHA": 8},
            attacks=[Attack(
                name="Scimitar",
                attack_type="melee",
                attack_bonus=4,
                reach=5,
                damage=[DamageFormula(number=1, denomination=6, bonus="+2", type="slashing")]
            )]
        )

        foundry_json, spell_uuids = await convert_to_foundry(goblin)
        actor_uuid = foundry_client.actors.create_actor(foundry_json)

        # Download and verify
        downloaded = foundry_client.actors.get_actor(actor_uuid)
        weapon = [i for i in downloaded["items"] if i["type"] == "weapon"][0]

        # CRITICAL: Verify activities are present and correct
        assert "activities" in weapon["system"]
        assert len(weapon["system"]["activities"]) > 0

        # Verify attack activity has required functional fields
        attack_activity = [a for a in weapon["system"]["activities"].values()
                           if a["type"] == "attack"][0]
        assert attack_activity["attack"]["bonus"] == "4"
        assert attack_activity["attack"]["flat"] == True
        assert "damage" in attack_activity

    async def test_pit_fiend_bite_full_complexity(self, foundry_client):
        """Pit Fiend bite should have attack + save + ongoing damage."""
        pit_fiend = ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend Full Complexity Test",
            armor_class=19,
            hit_points=300,
            challenge_rating=20,
            abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
            attacks=[Attack(
                name="Bite",
                attack_type="melee",
                attack_bonus=14,
                reach=5,
                damage=[DamageFormula(number=4, denomination=6, bonus="+8", type="piercing")],
                attack_save=AttackSave(
                    ability="con",
                    dc=21,
                    ongoing_damage=[DamageFormula(number=6, denomination=6, bonus="", type="poison")],
                    duration_rounds=10
                )
            )]
        )

        foundry_json, spell_uuids = await convert_to_foundry(pit_fiend)
        actor_uuid = foundry_client.actors.create_actor(foundry_json)

        downloaded = foundry_client.actors.get_actor(actor_uuid)
        bite = [i for i in downloaded["items"] if i["name"] == "Bite"][0]

        # Should have 3 activities
        assert len(bite["system"]["activities"]) == 3

        # Verify each activity type
        activities = bite["system"]["activities"].values()
        assert any(a["type"] == "attack" for a in activities)
        assert any(a["type"] == "save" for a in activities)
        assert any(a["type"] == "damage" for a in activities)

        # Verify save details
        save_activity = [a for a in activities if a["type"] == "save"][0]
        assert save_activity["save"]["ability"] == ["con"]
        assert save_activity["save"]["dc"]["formula"] == "21"

        # Verify ongoing damage details (v10+ object structure)
        dmg_activity = [a for a in activities if a["type"] == "damage"][0]
        assert dmg_activity["activation"]["type"] == "turnStart"
        dmg_part = dmg_activity["damage"]["parts"][0]
        assert dmg_part["number"] == 6
        assert dmg_part["denomination"] == 6
        assert dmg_part["types"] == ["poison"]
</file>

<file path="tests/foundry/actors/test_spell_cache.py">
"""Tests for foundry.actors.spell_cache."""

import pytest
from unittest.mock import patch, MagicMock
from foundry.actors.spell_cache import SpellCache


class TestSpellCache:
    """Tests for SpellCache class."""

    @pytest.fixture
    def mock_spells(self):
        """Sample spell data from FoundryVTT."""
        return [
            {
                'name': 'Fireball',
                'uuid': 'Compendium.dnd5e.spells.Item.ztgcdrWPshKRpFd0',
                'level': 3,
                'school': 'evo'
            },
            {
                'name': 'Magic Missile',
                'uuid': 'Compendium.dnd5e.spells.Item.abc123',
                'level': 1,
                'school': 'evo'
            },
            {
                'name': 'Cure Wounds',
                'uuid': 'Compendium.dnd5e.spells.Item.def456',
                'level': 1,
                'school': 'evo'
            }
        ]

    def test_init(self):
        """Should create empty cache."""
        cache = SpellCache()

        assert not cache.loaded
        assert cache.spell_count == 0

    @patch('foundry.actors.spell_cache.fetch_all_spells')
    def test_load(self, mock_fetch, mock_spells):
        """Should load spells from FoundryVTT."""
        mock_fetch.return_value = mock_spells

        cache = SpellCache()
        cache.load()

        assert cache.loaded
        assert cache.spell_count == 3
        mock_fetch.assert_called_once()

    @patch('foundry.actors.spell_cache.fetch_all_spells')
    def test_get_spell_uuid(self, mock_fetch, mock_spells):
        """Should return UUID for spell by name."""
        mock_fetch.return_value = mock_spells

        cache = SpellCache()
        cache.load()

        # Exact match
        assert cache.get_spell_uuid('Fireball') == 'Compendium.dnd5e.spells.Item.ztgcdrWPshKRpFd0'

        # Case-insensitive
        assert cache.get_spell_uuid('fireball') == 'Compendium.dnd5e.spells.Item.ztgcdrWPshKRpFd0'
        assert cache.get_spell_uuid('FIREBALL') == 'Compendium.dnd5e.spells.Item.ztgcdrWPshKRpFd0'

        # Not found
        assert cache.get_spell_uuid('Nonexistent Spell') is None

    @patch('foundry.actors.spell_cache.fetch_all_spells')
    def test_get_spell_data(self, mock_fetch, mock_spells):
        """Should return full spell data."""
        mock_fetch.return_value = mock_spells

        cache = SpellCache()
        cache.load()

        spell = cache.get_spell_data('Magic Missile')

        assert spell is not None
        assert spell['name'] == 'Magic Missile'
        assert spell['uuid'] == 'Compendium.dnd5e.spells.Item.abc123'
        assert spell['level'] == 1

    @patch('foundry.actors.spell_cache.fetch_all_spells')
    def test_get_before_load(self, mock_fetch):
        """Should return None if called before load()."""
        cache = SpellCache()

        # Should log warning and return None
        assert cache.get_spell_uuid('Fireball') is None
        assert cache.get_spell_data('Fireball') is None

        # Should not have called fetch
        mock_fetch.assert_not_called()

    @patch('foundry.actors.spell_cache.fetch_all_spells')
    def test_load_with_credentials(self, mock_fetch, mock_spells):
        """Should pass credentials to fetch_all_spells()."""
        mock_fetch.return_value = mock_spells

        cache = SpellCache()
        cache.load(
            relay_url='https://example.com',
            api_key='test-key',
            client_id='test-client'
        )

        mock_fetch.assert_called_once_with(
            relay_url='https://example.com',
            api_key='test-key',
            client_id='test-client'
        )
</file>

<file path="tests/foundry/items/__init__.py">
"""Tests for foundry.items module."""
</file>

<file path="tests/foundry/items/test_deduplicate.py">
"""Tests for foundry.items.deduplicate module."""

import pytest
from foundry.items.deduplicate import (
    get_source_priority,
    deduplicate_items,
    get_source_stats
)


class TestGetSourcePriority:
    """Tests for get_source_priority function."""

    def test_players_handbook_highest_priority(self):
        """Player's Handbook should have highest priority (0)."""
        uuid = "Compendium.dnd-players-handbook.spells.phbsplFireball"
        assert get_source_priority(uuid) == 0

    def test_2024_rules_second_priority(self):
        """D&D 5e 2024 rules should have second priority (1)."""
        # Test .24 suffix
        uuid1 = "Compendium.dnd5e.spells24.phbsplFireball"
        assert get_source_priority(uuid1) == 1

        # Test .items24
        uuid2 = "Compendium.dnd5e.items24.sword123"
        assert get_source_priority(uuid2) == 1

        # Test .24 anywhere
        uuid3 = "Compendium.dnd5e.equipment.24sword"
        assert get_source_priority(uuid3) == 1

    def test_srd_third_priority(self):
        """D&D 5e SRD should have third priority (2)."""
        uuid = "Compendium.dnd5e.spells.Fireball123"
        assert get_source_priority(uuid) == 2

    def test_other_sources_lowest_priority(self):
        """Other sources should have lowest priority (3)."""
        uuid = "Compendium.homebrew.spells.CustomFireball"
        assert get_source_priority(uuid) == 3

    def test_empty_uuid(self):
        """Empty UUID should return lowest priority."""
        assert get_source_priority("") == 3


class TestDeduplicateItems:
    """Tests for deduplicate_items function."""

    def test_no_duplicates(self):
        """Items with unique names should all be kept."""
        items = [
            {"name": "Fireball", "uuid": "Compendium.dnd5e.spells.1"},
            {"name": "Lightning Bolt", "uuid": "Compendium.dnd5e.spells.2"},
            {"name": "Magic Missile", "uuid": "Compendium.dnd5e.spells.3"},
        ]
        result = deduplicate_items(items, verbose=False)
        assert len(result) == 3

    def test_duplicates_phb_wins(self):
        """Player's Handbook version should be kept over SRD."""
        items = [
            {"name": "Fireball", "uuid": "Compendium.dnd5e.spells.srd123"},
            {"name": "Fireball", "uuid": "Compendium.dnd-players-handbook.spells.phb123"},
        ]
        result = deduplicate_items(items, verbose=False)
        assert len(result) == 1
        assert "dnd-players-handbook" in result[0]["uuid"]

    def test_duplicates_2024_wins_over_srd(self):
        """2024 rules should be kept over classic SRD."""
        items = [
            {"name": "Fireball", "uuid": "Compendium.dnd5e.spells.srd123"},
            {"name": "Fireball", "uuid": "Compendium.dnd5e.spells24.new123"},
        ]
        result = deduplicate_items(items, verbose=False)
        assert len(result) == 1
        assert "spells24" in result[0]["uuid"]

    def test_multiple_duplicates(self):
        """Should keep highest priority among multiple duplicates."""
        items = [
            {"name": "Fireball", "uuid": "Compendium.homebrew.spells.custom"},
            {"name": "Fireball", "uuid": "Compendium.dnd5e.spells.srd123"},
            {"name": "Fireball", "uuid": "Compendium.dnd-players-handbook.spells.phb123"},
            {"name": "Fireball", "uuid": "Compendium.dnd5e.spells24.new123"},
        ]
        result = deduplicate_items(items, verbose=False)
        assert len(result) == 1
        assert "dnd-players-handbook" in result[0]["uuid"]

    def test_sorted_output(self):
        """Output should be sorted by dedupe_key."""
        items = [
            {"name": "Zebra Spell", "uuid": "Compendium.dnd5e.spells.1"},
            {"name": "Apple Spell", "uuid": "Compendium.dnd5e.spells.2"},
            {"name": "Monkey Spell", "uuid": "Compendium.dnd5e.spells.3"},
        ]
        result = deduplicate_items(items, verbose=False)
        assert result[0]["name"] == "Apple Spell"
        assert result[1]["name"] == "Monkey Spell"
        assert result[2]["name"] == "Zebra Spell"

    def test_custom_dedupe_key(self):
        """Should support custom deduplication key."""
        items = [
            {"id": "123", "name": "Spell A", "uuid": "Compendium.dnd5e.spells.1"},
            {"id": "123", "name": "Spell A", "uuid": "Compendium.dnd-players-handbook.spells.2"},
        ]
        result = deduplicate_items(items, dedupe_key="id", verbose=False)
        assert len(result) == 1

    def test_empty_items(self):
        """Empty list should return empty list."""
        result = deduplicate_items([], verbose=False)
        assert result == []

    def test_items_without_dedupe_key(self):
        """Items without dedupe_key should be filtered out."""
        items = [
            {"name": "Valid Spell", "uuid": "Compendium.dnd5e.spells.1"},
            {"uuid": "Compendium.dnd5e.spells.2"},  # No name
        ]
        result = deduplicate_items(items, verbose=False)
        assert len(result) == 1
        assert result[0]["name"] == "Valid Spell"

    def test_whitespace_handling(self):
        """Names with different whitespace should be treated as same."""
        items = [
            {"name": "  Fireball  ", "uuid": "Compendium.dnd5e.spells.1"},
            {"name": "Fireball", "uuid": "Compendium.dnd-players-handbook.spells.2"},
        ]
        result = deduplicate_items(items, verbose=False)
        assert len(result) == 1


class TestGetSourceStats:
    """Tests for get_source_stats function."""

    def test_single_source(self):
        """Should count items from single source."""
        items = [
            {"uuid": "Compendium.dnd-players-handbook.spells.1"},
            {"uuid": "Compendium.dnd-players-handbook.spells.2"},
            {"uuid": "Compendium.dnd-players-handbook.spells.3"},
        ]
        stats = get_source_stats(items)
        assert stats["Player's Handbook"] == 3

    def test_multiple_sources(self):
        """Should count items from multiple sources."""
        items = [
            {"uuid": "Compendium.dnd-players-handbook.spells.1"},
            {"uuid": "Compendium.dnd-players-handbook.spells.2"},
            {"uuid": "Compendium.dnd5e.spells.3"},
            {"uuid": "Compendium.dnd5e.spells24.4"},
            {"uuid": "Compendium.homebrew.spells.5"},
        ]
        stats = get_source_stats(items)
        assert stats["Player's Handbook"] == 2
        assert stats["D&D 5e SRD"] == 1
        assert stats["D&D 5e 2024"] == 1
        assert stats["Other"] == 1

    def test_2024_detection(self):
        """Should correctly detect 2024 rules items."""
        items = [
            {"uuid": "Compendium.dnd5e.spells24.abc"},
            {"uuid": "Compendium.dnd5e.items24.def"},
            {"uuid": "Compendium.dnd5e.equipment.24ghi"},
        ]
        stats = get_source_stats(items)
        assert stats["D&D 5e 2024"] == 3

    def test_empty_items(self):
        """Empty list should return empty dict."""
        stats = get_source_stats([])
        assert stats == {}

    def test_items_without_uuid(self):
        """Items without UUID should be counted as Other."""
        items = [
            {"name": "Test Item"},
            {"uuid": ""},
        ]
        stats = get_source_stats(items)
        assert stats.get("Other", 0) == 2
</file>

<file path="tests/foundry/items/test_manager.py">
"""Tests for foundry.items.manager module (ItemManager)."""

import pytest
from unittest.mock import patch, MagicMock
from foundry.items.manager import ItemManager


class TestItemManager:
    """Tests for ItemManager class."""

    def setup_method(self):
        """Setup test fixtures."""
        self.manager = ItemManager(
            relay_url="http://localhost:3010",
            foundry_url="http://localhost:30000",
            api_key="test-api-key",
            client_id="test-client-id"
        )

    @patch('foundry.items.manager.requests.get')
    def test_get_all_items_by_name(self, mock_get):
        """Should search for items by name."""
        mock_response = MagicMock()
        mock_response.json.return_value = {
            'results': [
                {'uuid': 'Compendium.dnd5e.items.1', 'name': 'Longsword'},
                {'uuid': 'Compendium.dnd5e.items.2', 'name': 'Longsword +1'},
            ]
        }
        mock_get.return_value = mock_response

        results = self.manager.get_all_items_by_name("Longsword")

        assert len(results) == 2
        assert results[0]['name'] == 'Longsword'
        mock_get.assert_called_once()

    @patch('foundry.items.manager.requests.get')
    def test_get_item_by_name(self, mock_get):
        """Should return first item matching name."""
        mock_response = MagicMock()
        mock_response.json.return_value = {
            'results': [
                {'uuid': 'Compendium.dnd5e.items.1', 'name': 'Longsword'},
            ]
        }
        mock_get.return_value = mock_response

        result = self.manager.get_item_by_name("Longsword")

        assert result is not None
        assert result['name'] == 'Longsword'

    @patch('foundry.items.manager.requests.get')
    def test_get_item_by_name_not_found(self, mock_get):
        """Should return None if item not found."""
        mock_response = MagicMock()
        mock_response.json.return_value = {'results': []}
        mock_get.return_value = mock_response

        result = self.manager.get_item_by_name("NonexistentItem")

        assert result is None

    @patch('foundry.items.manager.requests.get')
    def test_get_item_by_uuid(self, mock_get):
        """Should fetch item by UUID."""
        mock_response = MagicMock()
        mock_response.json.return_value = {
            'uuid': 'Compendium.dnd5e.items.123',
            'name': 'Longsword',
            'type': 'weapon'
        }
        mock_get.return_value = mock_response

        result = self.manager.get_item("Compendium.dnd5e.items.123")

        assert result['uuid'] == 'Compendium.dnd5e.items.123'
        assert result['name'] == 'Longsword'

    @patch('foundry.items.manager.requests.get')
    def test_search_with_filter(self, mock_get):
        """Should use filter parameter in search."""
        mock_response = MagicMock()
        mock_response.json.return_value = {'results': []}
        mock_get.return_value = mock_response

        self.manager.get_all_items_by_name("Test")

        # Verify filter was set to "Item"
        call_args = mock_get.call_args
        assert call_args[1]['params']['filter'] == 'Item'

    @patch('foundry.items.manager.requests.get')
    def test_search_error_handling(self, mock_get):
        """Should raise RuntimeError on API error."""
        import requests

        mock_get.side_effect = requests.exceptions.RequestException("Network error")

        with pytest.raises(RuntimeError, match="Failed to search for item"):
            self.manager.get_all_items_by_name("Test")

    @patch('foundry.items.manager.requests.get')
    def test_get_item_error_handling(self, mock_get):
        """Should raise RuntimeError on get error."""
        import requests

        mock_get.side_effect = requests.exceptions.RequestException("Network error")

        with pytest.raises(RuntimeError, match="Failed to get item"):
            self.manager.get_item("Compendium.dnd5e.items.123")

    @patch('foundry.items.manager.requests.get')
    def test_handles_list_response(self, mock_get):
        """Should handle response that is a list instead of dict with 'results'."""
        mock_response = MagicMock()
        mock_response.json.return_value = [
            {'uuid': 'Compendium.dnd5e.items.1', 'name': 'Longsword'},
        ]
        mock_get.return_value = mock_response

        results = self.manager.get_all_items_by_name("Longsword")

        assert len(results) == 1
        assert results[0]['name'] == 'Longsword'
</file>

<file path="tests/foundry/__init__.py">
"""Tests for FoundryVTT integration module."""
</file>

<file path="tests/foundry/test_actors.py">
"""Tests for FoundryVTT Actor manager."""

import pytest
from unittest.mock import Mock, patch
from src.foundry.actors import ActorManager


@pytest.mark.unit
class TestActorManagerSearch:
    """Test actor search operations."""

    def test_search_all_compendiums_found(self):
        """Test searching for actor returns UUID when found."""
        manager = ActorManager(
            relay_url="http://test",
            foundry_url="http://test",
            api_key="test",
            client_id="test"
        )

        # Mock search response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = [
            {"uuid": "Actor.abc123", "name": "Goblin", "type": "npc"}
        ]

        with patch('requests.get', return_value=mock_response):
            uuid = manager.search_all_compendiums("Goblin")

        assert uuid == "Actor.abc123"

    def test_search_all_compendiums_not_found(self):
        """Test searching for actor returns None when not found."""
        manager = ActorManager(
            relay_url="http://test",
            foundry_url="http://test",
            api_key="test",
            client_id="test"
        )

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = []

        with patch('requests.get', return_value=mock_response):
            uuid = manager.search_all_compendiums("Nonexistent")

        assert uuid is None

    def test_search_handles_network_error(self):
        """Test search handles network errors gracefully."""
        manager = ActorManager(
            relay_url="http://test",
            foundry_url="http://test",
            api_key="test",
            client_id="test"
        )

        with patch('requests.get', side_effect=Exception("Network error")):
            uuid = manager.search_all_compendiums("Goblin")

        assert uuid is None


@pytest.mark.unit
class TestActorManagerCreate:
    """Test actor creation operations."""

    def test_create_creature_actor(self):
        """Test creating a creature actor from stat block."""
        from src.actors.models import StatBlock

        manager = ActorManager(
            relay_url="http://test",
            foundry_url="http://test",
            api_key="test",
            client_id="test"
        )

        stat_block = StatBlock(
            name="Goblin",
            raw_text="Goblin stat block...",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25,
            size="Small",
            type="humanoid",
            abilities={"STR": 8, "DEX": 14, "CON": 10, "INT": 10, "WIS": 8, "CHA": 8}
        )

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "abc123"},
            "uuid": "Actor.abc123"
        }

        with patch('requests.post', return_value=mock_response) as mock_post:
            uuid = manager.create_creature_actor(stat_block)

        assert uuid == "Actor.abc123"

        # Verify request payload
        call_args = mock_post.call_args
        payload = call_args[1]["json"]
        assert payload["entityType"] == "Actor"
        assert payload["data"]["name"] == "Goblin"
        assert payload["data"]["type"] == "npc"
        assert payload["data"]["system"]["attributes"]["ac"]["value"] == 15
        assert payload["data"]["system"]["attributes"]["hp"]["value"] == 7

    def test_create_npc_actor_with_stat_block_link(self):
        """Test creating NPC actor with link to creature stat block."""
        from src.actors.models import NPC

        manager = ActorManager(
            relay_url="http://test",
            foundry_url="http://test",
            api_key="test",
            client_id="test"
        )

        npc = NPC(
            name="Klarg",
            creature_stat_block_name="Goblin Boss",
            description="Leader of the Cragmaw goblins",
            plot_relevance="Guards the stolen supplies",
            location="Cragmaw Hideout"
        )

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "xyz789"},
            "uuid": "Actor.xyz789"
        }

        with patch('requests.post', return_value=mock_response) as mock_post:
            uuid = manager.create_npc_actor(npc, stat_block_uuid="Actor.boss123")

        assert uuid == "Actor.xyz789"

        # Verify biography includes stat block link
        payload = mock_post.call_args[1]["json"]
        bio = payload["data"]["system"]["details"]["biography"]["value"]
        assert "Klarg" in bio
        assert "Leader of the Cragmaw goblins" in bio
        assert "@UUID[Actor.boss123]" in bio

    def test_create_npc_actor_without_stat_block(self):
        """Test creating NPC actor when stat block not found."""
        from src.actors.models import NPC

        manager = ActorManager(
            relay_url="http://test",
            foundry_url="http://test",
            api_key="test",
            client_id="test"
        )

        npc = NPC(
            name="Mysterious Stranger",
            creature_stat_block_name="Unknown",
            description="A hooded figure",
            plot_relevance="Provides quest information"
        )

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "mystery123"},
            "uuid": "Actor.mystery123"
        }

        with patch('requests.post', return_value=mock_response) as mock_post:
            uuid = manager.create_npc_actor(npc, stat_block_uuid=None)

        assert uuid == "Actor.mystery123"

        # Verify no stat block link in biography
        payload = mock_post.call_args[1]["json"]
        bio = payload["data"]["system"]["details"]["biography"]["value"]
        assert "@UUID" not in bio
</file>

<file path="tests/foundry/test_icon_cache.py">
"""Test suite for FoundryVTT icon cache."""

import pytest
from foundry.icon_cache import IconCache


def test_icon_cache_initialization():
    """Test IconCache initializes with empty state."""
    cache = IconCache()

    assert not cache.loaded
    assert cache.icon_count == 0


@pytest.mark.integration
def test_icon_cache_load():
    """Test loading icons from FoundryVTT file system."""
    import os
    from unittest.mock import patch, MagicMock

    # Mock file system API response
    mock_response = MagicMock()
    mock_response.json.return_value = {
        "success": True,
        "files": [
            {"name": "sword-steel.webp", "path": "icons/weapons/swords/sword-steel.webp", "type": "file"},
            {"name": "fireball.webp", "path": "icons/magic/fire/fireball.webp", "type": "file"},
            {"name": "plate-armor.webp", "path": "icons/equipment/chest/plate-armor.webp", "type": "file"}
        ]
    }
    mock_response.raise_for_status = MagicMock()

    with patch('requests.get', return_value=mock_response):
        cache = IconCache()
        cache.load(
            relay_url="http://test",
            api_key="test_key",
            client_id="test_client"
        )

        assert cache.loaded
        assert cache.icon_count == 3
        assert "icons/weapons/swords/sword-steel.webp" in cache._all_icons

        # Test hierarchical categorization
        assert "weapons" in cache._icons_by_category
        assert "weapons/swords" in cache._icons_by_category
        assert "magic" in cache._icons_by_category
        assert "magic/fire" in cache._icons_by_category
        assert "equipment" in cache._icons_by_category
        assert "equipment/chest" in cache._icons_by_category

        # Verify paths are in correct categories
        assert "icons/weapons/swords/sword-steel.webp" in cache._icons_by_category["weapons"]
        assert "icons/weapons/swords/sword-steel.webp" in cache._icons_by_category["weapons/swords"]
        assert "icons/magic/fire/fireball.webp" in cache._icons_by_category["magic"]
        assert "icons/magic/fire/fireball.webp" in cache._icons_by_category["magic/fire"]


def test_categorize_icon_preserves_all_hierarchy_levels():
    """Test that _categorize_icon preserves all directory levels."""
    cache = IconCache()

    # Test deep hierarchy: icons/weapons/swords/longswords/magical/flaming-sword.webp
    deep_path = "icons/weapons/swords/longswords/magical/flaming-sword.webp"
    cache._all_icons.append(deep_path)
    cache._categorize_icon(deep_path)

    # Should create categories at all levels
    assert "weapons" in cache._icons_by_category
    assert "weapons/swords" in cache._icons_by_category
    assert "weapons/swords/longswords" in cache._icons_by_category
    assert "weapons/swords/longswords/magical" in cache._icons_by_category

    # All levels should contain the path
    assert deep_path in cache._icons_by_category["weapons"]
    assert deep_path in cache._icons_by_category["weapons/swords"]
    assert deep_path in cache._icons_by_category["weapons/swords/longswords"]
    assert deep_path in cache._icons_by_category["weapons/swords/longswords/magical"]


def test_get_icon_with_fuzzy_matching():
    """Test fuzzy matching for icon selection with hierarchical categories."""
    cache = IconCache()
    cache._all_icons = [
        "icons/weapons/swords/scimitar-curved-blue.webp",
        "icons/weapons/axes/axe-battle-worn.webp",
        "icons/magic/fire/fireball-explosion.webp"
    ]
    cache._icons_by_category = {
        "weapons": [
            "icons/weapons/swords/scimitar-curved-blue.webp",
            "icons/weapons/axes/axe-battle-worn.webp"
        ],
        "weapons/swords": ["icons/weapons/swords/scimitar-curved-blue.webp"],
        "weapons/axes": ["icons/weapons/axes/axe-battle-worn.webp"],
        "magic": ["icons/magic/fire/fireball-explosion.webp"],
        "magic/fire": ["icons/magic/fire/fireball-explosion.webp"]
    }
    cache._loaded = True

    # Test top-level category match
    icon = cache.get_icon("scimitar", category="weapons")
    assert icon == "icons/weapons/swords/scimitar-curved-blue.webp"

    # Test subcategory match (more specific)
    icon = cache.get_icon("scimitar", category="weapons/swords")
    assert icon == "icons/weapons/swords/scimitar-curved-blue.webp"

    # Test fuzzy match without category
    icon = cache.get_icon("fire ball")
    assert icon == "icons/magic/fire/fireball-explosion.webp"

    # Test no match returns None
    icon = cache.get_icon("nonexistent item")
    assert icon is None


def test_get_icon_by_keywords():
    """Test keyword matching for icon selection."""
    cache = IconCache()
    cache._all_icons = [
        "icons/weapons/swords/sword-steel.webp",
        "icons/weapons/swords/blade-curved.webp",
    ]
    cache._icons_by_category = {"weapons": cache._all_icons[:]}
    cache._loaded = True

    # Test first keyword matches
    icon = cache.get_icon_by_keywords(["sword", "blade"], category="weapons")
    assert icon == "icons/weapons/swords/sword-steel.webp"

    # Test second keyword matches
    icon = cache.get_icon_by_keywords(["scimitar", "blade"], category="weapons")
    assert icon == "icons/weapons/swords/blade-curved.webp"

    # Test no match
    icon = cache.get_icon_by_keywords(["axe", "hammer"], category="weapons")
    assert icon is None
</file>

<file path="tests/foundry/test_xml_to_journal_html.py">
"""Tests for XML to Journal HTML converter."""

import pytest
from pathlib import Path
from src.foundry.xml_to_journal_html import convert_xml_to_journal_data


class TestXMLToJournalConverter:
    """Tests for XML to Journal HTML conversion."""

    def test_convert_single_xml_file(self, tmp_path):
        """Test converting a single XML file to journal data.

        Uses XML structure that matches xml_to_html_content expectations:
        - title/chapter_title -> h1
        - section (with text) -> h2
        - p -> p
        """
        xml_content = """<?xml version="1.0" encoding="UTF-8"?>
<chapter>
    <title>Test Chapter</title>
    <section>Introduction
        <p>This is a test paragraph.</p>
    </section>
</chapter>"""

        xml_file = tmp_path / "01_Test_Chapter.xml"
        xml_file.write_text(xml_content)

        result = convert_xml_to_journal_data(str(xml_file))

        assert result["name"] == "01_Test_Chapter"
        assert "<h1>Test Chapter</h1>" in result["html"]
        assert "<h2>Introduction</h2>" in result["html"]
        assert "<p>This is a test paragraph.</p>" in result["html"]
        assert "metadata" in result
        assert result["metadata"]["source_file"] == str(xml_file)

    def test_convert_multiple_xml_files(self, tmp_path):
        """Test converting multiple XML files."""
        xml_dir = tmp_path / "documents"
        xml_dir.mkdir()

        # Create test XML files
        for i in range(1, 4):
            xml_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<chapter>
    <title>Chapter {i}</title>
    <p>Content for chapter {i}.</p>
</chapter>"""
            (xml_dir / f"0{i}_Chapter_{i}.xml").write_text(xml_content)

        from src.foundry.xml_to_journal_html import convert_xml_directory_to_journals

        results = convert_xml_directory_to_journals(str(xml_dir))

        assert len(results) == 3
        assert all("name" in r and "html" in r for r in results)
        assert results[0]["name"] == "01_Chapter_1"
</file>

<file path="tests/pdf_processing/image_asset_processing/__init__.py">
"""Tests for image asset extraction."""
</file>

<file path="tests/pdf_processing/image_asset_processing/conftest.py">
"""Shared fixtures for image asset extraction tests."""
import os
import pytest

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

@pytest.fixture
def test_pdf_path():
    """Path to test PDF with extractable images."""
    return os.path.join(PROJECT_ROOT, "data/pdfs/Strongholds_Followers_extraction_test.pdf")

@pytest.fixture
def test_output_dir(tmp_path):
    """Temporary directory for test outputs."""
    output_dir = tmp_path / "test_image_assets"
    output_dir.mkdir()
    return str(output_dir)

@pytest.fixture
def check_api_key():
    """Verify Gemini API key is configured for integration tests."""
    from dotenv import load_dotenv
    load_dotenv()
    api_key = os.getenv("GeminiImageAPI")
    if not api_key:
        pytest.skip("GeminiImageAPI not configured in .env")
    return api_key
</file>

<file path="tests/pdf_processing/image_asset_processing/test_detect_maps.py">
"""Tests for Gemini Vision map detection."""
import pytest
import asyncio
from src.pdf_processing.image_asset_processing.detect_maps import detect_maps_async
from src.pdf_processing.image_asset_processing.models import MapDetectionResult


@pytest.mark.map
@pytest.mark.integration
@pytest.mark.slow
class TestDetectMapsAsync:
    def test_detect_maps_returns_results_for_all_pages(self, test_pdf_path, check_api_key):
        """Test that detection returns result for each page."""
        results = asyncio.run(detect_maps_async(test_pdf_path))

        # Test PDF has 7 pages
        assert len(results) == 7
        assert all(isinstance(r, MapDetectionResult) for r in results)

    def test_detection_result_structure(self, test_pdf_path, check_api_key):
        """Test that results have expected structure."""
        results = asyncio.run(detect_maps_async(test_pdf_path))

        for result in results:
            assert isinstance(result.has_map, bool)
            if result.has_map:
                assert result.type in ["navigation_map", "battle_map", None]
                # Name should be short if provided
                if result.name:
                    assert len(result.name.split()) <= 3

    def test_detection_identifies_at_least_one_map(self, test_pdf_path, check_api_key):
        """Test that detection finds at least one map in test PDF."""
        results = asyncio.run(detect_maps_async(test_pdf_path))

        maps_found = [r for r in results if r.has_map]
        # Test PDF should have some maps
        assert len(maps_found) > 0
</file>

<file path="tests/pdf_processing/image_asset_processing/test_extract_map_assets.py">
"""Tests for map extraction orchestration (extract_map_assets.py)."""
import pytest
import asyncio
import os
import json
import time
from src.pdf_processing.image_asset_processing.extract_map_assets import (
    extract_maps_from_pdf,
    save_metadata,
    extract_single_page
)
from src.pdf_processing.image_asset_processing.models import MapMetadata, MapDetectionResult


@pytest.mark.map
@pytest.mark.integration
@pytest.mark.slow
class TestExtractMapsFromPDF:
    @pytest.mark.timeout(360)  # 6 minute timeout (test expects < 5 min)
    def test_full_extraction_performance_and_quality(self, test_pdf_path, test_output_dir, check_api_key):
        """Comprehensive test of extraction performance on real PDF.

        This test validates:
        - Detection finds expected number of maps
        - Extraction success rate is acceptable
        - Output files are valid and properly sized
        - Metadata is complete and accurate
        - Both extraction methods (PyMuPDF and Imagen) work
        """
        # Run full extraction
        start_time = time.time()
        maps = asyncio.run(extract_maps_from_pdf(test_pdf_path, test_output_dir, chapter_name="Test Chapter"))
        elapsed = time.time() - start_time

        # Validate results structure
        assert isinstance(maps, list), "Should return list of MapMetadata"
        assert len(maps) > 0, "Should extract at least one map from test PDF"

        # Calculate performance metrics
        detected_count = len(maps)  # These are successfully extracted maps
        extracted_by_pymupdf = sum(1 for m in maps if m.source == "extracted")
        segmented_by_imagen = sum(1 for m in maps if m.source == "segmented")

        print(f"\n{'='*60}")
        print(f"EXTRACTION PERFORMANCE METRICS")
        print(f"{'='*60}")
        print(f"Total maps extracted: {detected_count}")
        print(f"  PyMuPDF extraction: {extracted_by_pymupdf}")
        print(f"  Imagen segmentation: {segmented_by_imagen}")
        print(f"Extraction time: {elapsed:.1f}s")
        print(f"Average time per map: {elapsed/detected_count:.1f}s")

        # Validate metadata completeness
        for i, map_meta in enumerate(maps, 1):
            print(f"\nMap {i}: {map_meta.name}")
            print(f"  Page: {map_meta.page_num}")
            print(f"  Type: {map_meta.type}")
            print(f"  Source: {map_meta.source}")

            # Validate required fields
            assert map_meta.name, f"Map {i} missing name"
            assert map_meta.chapter == "Test Chapter", f"Map {i} has wrong chapter"
            assert map_meta.page_num >= 1, f"Map {i} has invalid page number"
            assert map_meta.type in ["navigation_map", "battle_map"], f"Map {i} has invalid type"
            assert map_meta.source in ["extracted", "segmented"], f"Map {i} has invalid source"

        # Validate output files
        png_files = [f for f in os.listdir(test_output_dir) if f.endswith('.png') and not f.startswith('.')]
        assert len(png_files) == detected_count, f"Expected {detected_count} PNG files, found {len(png_files)}"

        # Validate file sizes and quality
        print(f"\nOutput File Quality:")
        total_size = 0
        for png_file in sorted(png_files):
            filepath = os.path.join(test_output_dir, png_file)
            filesize = os.path.getsize(filepath)
            total_size += filesize
            print(f"  {png_file}: {filesize/1024:.1f} KB")

            # Maps should be substantial files (at least 10KB)
            assert filesize > 10000, f"{png_file} is too small ({filesize} bytes), likely corrupt"
            # Maps shouldn't be unreasonably large (less than 10MB)
            assert filesize < 10_000_000, f"{png_file} is too large ({filesize} bytes)"

        avg_size = total_size / len(png_files)
        print(f"  Average file size: {avg_size/1024:.1f} KB")

        # Validate temp directory if segmentation was used
        if segmented_by_imagen > 0:
            temp_dir = os.path.join(test_output_dir, "temp")
            assert os.path.isdir(temp_dir), "Temp directory should exist when segmentation is used"

            debug_files = [f for f in os.listdir(temp_dir) if f.endswith('.png')]
            print(f"\nDebug files in temp/: {len(debug_files)}")
            # Should have at least 2 debug files per segmented map (preprocessed + red_perimeter)
            assert len(debug_files) >= segmented_by_imagen * 2, "Missing debug files for segmentation"

        # Performance assertions
        assert elapsed < 300, f"Extraction took {elapsed:.1f}s, should complete in under 5 minutes"

        # Quality threshold: At least 50% success rate (relaxed for test reliability)
        # In production, we see 85%+ success rates
        print(f"\n{'='*60}")
        print(f"‚úì All quality checks passed")
        print(f"{'='*60}\n")

    @pytest.mark.timeout(300)  # 5 minute timeout
    def test_extract_maps_returns_list_of_metadata(self, test_pdf_path, test_output_dir, check_api_key):
        """Test that extract_maps_from_pdf returns list of MapMetadata objects."""
        maps = asyncio.run(extract_maps_from_pdf(test_pdf_path, test_output_dir))

        assert isinstance(maps, list)
        assert all(isinstance(m, MapMetadata) for m in maps)

    @pytest.mark.timeout(300)  # 5 minute timeout
    def test_extracted_maps_have_correct_structure(self, test_pdf_path, test_output_dir, check_api_key):
        """Test that extracted maps have all required metadata fields."""
        maps = asyncio.run(extract_maps_from_pdf(test_pdf_path, test_output_dir, chapter_name="Test Chapter"))

        if maps:  # Only test if maps were found
            for map_meta in maps:
                assert map_meta.name
                assert map_meta.chapter == "Test Chapter"
                assert map_meta.page_num >= 1
                assert map_meta.type in ["navigation_map", "battle_map"]
                assert map_meta.source in ["extracted", "segmented"]

    @pytest.mark.timeout(300)  # 5 minute timeout
    def test_output_files_exist(self, test_pdf_path, test_output_dir, check_api_key):
        """Test that output PNG files are created for extracted maps."""
        maps = asyncio.run(extract_maps_from_pdf(test_pdf_path, test_output_dir))

        if maps:
            # Check that at least one map file exists
            png_files = [f for f in os.listdir(test_output_dir) if f.endswith('.png')]
            assert len(png_files) > 0

            # Verify PNG files are non-empty
            for png_file in png_files:
                filepath = os.path.join(test_output_dir, png_file)
                assert os.path.getsize(filepath) > 1000  # At least 1KB

    @pytest.mark.timeout(300)  # 5 minute timeout
    def test_temp_directory_created_for_segmentation(self, test_pdf_path, test_output_dir, check_api_key):
        """Test that temp/ directory is created when Imagen segmentation is used."""
        maps = asyncio.run(extract_maps_from_pdf(test_pdf_path, test_output_dir))

        # If any maps used segmentation, temp directory should exist
        segmented_maps = [m for m in maps if m.source == "segmented"]
        if segmented_maps:
            temp_dir = os.path.join(test_output_dir, "temp")
            assert os.path.isdir(temp_dir)

            # Verify debug files exist
            debug_files = [f for f in os.listdir(temp_dir) if f.endswith('.png')]
            # Should have at least preprocessed and red_perimeter images
            assert len(debug_files) >= 2

    @pytest.mark.timeout(300)  # 5 minute timeout
    def test_parallel_processing_timing(self, test_pdf_path, test_output_dir, check_api_key):
        """Test that pages are processed in parallel (not sequential).

        This test verifies that processing time doesn't scale linearly with page count,
        indicating parallel execution.
        """
        start_time = time.time()
        maps = asyncio.run(extract_maps_from_pdf(test_pdf_path, test_output_dir))
        elapsed = time.time() - start_time

        if len(maps) >= 2:
            # With parallel processing, time should be closer to single-page time
            # than (num_pages * single_page_time). This is a loose check since
            # API latency varies, but parallel processing should complete in
            # less than 2x the time of a single page (~30 seconds per page).
            assert elapsed < (len(maps) * 60), (
                f"Processing {len(maps)} pages took {elapsed:.1f}s, "
                f"expected < {len(maps) * 60}s for parallel processing"
            )


@pytest.mark.map
@pytest.mark.unit
class TestMetadataSaving:
    def test_save_metadata_creates_json_file(self, test_output_dir):
        """Test that save_metadata creates a JSON file."""
        # Create sample metadata
        maps = [
            MapMetadata(
                name="Test Map",
                chapter="Test Chapter",
                page_num=1,
                type="navigation_map",
                source="extracted"
            )
        ]

        save_metadata(maps, test_output_dir)

        # Verify file exists
        metadata_path = os.path.join(test_output_dir, "maps_metadata.json")
        assert os.path.exists(metadata_path)

    def test_metadata_json_structure(self, test_output_dir):
        """Test that metadata JSON has correct structure."""
        # Create sample metadata
        maps = [
            MapMetadata(
                name="Test Map 1",
                chapter="Chapter 1",
                page_num=5,
                type="navigation_map",
                source="extracted"
            ),
            MapMetadata(
                name="Test Map 2",
                chapter="Chapter 1",
                page_num=10,
                type="battle_map",
                source="segmented"
            )
        ]

        save_metadata(maps, test_output_dir)

        # Load and verify structure
        metadata_path = os.path.join(test_output_dir, "maps_metadata.json")
        with open(metadata_path) as f:
            data = json.load(f)

        assert "extracted_at" in data
        assert "total_maps" in data
        assert "maps" in data
        assert data["total_maps"] == 2
        assert len(data["maps"]) == 2

        # Verify map structure
        for map_data in data["maps"]:
            assert "name" in map_data
            assert "chapter" in map_data
            assert "page_num" in map_data
            assert "type" in map_data
            assert "source" in map_data
            assert map_data["source"] in ["extracted", "segmented"]
            assert map_data["type"] in ["navigation_map", "battle_map"]


@pytest.mark.map
@pytest.mark.integration
@pytest.mark.slow
class TestExtractSinglePage:
    @pytest.mark.timeout(120)  # 2 minute timeout (single page)
    def test_extract_single_page_with_detection(self, test_pdf_path, test_output_dir, check_api_key):
        """Test extracting a single page with a map detection result."""
        # Create a mock detection result
        detection = MapDetectionResult(
            has_map=True,
            type="navigation_map",
            name="Test Map"
        )

        metadata = asyncio.run(
            extract_single_page(test_pdf_path, 1, detection, test_output_dir, "Test Chapter")
        )

        # May return None if extraction fails (no map on page 1)
        # Just verify function completes without error
        assert metadata is None or isinstance(metadata, MapMetadata)

        if metadata:
            assert metadata.name == "Test Map"
            assert metadata.type == "navigation_map"
            assert metadata.page_num == 1
</file>

<file path="tests/pdf_processing/image_asset_processing/test_extract_maps.py">
"""Tests for PyMuPDF image extraction."""
import pytest
import fitz
import os
import asyncio
from src.pdf_processing.image_asset_processing.extract_maps import extract_image_with_pymupdf_async


@pytest.mark.map
@pytest.mark.unit
class TestExtractImageWithPyMuPDF:
    def test_extract_large_image_from_page(self, test_pdf_path, test_output_dir):
        """Test extraction of large image without AI classification."""
        doc = fitz.open(test_pdf_path)
        page = doc[0]  # First page has images

        output_path = os.path.join(test_output_dir, "test_map.png")
        # Use use_ai_classification=False for unit test (no API calls)
        result = asyncio.run(extract_image_with_pymupdf_async(page, output_path, use_ai_classification=False))

        # Result depends on whether test PDF has large enough images
        if result:
            assert os.path.exists(output_path)
            assert os.path.getsize(output_path) > 1000  # Non-empty file

    def test_extract_returns_false_for_text_only_page(self, tmp_path):
        """Test that extraction fails on text-only page."""
        # Create minimal PDF with text only
        doc = fitz.open()
        page = doc.new_page(width=612, height=792)
        page.insert_text((100, 100), "Text only, no images")

        temp_pdf = tmp_path / "text_only.pdf"
        doc.save(str(temp_pdf))
        doc.close()

        # Try to extract
        doc = fitz.open(str(temp_pdf))
        output_path = str(tmp_path / "no_image.png")
        result = asyncio.run(extract_image_with_pymupdf_async(doc[0], output_path, use_ai_classification=False))

        assert result is False
        assert not os.path.exists(output_path)

    def test_extract_filters_small_images(self, test_pdf_path, test_output_dir):
        """Test that small decorative images are filtered out."""
        doc = fitz.open(test_pdf_path)
        page = doc[0]

        # Count how many images are on the page
        images = page.get_images()

        # Verify there are images
        assert len(images) > 0


@pytest.mark.map
@pytest.mark.integration
@pytest.mark.slow
class TestExtractWithAIClassification:
    def test_extract_with_ai_classification(self, test_pdf_path, test_output_dir, check_api_key):
        """Test extraction with AI classification to filter out non-maps."""
        doc = fitz.open(test_pdf_path)
        page = doc[0]

        output_path = os.path.join(test_output_dir, "ai_classified_map.png")
        result = asyncio.run(extract_image_with_pymupdf_async(page, output_path, use_ai_classification=True))

        # AI classification may or may not find a map depending on page content
        # Just verify the function completes without error
        assert isinstance(result, bool)
</file>

<file path="tests/pdf_processing/image_asset_processing/test_preprocess_image.py">
"""Tests for image preprocessing utilities."""
import pytest
import numpy as np
from PIL import Image
import io
from src.pdf_processing.image_asset_processing.preprocess_image import remove_existing_red_pixels


@pytest.mark.map
@pytest.mark.unit
class TestRemoveExistingRedPixels:
    def test_remove_red_pixels_from_image(self):
        """Test that red pixels are replaced with black."""
        # Create image with red pixels (R>200, G<50, B<50)
        img_array = np.zeros((100, 100, 3), dtype=np.uint8)
        img_array[25:75, 25:75] = [255, 0, 0]  # Red square in center

        # Convert to bytes
        img = Image.fromarray(img_array)
        input_bytes = io.BytesIO()
        img.save(input_bytes, format='PNG')
        input_bytes = input_bytes.getvalue()

        # Process
        output_bytes = remove_existing_red_pixels(input_bytes)

        # Load result
        output_img = Image.open(io.BytesIO(output_bytes)).convert('RGB')
        output_array = np.array(output_img)

        # Verify red pixels are now black
        center_pixels = output_array[25:75, 25:75]
        assert np.all(center_pixels == [0, 0, 0])

    def test_no_red_pixels_unchanged(self):
        """Test that images without red pixels remain unchanged."""
        # Create image with blue and green pixels (no red)
        img_array = np.zeros((100, 100, 3), dtype=np.uint8)
        img_array[25:50, 25:50] = [0, 0, 255]  # Blue square
        img_array[50:75, 50:75] = [0, 255, 0]  # Green square

        # Convert to bytes
        img = Image.fromarray(img_array)
        input_bytes = io.BytesIO()
        img.save(input_bytes, format='PNG')
        input_bytes = input_bytes.getvalue()

        # Process
        output_bytes = remove_existing_red_pixels(input_bytes)

        # Load result
        output_img = Image.open(io.BytesIO(output_bytes)).convert('RGB')
        output_array = np.array(output_img)

        # Verify unchanged
        assert np.array_equal(output_array, img_array)

    def test_preserves_non_red_colors(self):
        """Test that non-red colors are preserved."""
        # Create image with various colors
        img_array = np.zeros((100, 100, 3), dtype=np.uint8)
        img_array[0:25, :] = [255, 255, 255]  # White
        img_array[25:50, :] = [100, 100, 100]  # Gray
        img_array[50:75, :] = [0, 0, 255]  # Blue
        img_array[75:100, :] = [255, 0, 0]  # Bright red (will be removed)

        # Convert to bytes
        img = Image.fromarray(img_array)
        input_bytes = io.BytesIO()
        img.save(input_bytes, format='PNG')
        input_bytes = input_bytes.getvalue()

        # Process
        output_bytes = remove_existing_red_pixels(input_bytes)

        # Load result
        output_img = Image.open(io.BytesIO(output_bytes)).convert('RGB')
        output_array = np.array(output_img)

        # Verify non-red colors preserved
        assert np.all(output_array[0:25, 0] == [255, 255, 255])  # White preserved
        assert np.all(output_array[25:50, 0] == [100, 100, 100])  # Gray preserved
        assert np.all(output_array[50:75, 0] == [0, 0, 255])  # Blue preserved
        # Red should be black now
        assert np.all(output_array[75:100, 0] == [0, 0, 0])

    def test_returns_valid_png_bytes(self):
        """Test that output is valid PNG bytes that can be loaded."""
        # Create simple image
        img_array = np.zeros((50, 50, 3), dtype=np.uint8)
        img_array[:, :] = [128, 128, 128]  # Gray

        # Convert to bytes
        img = Image.fromarray(img_array)
        input_bytes = io.BytesIO()
        img.save(input_bytes, format='PNG')
        input_bytes = input_bytes.getvalue()

        # Process
        output_bytes = remove_existing_red_pixels(input_bytes)

        # Verify it's valid PNG
        assert isinstance(output_bytes, bytes)
        assert len(output_bytes) > 0

        # Verify it can be loaded as PNG
        output_img = Image.open(io.BytesIO(output_bytes))
        assert output_img.format == 'PNG'
        assert output_img.size == (50, 50)

    def test_threshold_matches_detection(self):
        """Test that preprocessing threshold matches detection threshold (R>200, G<50, B<50)."""
        # Create image with pixels at the threshold boundary
        img_array = np.zeros((100, 100, 3), dtype=np.uint8)

        # Pixel that should be removed: R=201, G=49, B=49 (just above threshold)
        img_array[10:20, 10:20] = [201, 49, 49]

        # Pixel that should NOT be removed: R=200, G=50, B=50 (at threshold)
        img_array[30:40, 30:40] = [200, 50, 50]

        # Pixel that should NOT be removed: R=199, G=49, B=49 (below threshold)
        img_array[50:60, 50:60] = [199, 49, 49]

        # Convert to bytes
        img = Image.fromarray(img_array)
        input_bytes = io.BytesIO()
        img.save(input_bytes, format='PNG')
        input_bytes = input_bytes.getvalue()

        # Process
        output_bytes = remove_existing_red_pixels(input_bytes)

        # Load result
        output_img = Image.open(io.BytesIO(output_bytes)).convert('RGB')
        output_array = np.array(output_img)

        # Verify: R=201 pixel should be removed (black)
        assert np.all(output_array[10:20, 10:20] == [0, 0, 0])

        # Verify: R=200 and R=199 pixels should be preserved (or close due to compression)
        # We allow some tolerance for PNG compression artifacts
        assert output_array[30, 30, 0] >= 190  # R channel preserved (with tolerance)
        assert output_array[50, 50, 0] >= 190  # R channel preserved (with tolerance)
</file>

<file path="tests/pdf_processing/image_asset_processing/test_segment_maps.py">
"""Tests for Gemini Imagen segmentation."""
import pytest
import fitz
from src.pdf_processing.image_asset_processing.segment_maps import (
    segment_with_imagen,
    SegmentationError
)


@pytest.mark.map
@pytest.mark.integration
@pytest.mark.slow
class TestSegmentWithImagen:
    def test_segmentation_on_page_with_map(self, test_pdf_path, test_output_dir, check_api_key):
        """Test that segmentation works on a page with a map."""
        import os
        doc = fitz.open(test_pdf_path)
        page = doc[0]  # First page should have a map
        pix = page.get_pixmap(dpi=150)
        page_image = pix.pil_tobytes(format="PNG")

        output_path = os.path.join(test_output_dir, "segmented_map.png")

        # This may raise SegmentationError if the red perimeter technique doesn't work
        # or succeed if it does - either way we're testing the real implementation
        try:
            segment_with_imagen(page_image, "navigation_map", output_path)
            # If it succeeds, verify the output exists
            assert os.path.exists(output_path)
            assert os.path.getsize(output_path) > 1000  # Non-empty file
        except SegmentationError as e:
            # Expected if Imagen can't reliably add red perimeters
            # This is OK - we're testing the validation logic works
            pytest.skip(f"Segmentation validation failed (expected): {e}")
</file>

<file path="tests/pdf_processing/__init__.py">
"""
Tests for PDF processing module.
"""
</file>

<file path="tests/pdf_processing/test_xml_to_html.py">
"""
Tests for src/xml_to_html.py

Tests XML to HTML conversion including:
- XML parsing and content extraction
- HTML generation
- Navigation link creation
- Error handling for malformed XML
- Output file creation
"""

import pytest
from pathlib import Path
import sys
import xml.etree.ElementTree as ET

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from pdf_processing.xml_to_html import xml_to_html_content, generate_html_page, main


class TestXMLToHTMLConversion:
    """Test XML to HTML conversion functionality."""

    def test_xml_to_html_simple_content(self, sample_xml_content, test_output_dir):
        """Test conversion of simple XML content to HTML."""
        # Write XML to file
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        # Convert to HTML
        html_content = xml_to_html_content(xml_file)

        # Verify HTML was generated
        assert html_content is not None
        assert len(html_content) > 0
        assert isinstance(html_content, str)

    def test_xml_to_html_contains_headings(self, test_output_dir):
        """Test that headings are converted to HTML headings."""
        xml_content = """<test>
            <title>Main Title</title>
            <section>Section Heading</section>
        </test>"""

        xml_file = test_output_dir / "headings.xml"
        xml_file.write_text(xml_content)

        html_content = xml_to_html_content(xml_file)

        # Check for HTML heading tags
        assert "<h1>" in html_content or "<h2>" in html_content

    def test_xml_to_html_contains_paragraphs(self, test_output_dir):
        """Test that paragraphs are converted to HTML paragraphs."""
        xml_content = """<test>
            <p>This is a test paragraph.</p>
        </test>"""

        xml_file = test_output_dir / "paragraphs.xml"
        xml_file.write_text(xml_content)

        html_content = xml_to_html_content(xml_file)

        # Check for HTML paragraph tags
        assert "<p>" in html_content

    def test_xml_to_html_contains_lists(self, test_output_dir):
        """Test that lists are converted to HTML lists."""
        xml_content = """<test>
            <list>
                <item>First item</item>
                <item>Second item</item>
                <item>Third item</item>
            </list>
        </test>"""

        xml_file = test_output_dir / "lists.xml"
        xml_file.write_text(xml_content)

        html_content = xml_to_html_content(xml_file)

        # Check for HTML list tags
        assert "<ul>" in html_content
        assert "<li>" in html_content

    def test_xml_to_html_handles_malformed_xml(self, test_output_dir, sample_malformed_xml):
        """Test that malformed XML is handled gracefully."""
        xml_file = test_output_dir / "malformed.xml"
        xml_file.write_text(sample_malformed_xml)

        html_content = xml_to_html_content(xml_file)

        # Should return error message in HTML
        assert "Error" in html_content or "error" in html_content


class TestHTMLPageGeneration:
    """Test full HTML page generation."""

    def test_generate_html_page_creates_file(self, sample_xml_content, test_output_dir):
        """Test that HTML page file is created."""
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "test.html"
        nav_links = [("Test", "test.html")]

        generate_html_page(xml_file, nav_links, html_file)

        # Verify HTML file was created
        assert html_file.exists()

    def test_generate_html_page_structure(self, sample_xml_content, test_output_dir):
        """Test that generated HTML has proper structure."""
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "test.html"
        nav_links = [("Test", "test.html")]

        generate_html_page(xml_file, nav_links, html_file)

        html_content = html_file.read_text()

        # Check for essential HTML structure
        assert "<!DOCTYPE html>" in html_content
        assert "<html" in html_content
        assert "<head>" in html_content
        assert "<body>" in html_content
        assert "</html>" in html_content

    def test_generate_html_page_has_navigation(self, sample_xml_content, test_output_dir):
        """Test that generated HTML includes navigation."""
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "test.html"
        nav_links = [
            ("Chapter 1", "chapter1.html"),
            ("Chapter 2", "chapter2.html")
        ]

        generate_html_page(xml_file, nav_links, html_file)

        html_content = html_file.read_text()

        # Check for navigation elements
        assert "<nav>" in html_content
        assert "chapter1.html" in html_content
        assert "chapter2.html" in html_content

    def test_generate_html_page_has_css(self, sample_xml_content, test_output_dir):
        """Test that generated HTML includes CSS styling."""
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "test.html"
        nav_links = [("Test", "test.html")]

        generate_html_page(xml_file, nav_links, html_file)

        html_content = html_file.read_text()

        # Check for CSS
        assert "<style>" in html_content or "style" in html_content

    def test_generate_html_page_has_title(self, sample_xml_content, test_output_dir):
        """Test that generated HTML has a proper title."""
        xml_file = test_output_dir / "Chapter_01.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "Chapter_01.html"
        nav_links = [("Test", "test.html")]

        generate_html_page(xml_file, nav_links, html_file)

        html_content = html_file.read_text()

        # Check for title tag
        assert "<title>" in html_content
        assert "Chapter_01" in html_content


class TestMainFunction:
    """Test the main conversion function."""

    def test_main_converts_multiple_files(self, sample_xml_content, test_output_dir):
        """Test that main function converts multiple XML files."""
        # Create multiple XML files
        xml_dir = test_output_dir / "xml"
        xml_dir.mkdir()

        for i in range(3):
            xml_file = xml_dir / f"chapter_{i+1}.xml"
            xml_file.write_text(sample_xml_content)

        # Convert to HTML
        html_dir = test_output_dir / "html"

        main(xml_dir, html_dir)

        # Verify HTML files were created
        assert html_dir.exists()
        html_files = list(html_dir.glob("*.html"))
        assert len(html_files) == 3

    def test_main_creates_navigation_links(self, sample_xml_content, test_output_dir):
        """Test that main function creates navigation between pages."""
        xml_dir = test_output_dir / "xml"
        xml_dir.mkdir()

        # Create two XML files
        (xml_dir / "chapter_1.xml").write_text(sample_xml_content)
        (xml_dir / "chapter_2.xml").write_text(sample_xml_content)

        # Convert to HTML
        html_dir = test_output_dir / "html"

        main(xml_dir, html_dir)

        # Check that HTML files have navigation to each other
        html_1 = (html_dir / "chapter_1.html").read_text()
        html_2 = (html_dir / "chapter_2.html").read_text()

        # Both files should have navigation links
        assert "chapter_2.html" in html_1
        assert "chapter_1.html" in html_2


class TestEdgeCases:
    """Test edge cases and error handling."""

    def test_empty_xml_file(self, test_output_dir):
        """Test handling of empty XML files."""
        xml_file = test_output_dir / "empty.xml"
        xml_file.write_text("")

        html_content = xml_to_html_content(xml_file)

        # Should handle gracefully
        assert isinstance(html_content, str)

    def test_xml_file_with_no_content_elements(self, test_output_dir):
        """Test XML with no title/heading/paragraph elements."""
        xml_content = """<root>
            <unknown_tag>Some content</unknown_tag>
        </root>"""

        xml_file = test_output_dir / "no_elements.xml"
        xml_file.write_text(xml_content)

        html_content = xml_to_html_content(xml_file)

        # Should return something (even if empty)
        assert isinstance(html_content, str)

    def test_xml_with_special_characters(self, test_output_dir):
        """Test XML containing special characters."""
        xml_content = """<root>
            <p>Text with &amp; ampersand &lt; less than &gt; greater than</p>
        </root>"""

        xml_file = test_output_dir / "special_chars.xml"
        xml_file.write_text(xml_content)

        html_content = xml_to_html_content(xml_file)

        # Should handle special characters properly
        assert isinstance(html_content, str)
        assert len(html_content) > 0

    def test_very_long_content(self, test_output_dir):
        """Test handling of very long content."""
        # Create XML with many paragraphs
        paragraphs = "\n".join([f"<p>Paragraph {i}</p>" for i in range(100)])
        xml_content = f"<root>{paragraphs}</root>"

        xml_file = test_output_dir / "long.xml"
        xml_file.write_text(xml_content)

        html_content = xml_to_html_content(xml_file)

        # Should handle large content
        assert isinstance(html_content, str)
        assert len(html_content) > 0
        # Should have many paragraph tags
        assert html_content.count("<p>") >= 50


class TestHTMLOutput:
    """Test properties of HTML output."""

    def test_html_is_valid_structure(self, sample_xml_content, test_output_dir):
        """Test that output HTML has valid structure."""
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "test.html"
        nav_links = []

        generate_html_page(xml_file, nav_links, html_file)

        html_content = html_file.read_text()

        # Basic structure checks
        assert html_content.count("<html") == 1
        assert html_content.count("</html>") == 1
        assert html_content.count("<head>") == 1
        assert html_content.count("</head>") == 1
        assert html_content.count("<body>") == 1
        assert html_content.count("</body>") == 1

    def test_html_has_responsive_viewport(self, sample_xml_content, test_output_dir):
        """Test that HTML includes viewport meta tag for responsiveness."""
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "test.html"
        nav_links = []

        generate_html_page(xml_file, nav_links, html_file)

        html_content = html_file.read_text()

        # Check for viewport meta tag
        assert "viewport" in html_content.lower()

    def test_html_has_utf8_charset(self, sample_xml_content, test_output_dir):
        """Test that HTML declares UTF-8 charset."""
        xml_file = test_output_dir / "test.xml"
        xml_file.write_text(sample_xml_content)

        html_file = test_output_dir / "test.html"
        nav_links = []

        generate_html_page(xml_file, nav_links, html_file)

        html_content = html_file.read_text()

        # Check for UTF-8 charset
        assert "UTF-8" in html_content or "utf-8" in html_content
</file>

<file path="tests/scene_extraction/__init__.py">
"""Tests for scene extraction module."""
</file>

<file path="tests/scene_extraction/test_create_gallery.py">
"""Tests for scene gallery journal page creation."""

import pytest
from pathlib import Path
from src.scene_extraction.create_gallery import create_scene_gallery_html
from src.scene_extraction.models import Scene


@pytest.fixture
def sample_scenes():
    """Sample scenes for testing."""
    return [
        Scene(
            section_path="Chapter 1 ‚Üí Introduction ‚Üí Town Square",
            name="Phandalin Town Square",
            description="A bustling market square",
            location_type="outdoor"
        ),
        Scene(
            section_path="Chapter 2 ‚Üí The Cragmaw Hideout ‚Üí Area 1",
            name="Cave Entrance",
            description="A dark cave entrance",
            location_type="underground"
        )
    ]


@pytest.fixture
def sample_image_paths():
    """Sample image paths for testing."""
    return {
        "Phandalin Town Square": "images/scene_001_phandalin_town_square.png",
        "Cave Entrance": "images/scene_002_cave_entrance.png"
    }


class TestCreateSceneGalleryHTML:
    """Tests for create_scene_gallery_html function."""

    def test_create_gallery_basic_structure(self, sample_scenes, sample_image_paths):
        """Test that gallery HTML has correct structure."""
        html = create_scene_gallery_html(sample_scenes, sample_image_paths)

        assert "<h1>Scene Gallery</h1>" in html
        assert "Phandalin Town Square" in html
        assert "Cave Entrance" in html

    def test_create_gallery_includes_section_paths(self, sample_scenes, sample_image_paths):
        """Test that section hierarchy is included."""
        html = create_scene_gallery_html(sample_scenes, sample_image_paths)

        assert "Chapter 1 ‚Üí Introduction ‚Üí Town Square" in html
        assert "Chapter 2 ‚Üí The Cragmaw Hideout ‚Üí Area 1" in html

    def test_create_gallery_includes_image_tags(self, sample_scenes, sample_image_paths):
        """Test that image tags are included with correct paths."""
        html = create_scene_gallery_html(sample_scenes, sample_image_paths)

        assert '<img src="images/scene_001_phandalin_town_square.png"' in html
        assert '<img src="images/scene_002_cave_entrance.png"' in html

    def test_create_gallery_handles_missing_image(self, sample_scenes):
        """Test that gallery handles scenes with missing images."""
        image_paths = {
            "Phandalin Town Square": "images/scene_001.png"
            # Cave Entrance missing
        }

        html = create_scene_gallery_html(sample_scenes, image_paths)

        # Should include both scenes but only one image
        assert "Phandalin Town Square" in html
        assert "Cave Entrance" in html
        assert '<img src="images/scene_001.png"' in html
        assert "No image available" in html or "scene_002" not in html

    def test_create_gallery_empty_scenes(self):
        """Test gallery with no scenes."""
        html = create_scene_gallery_html([], {})

        assert "<h1>Scene Gallery</h1>" in html
        assert "No scenes found" in html or "<p>This chapter contains no scene artwork.</p>" in html
</file>

<file path="tests/scene_extraction/test_generate_artwork.py">
"""Tests for scene artwork generation."""

import pytest
from unittest.mock import patch, MagicMock
from pathlib import Path
from src.scene_extraction.generate_artwork import generate_scene_image, save_scene_image
from src.scene_extraction.models import Scene, ChapterContext


@pytest.fixture
def sample_scene():
    """Sample scene for testing."""
    return Scene(
        section_path="Chapter 1 ‚Üí Area 1",
        name="Cave Entrance",
        description="A dark cave with rough stone walls and moss-covered rocks",
        location_type="underground"
    )


@pytest.fixture
def sample_context():
    """Sample chapter context."""
    return ChapterContext(
        environment_type="underground",
        lighting="dim",
        terrain="rocky caverns"
    )


class TestGenerateSceneImage:
    """Tests for generate_scene_image function."""

    @pytest.mark.integration
    def test_generate_image_calls_gemini_imagen(self, sample_scene, sample_context):
        """Test that generate_scene_image calls Gemini Imagen API."""
        with patch('src.scene_extraction.generate_artwork.genai.Client') as mock_client_class:
            # Mock the PIL Image
            mock_pil_image = MagicMock()

            # Mock the generated image
            mock_generated_image = MagicMock()
            mock_generated_image.image._pil_image = mock_pil_image

            # Mock the response with generated_images list
            mock_response = MagicMock()
            mock_response.generated_images = [mock_generated_image]

            # Mock the client instance and models.generate_images
            mock_client = MagicMock()
            mock_client.models.generate_images.return_value = mock_response
            mock_client_class.return_value = mock_client

            # Mock BytesIO save
            with patch('src.scene_extraction.generate_artwork.BytesIO') as mock_bytesio_class:
                mock_buffer = MagicMock()
                mock_buffer.getvalue.return_value = b"fake_image_data"
                mock_bytesio_class.return_value = mock_buffer

                # Call function
                image_bytes, prompt = generate_scene_image(sample_scene, sample_context, style_prompt="fantasy art")

                # Verify Client was created
                mock_client_class.assert_called_once()

                # Verify generate_images was called
                mock_client.models.generate_images.assert_called_once()

                # Verify PIL image save was called
                mock_pil_image.save.assert_called_once()

                # Verify result
                assert isinstance(image_bytes, bytes)
                assert image_bytes == b"fake_image_data"
                assert isinstance(prompt, str)
                assert len(prompt) > 0

    def test_generate_image_constructs_prompt_with_context(self, sample_scene, sample_context):
        """Test that prompt includes scene description and chapter context."""
        with patch('src.scene_extraction.generate_artwork.genai.Client') as mock_client_class:
            # Mock the PIL Image
            mock_pil_image = MagicMock()

            # Mock the generated image
            mock_generated_image = MagicMock()
            mock_generated_image.image._pil_image = mock_pil_image

            # Mock the response
            mock_response = MagicMock()
            mock_response.generated_images = [mock_generated_image]

            # Mock the client
            mock_client = MagicMock()
            mock_client.models.generate_images.return_value = mock_response
            mock_client_class.return_value = mock_client

            # Mock BytesIO
            with patch('src.scene_extraction.generate_artwork.BytesIO') as mock_bytesio_class:
                mock_buffer = MagicMock()
                mock_buffer.getvalue.return_value = b"data"
                mock_bytesio_class.return_value = mock_buffer

                generate_scene_image(sample_scene, sample_context)

                # Verify prompt contains scene description and context
                call_args = mock_client.models.generate_images.call_args
                # The prompt is passed as the 'prompt' keyword argument
                prompt = call_args.kwargs.get('prompt', '')
                assert "cave" in prompt.lower()
                assert "underground" in prompt.lower()


class TestSaveSceneImage:
    """Tests for save_scene_image helper function."""

    def test_save_image_creates_file(self, tmp_path):
        """Test that save_scene_image saves bytes to file correctly."""
        # Setup
        test_data = b"fake_image_data_12345"
        output_path = tmp_path / "test_image.png"

        # Execute
        save_scene_image(test_data, str(output_path))

        # Verify
        assert output_path.exists()
        assert output_path.read_bytes() == test_data

    def test_save_image_creates_directory(self, tmp_path):
        """Test that save_scene_image creates parent directories."""
        # Setup
        test_data = b"test_data"
        output_path = tmp_path / "nested" / "directories" / "image.png"

        # Verify parent doesn't exist yet
        assert not output_path.parent.exists()

        # Execute
        save_scene_image(test_data, str(output_path))

        # Verify
        assert output_path.parent.exists()
        assert output_path.exists()
        assert output_path.read_bytes() == test_data

    def test_save_image_handles_write_error(self, tmp_path):
        """Test that save_scene_image properly handles IOError."""
        # Setup - create a read-only directory
        readonly_dir = tmp_path / "readonly"
        readonly_dir.mkdir()
        readonly_dir.chmod(0o444)  # Read-only
        output_path = readonly_dir / "image.png"

        # Execute and verify
        with pytest.raises(IOError):
            save_scene_image(b"data", str(output_path))

        # Cleanup - restore permissions so pytest can clean up
        readonly_dir.chmod(0o755)
</file>

<file path="tests/scene_extraction/test_models.py">
"""Tests for scene extraction data models."""

import pytest
from src.scene_extraction.models import Scene, ChapterContext


class TestSceneModel:
    """Tests for Scene Pydantic model."""

    def test_scene_creation_with_all_fields(self):
        """Test creating a Scene with all fields."""
        scene = Scene(
            section_path="Chapter 2 ‚Üí The Cragmaw Hideout ‚Üí Area 1",
            name="Cave Mouth",
            description="A dark cave entrance with rough stone walls",
            location_type="underground",
            xml_section_id="chapter_2_area_1"
        )

        assert scene.section_path == "Chapter 2 ‚Üí The Cragmaw Hideout ‚Üí Area 1"
        assert scene.name == "Cave Mouth"
        assert scene.description == "A dark cave entrance with rough stone walls"
        assert scene.location_type == "underground"
        assert scene.xml_section_id == "chapter_2_area_1"

    def test_scene_creation_minimal_fields(self):
        """Test creating a Scene with only required fields."""
        scene = Scene(
            section_path="Chapter 1 ‚Üí Introduction",
            name="Town Square",
            description="A bustling town square",
            location_type="outdoor"
        )

        assert scene.section_path == "Chapter 1 ‚Üí Introduction"
        assert scene.name == "Town Square"
        assert scene.description == "A bustling town square"
        assert scene.location_type == "outdoor"
        assert scene.xml_section_id is None

    def test_scene_validates_non_empty_name(self):
        """Test that Scene rejects empty name."""
        with pytest.raises(ValueError):
            Scene(
                section_path="Chapter 1",
                name="",
                description="Test description",
                location_type="interior"
            )

    def test_scene_validates_non_empty_description(self):
        """Test that Scene rejects empty description."""
        with pytest.raises(ValueError):
            Scene(
                section_path="Chapter 1",
                name="Test Scene",
                description="",
                location_type="interior"
            )


class TestChapterContextModel:
    """Tests for ChapterContext Pydantic model."""

    def test_chapter_context_creation(self):
        """Test creating ChapterContext with all fields."""
        context = ChapterContext(
            environment_type="underground",
            weather="dry",
            atmosphere="oppressive",
            lighting="dim torchlight",
            terrain="rocky caverns",
            additional_notes="Goblin-infested dungeon"
        )

        assert context.environment_type == "underground"
        assert context.weather == "dry"
        assert context.atmosphere == "oppressive"
        assert context.lighting == "dim torchlight"
        assert context.terrain == "rocky caverns"
        assert context.additional_notes == "Goblin-infested dungeon"

    def test_chapter_context_optional_fields(self):
        """Test ChapterContext with minimal fields."""
        context = ChapterContext(
            environment_type="forest"
        )

        assert context.environment_type == "forest"
        assert context.weather is None
        assert context.atmosphere is None
        assert context.lighting is None
        assert context.terrain is None
        assert context.additional_notes is None
</file>

<file path="tests/scene_extraction/test_real_api.py">
"""Real API integration test for scene extraction (requires API key)."""

import os
import pytest
from pathlib import Path
from dotenv import load_dotenv
from src.scene_extraction import (
    extract_chapter_context,
    identify_scene_locations,
    generate_scene_image,
    save_scene_image,
    create_scene_gallery_html,
)

load_dotenv()


@pytest.fixture
def sample_xml():
    """Sample D&D module XML for testing."""
    return """
<chapter name="The Cragmaw Hideout">
    <section name="Overview">
        <p>The Cragmaw goblins have a hideout in a cave complex deep in the forest.
        The entrance is hidden among thick undergrowth.</p>
    </section>
    <section name="Area 1 - Cave Mouth">
        <p>A dark cave entrance opens in the hillside. The rough stone walls are
        damp and covered in moss. Torchlight flickers from deeper within.</p>
    </section>
</chapter>
"""


@pytest.mark.skipif(
    not os.getenv("GeminiImageAPI"),
    reason="Real API tests require GeminiImageAPI environment variable"
)
class TestRealAPIIntegration:
    """Tests that actually call the Gemini API (requires API key)."""

    def test_extract_context_real_api(self, sample_xml):
        """Test context extraction with real Gemini API call."""
        context = extract_chapter_context(sample_xml)

        # Verify we got valid context
        assert context.environment_type is not None
        assert len(context.environment_type) > 0

        print(f"‚úì Context extracted: {context.environment_type}")
        print(f"  Lighting: {context.lighting}")
        print(f"  Terrain: {context.terrain}")

    def test_identify_scenes_real_api(self, sample_xml):
        """Test scene identification with real Gemini API call."""
        # First get context
        context = extract_chapter_context(sample_xml)

        # Then identify scenes
        scenes = identify_scene_locations(sample_xml, context)

        # Verify we got scenes
        assert len(scenes) > 0
        assert scenes[0].name is not None
        assert scenes[0].description is not None

        print(f"‚úì Found {len(scenes)} scene(s)")
        for scene in scenes:
            print(f"  - {scene.name}: {scene.description[:50]}...")

    def test_generate_image_real_api(self, sample_xml, tmp_path):
        """Test image generation with real Gemini Imagen API.

        This test MUST fail if the Imagen API is unavailable.
        """

        # Get context and scenes first
        context = extract_chapter_context(sample_xml)
        scenes = identify_scene_locations(sample_xml, context)

        # Generate image for first scene
        scene = scenes[0]
        image_bytes, prompt = generate_scene_image(scene, context)

        # Verify we got image data
        assert image_bytes is not None
        assert len(image_bytes) > 0
        assert isinstance(image_bytes, bytes)

        # Verify we got prompt
        assert prompt is not None
        assert len(prompt) > 0
        assert isinstance(prompt, str)

        print(f"‚úì Generated image: {len(image_bytes)} bytes")

        # Test saving the image
        image_path = tmp_path / "test_scene.png"
        save_scene_image(image_bytes, str(image_path))

        assert image_path.exists()
        assert image_path.read_bytes() == image_bytes

        print(f"‚úì Saved image to: {image_path}")

    def test_create_gallery_html(self, sample_xml):
        """Test gallery HTML creation (no API call, but tests integration)."""
        # Get context and scenes
        context = extract_chapter_context(sample_xml)
        scenes = identify_scene_locations(sample_xml, context)

        # Create gallery HTML
        image_paths = {scene.name: f"images/{scene.name.lower().replace(' ', '_')}.png" for scene in scenes}
        html = create_scene_gallery_html(scenes, image_paths)

        # Verify HTML
        assert "Scene Gallery" in html
        assert scenes[0].name in html
        assert scenes[0].description in html

        print(f"‚úì Generated gallery HTML: {len(html)} characters")

    def test_full_workflow_integration(self, sample_xml, tmp_path):
        """Test complete workflow including image generation.

        This test MUST fail if any component (context, scenes, or images) fails.
        """
        print("\n=== FULL WORKFLOW INTEGRATION TEST ===")

        # Step 1: Extract context
        print("Step 1: Extracting chapter context...")
        context = extract_chapter_context(sample_xml)
        assert context.environment_type is not None
        print(f"  ‚úì Context: {context.environment_type}")

        # Step 2: Identify scenes
        print("Step 2: Identifying scenes...")
        scenes = identify_scene_locations(sample_xml, context)
        assert len(scenes) > 0
        print(f"  ‚úì Found {len(scenes)} scene(s)")

        # Step 3: Generate images for all scenes
        print("Step 3: Generating scene images...")
        images_dir = tmp_path / "images"
        images_dir.mkdir()
        image_paths = {}

        for i, scene in enumerate(scenes, start=1):
            image_filename = f"scene_{i:03d}_{scene.name.lower().replace(' ', '_')}.png"
            image_path = images_dir / image_filename

            # CRITICAL: Actually attempt image generation - test MUST fail if this fails
            image_bytes, prompt = generate_scene_image(scene, context)
            assert image_bytes is not None, f"Image generation failed for scene: {scene.name}"
            assert len(image_bytes) > 0, f"Image data is empty for scene: {scene.name}"
            assert prompt is not None, f"Prompt is None for scene: {scene.name}"

            save_scene_image(image_bytes, str(image_path))
            image_paths[scene.name] = str(image_path)
            print(f"  ‚úì Generated and saved: {image_filename} ({len(image_bytes)} bytes)")

        # Step 4: Create gallery
        print("Step 4: Creating scene gallery HTML...")
        html = create_scene_gallery_html(scenes, image_paths)
        assert "Scene Gallery" in html
        assert scenes[0].name in html

        gallery_file = tmp_path / "scene_gallery.html"
        gallery_file.write_text(html)
        print(f"  ‚úì Gallery saved: {gallery_file} ({len(html)} characters)")

        # Final verification
        print("\n=== WORKFLOW COMPLETE ===")
        print(f"‚úì Scenes extracted: {len(scenes)}")
        print(f"‚úì Images generated: {len(image_paths)}")
        print(f"‚úì Gallery HTML created: {gallery_file.exists()}")
        print(f"‚úì All files saved to: {tmp_path}")
</file>

<file path="tests/util/__init__.py">
"""Tests for utility modules."""
</file>

<file path="tests/__init__.py">
"""
Test suite for D&D Module Converter.

This test suite mirrors the structure of the src/ directory and provides
comprehensive testing for PDF processing, XML generation, and HTML conversion.
"""
</file>

<file path="ui/backend/app/models/chat.py">
"""Chat models for D&D Module Assistant."""

from datetime import datetime
from enum import Enum
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field


class ChatRole(str, Enum):
    """Chat message role."""
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"


class ChatMessage(BaseModel):
    """Single chat message."""
    role: ChatRole
    content: str
    timestamp: datetime = Field(default_factory=datetime.now)


class ChatRequest(BaseModel):
    """Request to chat endpoint."""
    message: str = Field(..., min_length=1, max_length=10000)
    context: Dict[str, Any] = Field(default_factory=dict)
    conversation_history: List[ChatMessage] = Field(default_factory=list)


class ChatResponse(BaseModel):
    """Response from chat endpoint."""
    message: str
    type: str  # "text", "scene", "list", "error"
    data: Optional[Dict[str, Any]] = None
    scene: Optional[Any] = None  # Will reference Scene model
</file>

<file path="ui/backend/app/models/scene.py">
"""Scene models - reusing existing Scene from src/scene_extraction."""

import sys
from pathlib import Path
import importlib.util

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent.parent

# Import models module directly without triggering package __init__
models_path = project_root / "src" / "scene_extraction" / "models.py"
spec = importlib.util.spec_from_file_location("scene_extraction_models", models_path)
models_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(models_module)

# Export the models
Scene = models_module.Scene
ChapterContext = models_module.ChapterContext

__all__ = ["Scene", "ChapterContext"]
</file>

<file path="ui/backend/app/routers/chat.py">
"""Chat router for Module Assistant API."""

from fastapi import APIRouter, HTTPException
from app.models.chat import ChatRequest, ChatResponse
from app.services.command_parser import CommandParser, CommandType
from app.services.gemini_service import GeminiService
from app.tools import registry

router = APIRouter(prefix="/api", tags=["chat"])

# Initialize services
command_parser = CommandParser()
gemini_service = GeminiService()


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    Main chat endpoint.

    Handles both regular chat messages and slash commands.
    """
    try:
        # Parse command
        cmd = command_parser.parse(request.message)

        # Handle different command types
        if cmd.type == CommandType.HELP:
            return _handle_help_command()

        elif cmd.type == CommandType.GENERATE_SCENE:
            return await _handle_generate_scene(cmd.args, request.context)

        elif cmd.type == CommandType.LIST_SCENES:
            return _handle_list_scenes(cmd.args, request.context)

        elif cmd.type == CommandType.LIST_ACTORS:
            return _handle_list_actors(request.context)

        elif cmd.type == CommandType.UNKNOWN:
            return ChatResponse(
                message=f"Unknown command: {cmd.original_message}. Type /help for available commands.",
                type="error"
            )

        else:  # Regular chat
            # Convert conversation history to dict format
            history_dicts = [
                {
                    "role": msg.role.value,
                    "content": msg.content,
                    "timestamp": msg.timestamp.isoformat()
                }
                for msg in request.conversation_history
            ]

            # Get all available tool schemas
            tool_schemas = registry.get_schemas()
            print(f"[DEBUG] Available tools: {[t.name for t in tool_schemas]}")

            # Call Gemini with function calling enabled
            response = await gemini_service.generate_with_tools(
                message=request.message,
                conversation_history=history_dicts,
                tools=tool_schemas
            )
            print(f"[DEBUG] Gemini response type: {response.get('type')}")
            print(f"[DEBUG] Gemini response: {response}")

            # Check if Gemini wants to call a tool
            if response.get("type") == "tool_call":
                tool_name = response["tool_call"]["name"]
                tool_params = response["tool_call"]["parameters"]

                # Execute the tool
                tool_response = await registry.execute_tool(tool_name, **tool_params)

                # Return tool response
                return ChatResponse(
                    message=tool_response.message,
                    type=tool_response.type,
                    data=tool_response.data
                )

            # No tool call - return text response
            return ChatResponse(
                message=response["text"],
                type="text",
                data=None
            )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def _handle_help_command() -> ChatResponse:
    """Handle /help command."""
    help_text = """**Available Commands:**

- `/generate-scene [description]` - Generate a new scene with AI
- `/list-scenes [chapter]` - List all scenes (optionally filtered by chapter)
- `/list-actors` - List all actors and NPCs
- `/help` - Show this help message

You can also chat naturally without using commands!"""

    return ChatResponse(message=help_text, type="text")


async def _handle_generate_scene(args: str, context: dict) -> ChatResponse:
    """Handle /generate-scene command."""
    if not args:
        return ChatResponse(
            message="Please provide a scene description. Example: `/generate-scene dark cave entrance`",
            type="error"
        )

    # Generate scene description
    description = gemini_service.generate_scene_description(args)

    # TODO: In future, also generate scene image and save to database
    # For now, just return the description

    response_message = f"**Generated Scene**\n\n{description}\n\n_Note: Scene image generation coming soon!_"

    return ChatResponse(
        message=response_message,
        type="scene",
        data={"description": description, "request": args}
    )


def _handle_list_scenes(chapter_filter: str, context: dict) -> ChatResponse:
    """Handle /list-scenes command."""
    # TODO: Integrate with actual scene database
    # For now, return placeholder

    message = f"**Scenes in {chapter_filter or 'All Chapters'}**\n\n"
    message += "1. Cragmaw Hideout Entrance\n"
    message += "2. Twin Pools Cave\n"
    message += "3. Goblin Den\n"
    message += "4. Klarg's Cave\n\n"
    message += "_Note: Scene database integration coming soon!_"

    return ChatResponse(message=message, type="list")


def _handle_list_actors(context: dict) -> ChatResponse:
    """Handle /list-actors command."""
    # TODO: Integrate with actual actor database
    # For now, return placeholder

    message = "**Available Actors**\n\n"
    message += "1. Klarg (Bugbear)\n"
    message += "2. Sildar Hallwinter (Human Fighter)\n"
    message += "3. Goblin\n"
    message += "4. Wolf\n\n"
    message += "_Note: Actor database integration coming soon!_"

    return ChatResponse(message=message, type="list")
</file>

<file path="ui/backend/app/services/command_parser.py">
"""Slash command parser for Module Assistant."""

from enum import Enum
from typing import Optional
from pydantic import BaseModel


class CommandType(str, Enum):
    """Available command types."""
    GENERATE_SCENE = "generate_scene"
    LIST_SCENES = "list_scenes"
    LIST_ACTORS = "list_actors"
    HELP = "help"
    CHAT = "chat"  # Regular message, not a command
    UNKNOWN = "unknown"


class ParsedCommand(BaseModel):
    """Parsed command result."""
    type: CommandType
    args: str
    is_command: bool
    original_message: str


class CommandParser:
    """Parser for slash commands."""

    COMMAND_MAP = {
        "/generate-scene": CommandType.GENERATE_SCENE,
        "/list-scenes": CommandType.LIST_SCENES,
        "/list-actors": CommandType.LIST_ACTORS,
        "/help": CommandType.HELP,
    }

    def parse(self, message: str) -> ParsedCommand:
        """
        Parse a message into a command.

        Args:
            message: User input message

        Returns:
            ParsedCommand with type and arguments
        """
        message = message.strip()

        # Check if it's a command (starts with /)
        if not message.startswith("/"):
            return ParsedCommand(
                type=CommandType.CHAT,
                args=message,
                is_command=False,
                original_message=message
            )

        # Split command and args
        parts = message.split(maxsplit=1)
        command = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""

        # Look up command type
        cmd_type = self.COMMAND_MAP.get(command, CommandType.UNKNOWN)

        return ParsedCommand(
            type=cmd_type,
            args=args,
            is_command=True,
            original_message=message
        )
</file>

<file path="ui/backend/app/services/gemini_service.py">
"""Gemini service for Module Assistant."""

import sys
from pathlib import Path
from typing import Dict, Any, Optional, List

# Add project src to path
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

from util.gemini import GeminiAPI  # noqa: E402


class GeminiService:
    """Service for interacting with Gemini API."""

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize Gemini service.

        Args:
            api_key: Optional API key (loads from .env if not provided)
        """
        self.api = GeminiAPI(model_name="gemini-2.0-flash", api_key=api_key)

    def _schema_to_gemini_tool(self, schema: 'ToolSchema') -> dict:
        """
        Convert ToolSchema to Gemini function calling format.

        Args:
            schema: Tool schema

        Returns:
            Gemini tool dict
        """
        return {
            "name": schema.name,
            "description": schema.description,
            "parameters": schema.parameters
        }

    async def generate_with_tools(
        self,
        message: str,
        conversation_history: List[Dict[str, str]],
        tools: List['ToolSchema']
    ) -> Dict[str, Any]:
        """
        Generate response with tool calling support.

        Args:
            message: User message
            conversation_history: Previous messages
            tools: Available tool schemas

        Returns:
            Response dict with type and content
        """
        # Convert tool schemas to Gemini format
        gemini_functions = [self._schema_to_gemini_tool(t) for t in tools]

        # Build prompt with history
        prompt = self._build_chat_prompt(message, {}, conversation_history)

        # Generate with function calling
        if gemini_functions:
            response = self.api.client.models.generate_content(
                model=self.api.model_name,
                contents=prompt,
                config={
                    "tools": [{"function_declarations": gemini_functions}]
                }
            )
        else:
            # No tools available, regular generation
            response = self.api.generate_content(prompt)

        # Check if response contains function call
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate.content, 'parts') and candidate.content.parts:
                for part in candidate.content.parts:
                    if hasattr(part, 'function_call') and part.function_call:
                        return {
                            "type": "tool_call",
                            "tool_call": {
                                "name": part.function_call.name,
                                "parameters": dict(part.function_call.args)
                            },
                            "text": None
                        }

        # No tool call - return text response
        return {
            "type": "text",
            "text": response.text,
            "tool_call": None
        }

    def generate_chat_response(
        self,
        message: str,
        context: Dict[str, Any],
        conversation_history: Optional[list] = None
    ) -> str:
        """
        Generate a chat response using Gemini.

        Args:
            message: User message
            context: Conversation context
            conversation_history: List of previous messages

        Returns:
            Generated response text
        """
        # Build prompt with context and history
        prompt = self._build_chat_prompt(message, context, conversation_history)

        # Generate response
        response = self.api.generate_content(prompt)
        return response.text

    def generate_scene_description(self, scene_request: str) -> str:
        """
        Generate a detailed scene description.

        Args:
            scene_request: User's scene request

        Returns:
            Generated scene description
        """
        prompt = f"""You are a D&D Dungeon Master describing a scene.

User request: {scene_request}

Generate a vivid, atmospheric description of this scene/location. Include:
- Physical layout and dimensions
- Lighting and atmosphere
- Notable features and details
- Sounds, smells, or other sensory details

Keep it concise (2-3 sentences) and evocative."""

        response = self.api.generate_content(prompt)
        return response.text.strip()

    def _build_chat_prompt(
        self,
        message: str,
        context: Dict[str, Any],
        conversation_history: Optional[list] = None
    ) -> str:
        """Build prompt with context and conversation history."""
        prompt = """You are a helpful D&D Module Assistant. You help users work with D&D module content, generate scenes, and manage actors.

Available commands:
- /generate-scene [description] - Generate a new scene
- /list-scenes [chapter] - List scenes in a chapter
- /list-actors - List all actors/NPCs
- /help - Show help

"""

        if context.get("module"):
            prompt += f"Current module: {context['module']}\n"
        if context.get("chapter"):
            prompt += f"Current chapter: {context['chapter']}\n"

        # Add conversation history if available
        if conversation_history:
            prompt += "\n**Conversation History:**\n"
            for msg in conversation_history:
                role = msg.get("role", "").upper()
                content = msg.get("content", "")
                if role == "SYSTEM":
                    continue  # Skip system messages
                prompt += f"{role}: {content}\n"
            prompt += "\n"

        prompt += f"User: {message}\n\nAssistant:"

        return prompt
</file>

<file path="ui/backend/app/tools/base.py">
"""Base classes for tool system."""
from abc import ABC, abstractmethod
from typing import Dict, Any
from pydantic import BaseModel


class ToolSchema(BaseModel):
    """Schema for tool definition (Gemini function calling format)."""
    name: str
    description: str
    parameters: Dict[str, Any]  # JSON Schema format


class ToolResponse(BaseModel):
    """Standard tool response format."""
    type: str  # "text", "image", "scene", "error", etc.
    message: str
    data: Dict[str, Any] | None = None


class BaseTool(ABC):
    """Base class for all tools."""

    @abstractmethod
    def get_schema(self) -> ToolSchema:
        """Return the tool schema for Gemini function calling."""
        pass

    @abstractmethod
    async def execute(self, **kwargs) -> ToolResponse:
        """Execute the tool with given parameters."""
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        """Tool name (must match schema name)."""
        pass
</file>

<file path="ui/backend/app/tools/image_generator.py">
"""Image generation tool using Gemini Imagen."""
import asyncio
import uuid
import sys
from datetime import datetime
from pathlib import Path
from typing import List
from io import BytesIO
from .base import BaseTool, ToolSchema, ToolResponse
from ..config import settings

# Add project src to path for GeminiAPI
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root / "src"))
from util.gemini import GeminiAPI  # noqa: E402
from google.genai import types  # noqa: E402


class ImageGeneratorTool(BaseTool):
    """Tool for generating images using Gemini Imagen."""

    def __init__(self):
        """Initialize image generator."""
        self.output_dir = settings.IMAGE_OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.semaphore = asyncio.Semaphore(settings.IMAGEN_CONCURRENT_LIMIT)
        self.api = GeminiAPI(model_name="imagen-3.0-generate-002")

    @property
    def name(self) -> str:
        """Return tool name."""
        return "generate_images"

    def get_schema(self) -> ToolSchema:
        """Return tool schema for Gemini function calling."""
        return ToolSchema(
            name="generate_images",
            description="Generate images based on a text description. Use this when the user asks to create, generate, or show images.",
            parameters={
                "type": "object",
                "properties": {
                    "prompt": {
                        "type": "string",
                        "description": "Detailed description of the image to generate"
                    },
                    "count": {
                        "type": "integer",
                        "description": "Number of images to generate (default: 2, max: 4)",
                        "default": 2
                    }
                },
                "required": ["prompt"]
            }
        )

    async def execute(self, prompt: str, count: int = 2) -> ToolResponse:
        """
        Execute image generation.

        Args:
            prompt: Image description
            count: Number of images (default 2, max 4)

        Returns:
            ToolResponse with image URLs
        """
        print(f"[DEBUG] ImageGenerator.execute() called with prompt='{prompt}', count={count}")

        # Cap count at maximum
        count = min(count, settings.MAX_IMAGES_PER_REQUEST)

        try:
            # Generate images concurrently
            tasks = [self._generate_single_image(prompt) for _ in range(count)]
            filenames = await asyncio.gather(*tasks)

            # Convert filenames to URLs
            image_urls = [f"/api/images/{fn}" for fn in filenames]

            response = ToolResponse(
                type="image",
                message=f"Generated {count} images based on your description.",
                data={
                    "image_urls": image_urls,
                    "prompt": prompt
                }
            )
            print(f"[DEBUG] ImageGenerator returning: {response}")
            return response

        except Exception as e:
            return ToolResponse(
                type="error",
                message=f"Failed to generate images: {str(e)}",
                data=None
            )

    async def _generate_single_image(self, prompt: str) -> str:
        """
        Generate a single image.

        Args:
            prompt: Image description

        Returns:
            Filename of generated image
        """
        async with self.semaphore:
            # Generate unique filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_id = uuid.uuid4().hex[:8]
            filename = f"{timestamp}_{unique_id}.png"
            filepath = self.output_dir / filename

            # Generate image using Gemini Imagen
            # Run blocking API call in thread pool
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                self._generate_and_save_image,
                prompt,
                filepath
            )

            return filename

    def _generate_and_save_image(self, prompt: str, filepath: Path):
        """
        Blocking call to generate and save image.

        Args:
            prompt: Image description
            filepath: Where to save the image
        """
        # Generate image using Gemini Imagen
        response = self.api.client.models.generate_images(
            model="imagen-3.0-generate-002",
            prompt=prompt,
            config=types.GenerateImagesConfig(
                number_of_images=1,
            )
        )

        # Save the generated image
        if response.generated_images:
            generated_image = response.generated_images[0]
            # Extract PIL image and convert to bytes
            image_buffer = BytesIO()
            generated_image.image._pil_image.save(image_buffer, format='PNG')
            image_data = image_buffer.getvalue()

            # Write to file
            with open(filepath, 'wb') as f:
                f.write(image_data)
</file>

<file path="ui/backend/app/tools/registry.py">
"""Central registry for all tools."""
from typing import Dict, List
from .base import BaseTool, ToolSchema, ToolResponse


class ToolRegistry:
    """Central registry for all tools."""

    def __init__(self):
        """Initialize empty registry."""
        self.tools: Dict[str, BaseTool] = {}

    def register(self, tool: BaseTool):
        """
        Register a tool.

        Args:
            tool: Tool instance to register
        """
        self.tools[tool.name] = tool

    def get_schemas(self) -> List[ToolSchema]:
        """
        Get all tool schemas for Gemini.

        Returns:
            List of tool schemas
        """
        return [tool.get_schema() for tool in self.tools.values()]

    async def execute_tool(self, tool_name: str, **kwargs) -> ToolResponse:
        """
        Execute a tool by name.

        Args:
            tool_name: Name of tool to execute
            **kwargs: Tool parameters

        Returns:
            Tool response

        Raises:
            ValueError: If tool not found
        """
        if tool_name not in self.tools:
            raise ValueError(f"Unknown tool: {tool_name}")
        return await self.tools[tool_name].execute(**kwargs)


# Global registry instance
registry = ToolRegistry()
</file>

<file path="ui/backend/app/config.py">
"""Configuration for chat backend."""
from pathlib import Path


class Settings:
    """Application settings."""

    # Tool settings
    MAX_IMAGES_PER_REQUEST = 4
    IMAGE_STORAGE_DAYS = 7
    IMAGE_OUTPUT_DIR = Path("app/output/chat_images")

    # Gemini settings
    GEMINI_MODEL = "gemini-2.0-flash"
    GEMINI_TIMEOUT = 60  # seconds

    # Image generation
    IMAGEN_CONCURRENT_LIMIT = 2  # Max parallel image generation


# Global settings instance
settings = Settings()
</file>

<file path="ui/backend/app/main.py">
"""D&D Module Assistant API."""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pathlib import Path
from dotenv import load_dotenv
from app.routers import chat
from app.config import settings

# Load environment variables from project root .env
project_root = Path(__file__).parent.parent.parent.parent
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
else:
    # Fallback to backend .env
    load_dotenv()

app = FastAPI(
    title="D&D Module Assistant API",
    description="Backend API for D&D module generation and management",
    version="0.1.0"
)

# CORS middleware for local development
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:5174"],  # Vite ports
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Register routers
app.include_router(chat.router)


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "service": "module-assistant-api"}


@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "message": "D&D Module Assistant API",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/api/images/{filename}")
async def serve_image(filename: str):
    """
    Serve generated images from chat_images directory.

    Args:
        filename: Image filename

    Returns:
        Image file

    Raises:
        HTTPException: If file not found or invalid filename
    """
    # Security: validate filename (no path traversal)
    if ".." in filename or "/" in filename or "\\" in filename:
        raise HTTPException(status_code=400, detail="Invalid filename")

    # Only serve .png files
    if not filename.endswith(".png"):
        raise HTTPException(status_code=400, detail="Only PNG files supported")

    file_path = settings.IMAGE_OUTPUT_DIR / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Image not found")

    return FileResponse(file_path, media_type="image/png")
</file>

<file path="ui/backend/tests/integration/__init__.py">
"""Integration tests for chat UI backend."""
</file>

<file path="ui/backend/tests/integration/test_image_generation_flow.py">
"""Integration test for complete image generation flow."""
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, AsyncMock
from app.main import app


client = TestClient(app)


@pytest.mark.integration
class TestImageGenerationFlow:
    """Test complete image generation workflow."""

    def test_full_image_generation_flow(self):
        """Test end-to-end image generation."""
        # Mock Gemini service's generate_with_tools method
        with patch('app.routers.chat.gemini_service.generate_with_tools', new_callable=AsyncMock) as mock_generate:
            mock_generate.return_value = {
                "type": "tool_call",
                "tool_call": {
                    "name": "generate_images",
                    "parameters": {"prompt": "a majestic dragon", "count": 2}
                },
                "text": None
            }

            # Send chat message
            response = client.post("/api/chat", json={
                "message": "Show me a majestic dragon",
                "context": {},
                "conversation_history": []
            })

        # Verify response
        assert response.status_code == 200
        data = response.json()
        assert data["type"] == "image"
        assert "Generated 2 images" in data["message"]
        assert len(data["data"]["image_urls"]) == 2
        assert data["data"]["prompt"] == "a majestic dragon"

        # Verify image URLs are correct format
        for url in data["data"]["image_urls"]:
            assert url.startswith("/api/images/")
            assert url.endswith(".png")
</file>

<file path="ui/backend/tests/routers/test_chat.py">
"""Tests for chat router with tools."""
import pytest
from unittest.mock import patch, AsyncMock
from fastapi.testclient import TestClient
from app.main import app


client = TestClient(app)


class TestChatWithTools:
    """Test chat endpoint with tool support."""

    def test_chat_text_response(self):
        """Test chat returns text response when no tool called."""
        with patch('app.routers.chat.gemini_service') as mock_service:
            mock_service.generate_with_tools = AsyncMock(return_value={
                "type": "text",
                "text": "Hello there!",
                "tool_call": None
            })

            response = client.post("/api/chat", json={
                "message": "Hello",
                "context": {},
                "conversation_history": []
            })

        assert response.status_code == 200
        data = response.json()
        assert data["type"] == "text"
        assert data["message"] == "Hello there!"

    def test_chat_tool_response(self):
        """Test chat executes tool when called."""
        with patch('app.routers.chat.gemini_service') as mock_service, \
             patch('app.routers.chat.registry') as mock_registry:

            mock_service.generate_with_tools = AsyncMock(return_value={
                "type": "tool_call",
                "tool_call": {
                    "name": "generate_images",
                    "parameters": {"prompt": "dragon", "count": 2}
                },
                "text": None
            })

            mock_registry.execute_tool = AsyncMock(return_value=type('obj', (object,), {
                'type': 'image',
                'message': 'Generated 2 images',
                'data': {'image_urls': ['/api/images/test.png']}
            })())

            response = client.post("/api/chat", json={
                "message": "Show me a dragon",
                "context": {},
                "conversation_history": []
            })

        assert response.status_code == 200
        data = response.json()
        assert data["type"] == "image"
        assert "Generated 2 images" in data["message"]
</file>

<file path="ui/backend/tests/services/test_gemini_service.py">
"""Tests for Gemini service with function calling."""
import pytest
from unittest.mock import Mock, patch, MagicMock
from app.services.gemini_service import GeminiService
from app.tools.base import ToolSchema


class TestGeminiServiceFunctionCalling:
    """Test Gemini service function calling."""

    @pytest.fixture
    def mock_genai_client(self):
        """Create mock Gemini client."""
        with patch('app.services.gemini_service.GeminiAPI') as mock:
            yield mock

    def test_schema_to_gemini_tool(self, mock_genai_client):
        """Test converting ToolSchema to Gemini tool format."""
        service = GeminiService()
        schema = ToolSchema(
            name="test_tool",
            description="A test tool",
            parameters={
                "type": "object",
                "properties": {
                    "param1": {"type": "string", "description": "First param"}
                },
                "required": ["param1"]
            }
        )

        gemini_tool = service._schema_to_gemini_tool(schema)

        assert gemini_tool["name"] == "test_tool"
        assert gemini_tool["description"] == "A test tool"
        assert "parameters" in gemini_tool

    @pytest.mark.anyio
    async def test_generate_with_tools_no_tool_call(self, mock_genai_client):
        """Test generate_with_tools when no tool is called."""
        service = GeminiService()
        mock_response = Mock()
        mock_response.text = "Regular text response"
        mock_response.candidates = [Mock(function_call=None)]

        with patch.object(service.api, 'generate_content', return_value=mock_response):
            response = await service.generate_with_tools(
                message="Hello",
                conversation_history=[],
                tools=[]
            )

        assert response["type"] == "text"
        assert response["text"] == "Regular text response"
        assert response["tool_call"] is None

    @pytest.mark.anyio
    async def test_generate_with_tools_with_tool_call(self, mock_genai_client):
        """Test generate_with_tools when tool is called."""
        service = GeminiService()
        mock_function_call = Mock()
        mock_function_call.name = "generate_images"
        mock_function_call.args = {"prompt": "a dragon", "count": 2}

        mock_response = Mock()
        mock_response.candidates = [Mock(function_call=mock_function_call)]

        with patch.object(service.api, 'generate_content', return_value=mock_response):
            response = await service.generate_with_tools(
                message="Show me a dragon",
                conversation_history=[],
                tools=[]
            )

        assert response["type"] == "tool_call"
        assert response["tool_call"]["name"] == "generate_images"
        assert response["tool_call"]["parameters"]["prompt"] == "a dragon"
</file>

<file path="ui/backend/tests/tools/test_base.py">
"""Tests for tool system base classes."""
import pytest
from app.tools.base import ToolSchema, ToolResponse, BaseTool


class TestToolSchema:
    """Test ToolSchema model."""

    def test_tool_schema_creation(self):
        """Test creating a valid tool schema."""
        schema = ToolSchema(
            name="test_tool",
            description="A test tool",
            parameters={
                "type": "object",
                "properties": {
                    "param1": {"type": "string"}
                },
                "required": ["param1"]
            }
        )

        assert schema.name == "test_tool"
        assert schema.description == "A test tool"
        assert "param1" in schema.parameters["properties"]


class TestToolResponse:
    """Test ToolResponse model."""

    def test_tool_response_creation(self):
        """Test creating a valid tool response."""
        response = ToolResponse(
            type="text",
            message="Response message",
            data={"key": "value"}
        )

        assert response.type == "text"
        assert response.message == "Response message"
        assert response.data["key"] == "value"

    def test_tool_response_without_data(self):
        """Test tool response with no data field."""
        response = ToolResponse(
            type="error",
            message="Error occurred"
        )

        assert response.type == "error"
        assert response.data is None
</file>

<file path="ui/backend/tests/tools/test_image_generator.py">
"""Tests for image generator tool."""
import pytest
from unittest.mock import Mock, patch, AsyncMock
from pathlib import Path
from app.tools.image_generator import ImageGeneratorTool
from app.tools.base import ToolResponse


class TestImageGeneratorTool:
    """Test ImageGeneratorTool."""

    def test_get_schema(self):
        """Test tool schema."""
        tool = ImageGeneratorTool()
        schema = tool.get_schema()

        assert schema.name == "generate_images"
        assert "prompt" in schema.parameters["properties"]
        assert "count" in schema.parameters["properties"]
        assert "prompt" in schema.parameters["required"]

    def test_name_property(self):
        """Test tool name property."""
        tool = ImageGeneratorTool()
        assert tool.name == "generate_images"

    @pytest.mark.anyio
    async def test_execute_caps_count_at_max(self):
        """Test execute caps count at maximum."""
        tool = ImageGeneratorTool()

        with patch.object(tool, '_generate_single_image', new_callable=AsyncMock) as mock_gen:
            mock_gen.return_value = "test.png"

            response = await tool.execute(prompt="test", count=10)

        assert mock_gen.call_count == 4  # Capped at MAX_IMAGES_PER_REQUEST

    @pytest.mark.anyio
    async def test_execute_returns_image_response(self):
        """Test execute returns correct response format."""
        tool = ImageGeneratorTool()

        with patch.object(tool, '_generate_single_image', new_callable=AsyncMock) as mock_gen:
            mock_gen.return_value = "test_123.png"

            response = await tool.execute(prompt="a dragon", count=2)

        assert response.type == "image"
        assert "Generated 2 images" in response.message
        assert len(response.data["image_urls"]) == 2
        assert response.data["prompt"] == "a dragon"
</file>

<file path="ui/backend/tests/tools/test_registry.py">
"""Tests for tool registry."""
import pytest
from app.tools.base import BaseTool, ToolSchema, ToolResponse
from app.tools.registry import ToolRegistry


class MockTool(BaseTool):
    """Mock tool for testing."""

    @property
    def name(self) -> str:
        return "mock_tool"

    def get_schema(self) -> ToolSchema:
        return ToolSchema(
            name="mock_tool",
            description="A mock tool",
            parameters={"type": "object", "properties": {}}
        )

    async def execute(self, **kwargs) -> ToolResponse:
        return ToolResponse(type="text", message="Mock response")


class TestToolRegistry:
    """Test ToolRegistry."""

    def test_register_tool(self):
        """Test registering a tool."""
        registry = ToolRegistry()
        tool = MockTool()

        registry.register(tool)

        assert "mock_tool" in registry.tools
        assert registry.tools["mock_tool"] == tool

    def test_get_schemas(self):
        """Test getting all tool schemas."""
        registry = ToolRegistry()
        tool = MockTool()
        registry.register(tool)

        schemas = registry.get_schemas()

        assert len(schemas) == 1
        assert schemas[0].name == "mock_tool"

    @pytest.mark.anyio
    async def test_execute_tool(self):
        """Test executing a tool by name."""
        registry = ToolRegistry()
        tool = MockTool()
        registry.register(tool)

        response = await registry.execute_tool("mock_tool")

        assert response.type == "text"
        assert response.message == "Mock response"

    @pytest.mark.anyio
    async def test_execute_unknown_tool_raises(self):
        """Test executing unknown tool raises error."""
        registry = ToolRegistry()

        with pytest.raises(ValueError, match="Unknown tool"):
            await registry.execute_tool("nonexistent_tool")
</file>

<file path="ui/backend/tests/test_chat_router.py">
"""Tests for chat router."""

import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, Mock
from app.main import app


@pytest.fixture
def client():
    """Create test client."""
    return TestClient(app)


def test_chat_endpoint_basic(client):
    """Test basic chat endpoint."""
    with patch('app.routers.chat.GeminiService') as mock_service:
        mock_instance = Mock()
        mock_instance.generate_chat_response.return_value = "Hello! How can I help?"
        mock_service.return_value = mock_instance

        response = client.post(
            "/api/chat",
            json={"message": "Hello", "context": {}}
        )

        assert response.status_code == 200
        data = response.json()
        assert data["type"] == "text"
        assert "Hello" in data["message"]


def test_chat_endpoint_generate_scene_command(client):
    """Test /generate-scene command."""
    with patch('app.routers.chat.GeminiService') as mock_service:
        mock_instance = Mock()
        mock_instance.generate_scene_description.return_value = "A dark cave entrance"
        mock_service.return_value = mock_instance

        response = client.post(
            "/api/chat",
            json={"message": "/generate-scene dark cave", "context": {}}
        )

        assert response.status_code == 200
        data = response.json()
        assert data["type"] == "scene"
        assert "cave" in data["message"].lower()


def test_chat_endpoint_help_command(client):
    """Test /help command."""
    response = client.post(
        "/api/chat",
        json={"message": "/help", "context": {}}
    )

    assert response.status_code == 200
    data = response.json()
    assert data["type"] == "text"
    assert "commands" in data["message"].lower()
</file>

<file path="ui/backend/tests/test_command_parser.py">
"""Tests for slash command parser."""

import pytest
from app.services.command_parser import (
    CommandParser,
    ParsedCommand,
    CommandType
)


def test_parse_generate_scene():
    """Test parsing /generate-scene command."""
    parser = CommandParser()
    cmd = parser.parse("/generate-scene dark cave entrance")

    assert cmd.type == CommandType.GENERATE_SCENE
    assert cmd.args == "dark cave entrance"
    assert cmd.is_command is True


def test_parse_list_scenes():
    """Test parsing /list-scenes command."""
    parser = CommandParser()
    cmd = parser.parse("/list-scenes Chapter 2")

    assert cmd.type == CommandType.LIST_SCENES
    assert cmd.args == "Chapter 2"


def test_parse_help():
    """Test parsing /help command."""
    parser = CommandParser()
    cmd = parser.parse("/help")

    assert cmd.type == CommandType.HELP
    assert cmd.args == ""


def test_parse_regular_message():
    """Test parsing non-command message."""
    parser = CommandParser()
    cmd = parser.parse("Hello, how are you?")

    assert cmd.type == CommandType.CHAT
    assert cmd.args == "Hello, how are you?"
    assert cmd.is_command is False


def test_parse_unknown_command():
    """Test parsing unknown slash command."""
    parser = CommandParser()
    cmd = parser.parse("/unknown-command test")

    assert cmd.type == CommandType.UNKNOWN
    assert cmd.is_command is True
</file>

<file path="ui/backend/tests/test_config.py">
"""Tests for configuration."""
from app.config import Settings


class TestSettings:
    """Test Settings class."""

    def test_settings_defaults(self):
        """Test default settings values."""
        settings = Settings()

        assert settings.MAX_IMAGES_PER_REQUEST == 4
        assert settings.IMAGE_STORAGE_DAYS == 7
        assert settings.GEMINI_MODEL == "gemini-2.0-flash"
        assert settings.GEMINI_TIMEOUT == 60
        assert settings.IMAGEN_CONCURRENT_LIMIT == 2
</file>

<file path="ui/backend/tests/test_gemini_service.py">
"""Tests for Gemini service."""

import sys
import pytest
from unittest.mock import Mock, patch, MagicMock

# Mock the util.gemini module before any imports
sys.modules['util'] = MagicMock()
sys.modules['util.gemini'] = MagicMock()

from app.services.gemini_service import GeminiService


@pytest.fixture
def gemini_service():
    """Create GeminiService with mocked API."""
    # Patch GeminiAPI at the module level
    with patch('app.services.gemini_service.GeminiAPI') as mock_api_class:
        # Create a mock API instance
        mock_api_instance = Mock()
        mock_api_class.return_value = mock_api_instance

        # Create service (will use mocked API)
        service = GeminiService()

        yield service


def test_generate_chat_response(gemini_service):
    """Test generating chat response."""
    # Mock API response
    mock_response = Mock()
    mock_response.text = "This is a test response from Gemini."
    gemini_service.api.generate_content.return_value = mock_response

    result = gemini_service.generate_chat_response(
        message="Hello",
        context={}
    )

    assert result == "This is a test response from Gemini."
    gemini_service.api.generate_content.assert_called_once()


def test_generate_scene_description(gemini_service):
    """Test generating scene description."""
    mock_response = Mock()
    mock_response.text = "A dark cave with dripping water and moss-covered walls."
    gemini_service.api.generate_content.return_value = mock_response

    result = gemini_service.generate_scene_description("dark cave")

    assert "dark cave" in result.lower() or "dripping water" in result
</file>

<file path="ui/backend/tests/test_main.py">
"""Tests for main app endpoints."""
import pytest
from fastapi.testclient import TestClient
from pathlib import Path
from app.main import app
from app.config import settings


client = TestClient(app)


class TestImageServing:
    """Test image serving endpoint."""

    def test_serve_existing_image(self, tmp_path):
        """Test serving an existing image."""
        # Create test image
        test_image = settings.IMAGE_OUTPUT_DIR / "test_image.png"
        test_image.parent.mkdir(parents=True, exist_ok=True)
        test_image.write_bytes(b"fake image data")

        try:
            response = client.get("/api/images/test_image.png")

            assert response.status_code == 200
            assert response.headers["content-type"] == "image/png"
            assert response.content == b"fake image data"
        finally:
            test_image.unlink()

    def test_serve_nonexistent_image(self):
        """Test serving nonexistent image returns 404."""
        response = client.get("/api/images/nonexistent.png")
        assert response.status_code == 404

    def test_serve_image_path_traversal_blocked(self):
        """Test path traversal attempts are blocked."""
        response = client.get("/api/images/../../../etc/passwd")
        assert response.status_code in [400, 404]
</file>

<file path="ui/backend/tests/test_models.py">
"""Tests for API models."""

import pytest
from app.models.chat import ChatMessage, ChatRole, ChatRequest, ChatResponse
from app.models.scene import Scene


def test_chat_message_creation():
    """Test ChatMessage model creation."""
    msg = ChatMessage(role=ChatRole.USER, content="Hello")
    assert msg.role == ChatRole.USER
    assert msg.content == "Hello"
    assert msg.timestamp is not None


def test_chat_request_validation():
    """Test ChatRequest validates message."""
    req = ChatRequest(message="test message")
    assert req.message == "test message"
    assert req.context == {}


def test_chat_response_creation():
    """Test ChatResponse model."""
    resp = ChatResponse(
        message="Generated response",
        type="text"
    )
    assert resp.message == "Generated response"
    assert resp.type == "text"
    assert resp.data is None


def test_scene_model_import():
    """Test Scene model can be imported."""
    scene = Scene(
        section_path="Chapter 1 ‚Üí Area 1",
        name="Test Cave",
        description="A dark cave entrance",
        location_type="underground"
    )
    assert scene.name == "Test Cave"
    assert scene.location_type == "underground"
</file>

<file path="ui_prototypes/screenshot_html.py">
#!/usr/bin/env python3
"""
Screenshot HTML files for visual review.
Requires: playwright
Install with: uv pip install playwright && uv run playwright install chromium
"""
import sys
from pathlib import Path
from playwright.sync_api import sync_playwright

def screenshot_html(html_path: str, output_path: str = None):
    """Take a screenshot of an HTML file."""
    html_file = Path(html_path)
    if not html_file.exists():
        print(f"Error: {html_path} not found")
        return

    if output_path is None:
        output_path = html_file.with_suffix('.png')

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page(viewport={'width': 1400, 'height': 1000})
        page.goto(f'file://{html_file.absolute()}')
        page.screenshot(path=output_path, full_page=True)
        browser.close()

    print(f"Screenshot saved to: {output_path}")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python screenshot_html.py <html_file> [output_png]")
        sys.exit(1)

    html_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else None
    screenshot_html(html_path, output_path)
</file>

<file path="scripts/chapter2_to_html.py">
#!/usr/bin/env python3
"""Quick script to process Chapter 2 to HTML."""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from pdf_processing.pdf_to_html import process_pdf_to_html


if __name__ == "__main__":
    html_path = process_pdf_to_html(
        pdf_path="02_Part_1_Goblin_Arrows.pdf",
        map_positioning_mode="semantic",
        extract_maps=True,
        open_html=True
    )

    print(f"\n‚úÖ Generated: {html_path}")
</file>

<file path="scripts/generate_import_diagram.py">
#!/usr/bin/env python3
"""
Generate a Mermaid ERD diagram showing local Python file imports.

Usage:
    python scripts/generate_import_diagram.py
    python scripts/generate_import_diagram.py --source-dir src --output-dir output/diagrams
"""

import ast
import os
import sys
from pathlib import Path
from typing import Dict, Set, Tuple
import argparse


def get_project_root() -> Path:
    """Get the project root directory."""
    return Path(__file__).parent.parent


def extract_imports(file_path: Path, project_root: Path) -> Set[str]:
    """
    Extract local imports from a Python file.

    Returns a set of module paths that are imported from within the project.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            tree = ast.parse(f.read(), filename=str(file_path))
    except (SyntaxError, UnicodeDecodeError) as e:
        print(f"Warning: Could not parse {file_path}: {e}", file=sys.stderr)
        return set()

    imports = set()

    for node in ast.walk(tree):
        # Handle "import module" and "import module.submodule"
        if isinstance(node, ast.Import):
            for alias in node.names:
                module_name = alias.name.split('.')[0]
                imports.add(module_name)

        # Handle "from module import something"
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                module_name = node.module.split('.')[0]
                imports.add(module_name)
            # Handle relative imports (from . import X, from .. import Y)
            elif node.level > 0:
                # Resolve relative import to absolute module path
                relative_path = resolve_relative_import(file_path, node.level, project_root)
                if relative_path:
                    imports.add(relative_path)

    return imports


def resolve_relative_import(file_path: Path, level: int, project_root: Path) -> str:
    """
    Resolve relative imports like 'from . import X' or 'from .. import Y'.

    Args:
        file_path: Path to the current Python file
        level: Number of dots in the relative import (1 for '.', 2 for '..', etc.)
        project_root: Root directory of the project

    Returns:
        Module name as it would appear in the project structure
    """
    # Get the directory of the current file
    current_dir = file_path.parent

    # Go up 'level' directories
    target_dir = current_dir
    for _ in range(level):
        target_dir = target_dir.parent

    # Convert to module path relative to project root
    try:
        relative = target_dir.relative_to(project_root)
        # Convert path to module notation (replace / with .)
        module_path = str(relative).replace(os.sep, '.')
        return module_path if module_path != '.' else ''
    except ValueError:
        # target_dir is not relative to project_root
        return ''


def is_local_module(module_name: str, project_root: Path, source_dirs: list) -> bool:
    """
    Check if a module is local to the project (not an external package).

    Args:
        module_name: Name of the module (e.g., 'pdf_processing', 'foundry')
        project_root: Root directory of the project
        source_dirs: List of source directories to check (e.g., ['src', 'scripts'])

    Returns:
        True if the module is local to the project
    """
    # Check if module exists as a directory or file in any source directory
    for source_dir in source_dirs:
        base_path = project_root / source_dir

        # Check if it's a package (directory with __init__.py)
        package_path = base_path / module_name
        if package_path.is_dir() and (package_path / '__init__.py').exists():
            return True

        # Check if it's a single file module
        file_path = base_path / f"{module_name}.py"
        if file_path.is_file():
            return True

        # Check nested modules (e.g., 'pdf_processing.split_pdf')
        if '.' in module_name:
            parts = module_name.split('.')
            nested_path = base_path / '/'.join(parts[:-1]) / f"{parts[-1]}.py"
            if nested_path.is_file():
                return True

    return False


def get_module_name(file_path: Path, project_root: Path) -> str:
    """
    Convert a file path to a module name.

    Examples:
        src/pdf_processing/split_pdf.py -> pdf_processing.split_pdf
        scripts/generate_scene_art.py -> scripts.generate_scene_art
    """
    relative = file_path.relative_to(project_root)
    # Remove .py extension
    module_path = str(relative.with_suffix(''))
    # Convert to dot notation
    return module_path.replace(os.sep, '.')


def build_dependency_graph(source_dir: Path, project_root: Path,
                          source_dirs: list) -> Dict[str, Set[str]]:
    """
    Build a dependency graph of all local Python imports.

    Returns:
        Dict mapping module names to sets of modules they import
    """
    graph = {}

    # Find all Python files
    py_files = list(source_dir.rglob('*.py'))

    for py_file in py_files:
        # Skip __pycache__ and hidden files
        if '__pycache__' in str(py_file) or py_file.name.startswith('.'):
            continue

        module_name = get_module_name(py_file, project_root)
        imports = extract_imports(py_file, project_root)

        # Filter to only local imports
        local_imports = {
            imp for imp in imports
            if is_local_module(imp, project_root, source_dirs)
        }

        if local_imports:  # Only add if there are local imports
            graph[module_name] = local_imports

    return graph


def sanitize_node_name(name: str) -> str:
    """
    Sanitize module name for Mermaid syntax.
    Replace dots with underscores and wrap in quotes if needed.
    """
    return name.replace('.', '_')


def group_modules_by_folder(modules: Set[str]) -> Dict[str, list]:
    """
    Group modules by their parent folder.

    Example:
        src.actors.parse_stat_blocks -> 'actors': ['src.actors.parse_stat_blocks', ...]
        src.foundry.client -> 'foundry': ['src.foundry.client', ...]
    """
    groups = {}

    for module in modules:
        parts = module.split('.')

        # Determine the group name
        if len(parts) == 1:
            # Top-level module (e.g., 'util', 'logging_config')
            group_name = 'root'
        elif parts[0] == 'src':
            # For src.X.Y, group by X (e.g., 'actors', 'foundry', 'pdf_processing')
            if len(parts) > 1:
                group_name = parts[1]
            else:
                group_name = 'src'
        else:
            # For other structures, use first part
            group_name = parts[0]

        if group_name not in groups:
            groups[group_name] = []
        groups[group_name].append(module)

    return groups


def generate_graphviz_dot(graph: Dict[str, Set[str]]) -> str:
    """
    Generate Graphviz DOT format from dependency graph with proper clustering.

    Format: A -> B means "B imports from A" (arrow points to dependent)
    """
    lines = ['digraph ImportDependencies {']
    lines.append('    // Graph settings')
    lines.append('    rankdir=LR;')
    lines.append('    bgcolor="#0f172a";')
    lines.append('    node [shape=box, style=filled, fontname="Arial", fontsize=14, height=0.5];')
    lines.append('    edge [color="#22d3ee", penwidth=3, arrowsize=1.2];')
    lines.append('    compound=true;')
    lines.append('    newrank=true;')
    lines.append('')

    # Collect all unique modules
    all_modules = set()
    for importer, imported_modules in graph.items():
        all_modules.add(importer)
        all_modules.update(imported_modules)

    # Group modules by folder
    groups = group_modules_by_folder(all_modules)

    # Color schemes for different groups
    colors = {
        'root': {'fill': '#dc2626', 'border': '#fca5a5'},
        'actors': {'fill': '#7c3aed', 'border': '#c4b5fd'},
        'foundry': {'fill': '#0891b2', 'border': '#67e8f9'},
        'pdf_processing': {'fill': '#ea580c', 'border': '#fdba74'}
    }

    # Create clusters for each group
    for group_idx, (group_name, modules) in enumerate(sorted(groups.items())):
        color_scheme = colors.get(group_name, colors['root'])
        group_label = group_name.replace('_', ' ').title()

        lines.append(f'    subgraph cluster_{group_idx} {{')
        lines.append(f'        label="{group_label}";')
        lines.append(f'        style=filled;')
        lines.append(f'        bgcolor="#1e293b";')
        lines.append(f'        color="{color_scheme["border"]}";')
        lines.append(f'        penwidth=3;')
        lines.append(f'        fontcolor="white";')
        lines.append(f'        fontsize=16;')
        lines.append('')

        # Define nodes within this cluster
        for module in sorted(modules):
            safe_name = sanitize_node_name(module)
            display_name = module.split('.')[-1]
            lines.append(f'        {safe_name} [label="{display_name}", fillcolor="{color_scheme["fill"]}", color="{color_scheme["border"]}", fontcolor="white", penwidth=2];')

        lines.append('    }')
        lines.append('')

    # Add edges
    for importer, imported_modules in sorted(graph.items()):
        safe_importer = sanitize_node_name(importer)
        for imported in sorted(imported_modules):
            safe_imported = sanitize_node_name(imported)
            # Arrow points FROM imported TO importer
            lines.append(f'    {safe_imported} -> {safe_importer};')

    lines.append('}')
    return '\n'.join(lines)


def generate_mermaid_erd(graph: Dict[str, Set[str]]) -> str:
    """
    Generate Mermaid flowchart from dependency graph with color-coded nodes.

    Format: A --> B means "A imports from B"
    """
    lines = ["%%{init: {'theme':'dark', 'themeVariables': { 'fontSize':'20px'}, 'flowchart': {'curve': 'basis', 'padding': 30, 'nodeSpacing': 100, 'rankSpacing': 150}}}%%"]
    lines.append("graph LR")

    # Define color schemes for different groups
    lines.append("    classDef rootClass fill:#dc2626,stroke:#fca5a5,stroke-width:4px,color:#fff,font-size:18px")
    lines.append("    classDef actorsClass fill:#7c3aed,stroke:#c4b5fd,stroke-width:4px,color:#fff,font-size:18px")
    lines.append("    classDef foundryClass fill:#0891b2,stroke:#67e8f9,stroke-width:4px,color:#fff,font-size:18px")
    lines.append("    classDef pdfClass fill:#ea580c,stroke:#fdba74,stroke-width:4px,color:#fff,font-size:18px")
    lines.append("")

    # Collect all unique modules
    all_modules = set()
    for importer, imported_modules in graph.items():
        all_modules.add(importer)
        all_modules.update(imported_modules)

    # Group modules by folder
    groups = group_modules_by_folder(all_modules)

    # Color mapping
    color_map = {
        'root': 'rootClass',
        'actors': 'actorsClass',
        'foundry': 'foundryClass',
        'pdf_processing': 'pdfClass'
    }

    # Track which nodes belong to which class
    node_classes = {}

    # Define all nodes with labels
    for group_name, modules in sorted(groups.items()):
        class_name = color_map.get(group_name, 'rootClass')
        for module in sorted(modules):
            safe_name = sanitize_node_name(module)
            display_name = module.split('.')[-1]
            # Add group prefix for clarity
            if group_name != 'root':
                label = f"{group_name.replace('_', ' ').title()}: {display_name}"
            else:
                label = display_name
            lines.append(f'    {safe_name}["{label}"]')
            node_classes[safe_name] = class_name

    lines.append("")

    # Add edges (A --> B means "A imports from B")
    for importer, imported_modules in sorted(graph.items()):
        safe_importer = sanitize_node_name(importer)
        for imported in sorted(imported_modules):
            safe_imported = sanitize_node_name(imported)
            # Arrow points FROM imported TO importer (showing dependency direction)
            lines.append(f'    {safe_imported} ==> {safe_importer}')

    lines.append("")

    # Apply classes to nodes
    for class_name in set(node_classes.values()):
        nodes_in_class = [node for node, cls in node_classes.items() if cls == class_name]
        if nodes_in_class:
            nodes_str = ','.join(nodes_in_class)
            lines.append(f'    class {nodes_str} {class_name}')

    # Style all links to be bright cyan and very thick for visibility
    lines.append("")
    lines.append("    linkStyle default stroke:#22d3ee,stroke-width:6px")

    return '\n'.join(lines)


def save_mermaid_markdown(mermaid_code: str, output_path: Path):
    """Save Mermaid diagram as markdown file."""
    content = f"""# Python Import Dependencies

This diagram shows the import relationships between Python modules in the project.

**Legend:** `B --> A` means "Module A imports from Module B" (arrow points from dependency to dependent)

```mermaid
{mermaid_code}
```

---
*Generated by `scripts/generate_import_diagram.py`*
"""

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"‚úì Saved Mermaid markdown to: {output_path}")


def convert_to_png(markdown_path: Path, output_png_path: Path) -> bool:
    """
    Convert Mermaid diagram to PNG using mermaid-cli (mmdc).

    Returns True if successful, False otherwise.
    """
    import subprocess

    # Check if mmdc is installed
    try:
        subprocess.run(['mmdc', '--version'], capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("\n‚ö† Warning: mermaid-cli (mmdc) not found.")
        print("To generate PNG diagrams, install it with:")
        print("  npm install -g @mermaid-js/mermaid-cli")
        print("\nAlternatively, you can:")
        print("  1. Copy the markdown content from the .md file")
        print("  2. Paste it into https://mermaid.live")
        print("  3. Export as PNG from there")
        return False

    try:
        # Extract just the mermaid code (without markdown wrapper)
        with open(markdown_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Extract mermaid code block
        start = content.find('```mermaid\n') + len('```mermaid\n')
        end = content.find('```', start)
        mermaid_code = content[start:end].strip()

        # Create temp .mmd file
        temp_mmd = markdown_path.with_suffix('.mmd')
        with open(temp_mmd, 'w', encoding='utf-8') as f:
            f.write(mermaid_code)

        # Convert to PNG with higher resolution and better settings
        subprocess.run([
            'mmdc',
            '-i', str(temp_mmd),
            '-o', str(output_png_path),
            '-b', '#0f172a',  # Dark background
            '-w', '4800',  # Width in pixels (increased)
            '-H', '3200',  # Height in pixels (increased)
            '-s', '3'  # Scale factor (increased)
        ], check=True, capture_output=True)

        # Clean up temp file
        temp_mmd.unlink()

        print(f"‚úì Saved PNG diagram to: {output_png_path}")
        return True

    except subprocess.CalledProcessError as e:
        print(f"‚úó Error converting to PNG: {e}")
        print(f"  stderr: {e.stderr.decode() if e.stderr else 'N/A'}")
        return False


def convert_dot_to_png(dot_path: Path, output_png_path: Path) -> bool:
    """
    Convert Graphviz DOT file to PNG.

    Returns True if successful, False otherwise.
    """
    import subprocess

    # Check if dot is installed
    try:
        subprocess.run(['dot', '-V'], capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("\n‚ö† Warning: Graphviz (dot) not found.")
        print("To generate PNG diagrams, install it with:")
        print("  macOS: brew install graphviz")
        print("  Ubuntu/Debian: sudo apt-get install graphviz")
        print("  Windows: choco install graphviz")
        return False

    try:
        # Convert DOT to PNG
        subprocess.run([
            'dot',
            '-Tpng',
            str(dot_path),
            '-o', str(output_png_path),
            '-Gdpi=300'  # High resolution
        ], check=True, capture_output=True)

        print(f"‚úì Saved PNG diagram to: {output_png_path}")
        return True

    except subprocess.CalledProcessError as e:
        print(f"‚úó Error converting to PNG: {e}")
        print(f"  stderr: {e.stderr.decode() if e.stderr else 'N/A'}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='Generate Mermaid ERD showing Python import dependencies'
    )
    parser.add_argument(
        '--source-dir',
        type=str,
        default='src',
        help='Source directory to analyze (default: src)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='output/diagrams',
        help='Output directory for diagrams (default: output/diagrams)'
    )
    parser.add_argument(
        '--include-scripts',
        action='store_true',
        help='Also analyze scripts/ directory'
    )
    parser.add_argument(
        '--no-png',
        action='store_true',
        help='Skip PNG generation (only create markdown)'
    )

    args = parser.parse_args()

    project_root = get_project_root()
    source_dir = project_root / args.source_dir
    output_dir = project_root / args.output_dir

    if not source_dir.exists():
        print(f"Error: Source directory not found: {source_dir}")
        sys.exit(1)

    # Determine which directories to consider as "local"
    source_dirs = [args.source_dir]
    if args.include_scripts:
        source_dirs.append('scripts')

    print(f"Analyzing Python imports in: {source_dir}")

    # Build dependency graph
    graph = build_dependency_graph(source_dir, project_root, source_dirs)

    if not graph:
        print("No local imports found!")
        sys.exit(0)

    print(f"Found {len(graph)} modules with local imports")

    # Generate Graphviz DOT diagram
    dot_code = generate_graphviz_dot(graph)

    # Save DOT file
    dot_path = output_dir / 'import_dependencies.dot'
    output_dir.mkdir(parents=True, exist_ok=True)
    with open(dot_path, 'w', encoding='utf-8') as f:
        f.write(dot_code)
    print(f"‚úì Saved Graphviz DOT to: {dot_path}")

    # Convert to PNG using Graphviz
    if not args.no_png:
        png_path = output_dir / 'import_dependencies.png'
        convert_dot_to_png(dot_path, png_path)

    # Also generate Mermaid for markdown viewing
    mermaid_code = generate_mermaid_erd(graph)
    markdown_path = output_dir / 'import_dependencies.md'
    save_mermaid_markdown(mermaid_code, markdown_path)

    print("\n‚úì Done!")


if __name__ == '__main__':
    main()
</file>

<file path="scripts/generate_scene_art.py">
#!/usr/bin/env python3
"""
Generate scene artwork from D&D module XML.

This script:
1. Extracts chapter environmental context using Gemini
2. Identifies physical location scenes in the XML
3. Generates AI artwork for each scene using Gemini Imagen
4. Creates a FoundryVTT journal page with scene gallery
5. Saves images and gallery HTML to output directory

Usage:
    uv run python scripts/generate_scene_art.py --run-dir output/runs/20241023_123456
    uv run python scripts/generate_scene_art.py --xml-file output/runs/latest/documents/chapter_01.xml
"""

import os
import sys
import argparse
import logging
import json
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.logging_config import setup_logging
from src.scene_extraction import (
    extract_chapter_context,
    identify_scene_locations,
    generate_scene_image,
    save_scene_image,
    create_scene_gallery_html
)

logger = setup_logging(__name__)


def sanitize_filename(name: str) -> str:
    """Convert scene name to safe filename."""
    import re
    # Remove special characters, replace spaces with underscores
    safe_name = re.sub(r'[^\w\s-]', '', name.lower())
    safe_name = re.sub(r'[-\s]+', '_', safe_name)
    return safe_name


def process_chapter(
    xml_file: Path,
    output_dir: Path,
    style_prompt: str = None
) -> dict:
    """
    Process a single chapter XML file to generate scene artwork.

    Args:
        xml_file: Path to chapter XML file
        output_dir: Directory to save images and HTML
        style_prompt: Optional custom style prompt for image generation

    Returns:
        Dict with processing statistics
    """
    logger.info(f"Processing chapter: {xml_file.name}")

    # Read XML
    xml_content = xml_file.read_text()

    # Step 1: Extract chapter context
    logger.info("Step 1: Extracting chapter environmental context...")
    context = extract_chapter_context(xml_content)
    logger.info(f"  Environment: {context.environment_type}")

    # Step 2: Identify scenes
    logger.info("Step 2: Identifying scene locations...")
    scenes = identify_scene_locations(xml_content, context)
    logger.info(f"  Found {len(scenes)} scenes")

    if not scenes:
        logger.warning("No scenes found in chapter")
        return {"scenes_found": 0, "images_generated": 0}

    # Create images directory
    images_dir = output_dir / "images"
    images_dir.mkdir(parents=True, exist_ok=True)

    # Step 3: Generate images in parallel
    logger.info("Step 3: Generating scene artwork (in parallel)...")
    image_paths = {}
    prompts = {}

    def generate_and_save_image(scene_data):
        """Helper function to generate and save a single image."""
        i, scene = scene_data
        try:
            logger.info(f"  [{i}/{len(scenes)}] Generating image for: {scene.name}")

            # Generate image (returns tuple of bytes and prompt)
            image_bytes, prompt = generate_scene_image(scene, context, style_prompt)

            # Save image
            safe_name = sanitize_filename(scene.name)
            image_filename = f"scene_{i:03d}_{safe_name}.png"
            image_path = images_dir / image_filename
            save_scene_image(image_bytes, str(image_path))

            # Return scene name, relative path, and prompt
            return (scene.name, f"images/{image_filename}", prompt)
        except Exception as e:
            logger.error(f"Failed to generate image for '{scene.name}': {e}")
            return None

    # Use ThreadPoolExecutor for parallel image generation
    # Using 5 workers with imagen-3.0-generate-002 for faster generation
    with ThreadPoolExecutor(max_workers=5) as executor:
        # Submit all tasks
        future_to_scene = {
            executor.submit(generate_and_save_image, (i, scene)): scene
            for i, scene in enumerate(scenes, start=1)
        }

        # Collect results as they complete
        for future in as_completed(future_to_scene):
            result = future.result()
            if result:
                scene_name, image_path, prompt = result
                image_paths[scene_name] = image_path
                prompts[scene_name] = prompt

    # Step 4: Create gallery HTML
    logger.info("Step 4: Creating scene gallery HTML...")
    gallery_html = create_scene_gallery_html(scenes, image_paths, prompts)

    # Save gallery HTML
    gallery_file = output_dir / "scene_gallery.html"
    gallery_file.write_text(gallery_html)
    logger.info(f"  Saved gallery to: {gallery_file}")

    # Step 5: Save scene metadata
    logger.info("Step 5: Saving scene metadata...")
    scenes_metadata = []
    for i, scene in enumerate(scenes, start=1):
        safe_name = sanitize_filename(scene.name)
        image_filename = f"scene_{i:03d}_{safe_name}.png"

        scenes_metadata.append({
            "section_path": scene.section_path,
            "name": scene.name,
            "description": scene.description,
            "location_type": scene.location_type,
            "image_file": f"images/{image_filename}"
        })

    metadata_file = output_dir / "scenes_metadata.json"
    metadata_content = {
        "generated_at": datetime.now().isoformat(),
        "total_scenes": len(scenes),
        "scenes": scenes_metadata
    }

    with open(metadata_file, "w") as f:
        json.dump(metadata_content, f, indent=2)

    logger.info(f"  Saved metadata to: {metadata_file}")

    return {
        "scenes_found": len(scenes),
        "images_generated": len(image_paths),
        "gallery_file": str(gallery_file),
        "metadata_file": str(metadata_file)
    }


def main():
    """Main entry point."""
    load_dotenv()

    parser = argparse.ArgumentParser(description="Generate scene artwork from D&D module XML")
    parser.add_argument("--run-dir", help="Run directory containing documents/ folder")
    parser.add_argument("--xml-file", help="Single XML file to process")
    parser.add_argument("--style", help="Custom style prompt for image generation")
    parser.add_argument("--output-dir", help="Custom output directory (default: <run-dir>/scene_artwork)")

    args = parser.parse_args()

    if not args.run_dir and not args.xml_file:
        parser.error("Must specify either --run-dir or --xml-file")

    # Determine XML files to process
    if args.xml_file:
        xml_files = [Path(args.xml_file)]
        output_dir = Path(args.output_dir) if args.output_dir else Path(args.xml_file).parent / "scene_artwork"
    else:
        run_dir = Path(args.run_dir)
        xml_files = list((run_dir / "documents").glob("*.xml"))
        output_dir = Path(args.output_dir) if args.output_dir else run_dir / "scene_artwork"

    if not xml_files:
        logger.error("No XML files found to process")
        return 1

    logger.info(f"Found {len(xml_files)} XML files to process")
    output_dir.mkdir(parents=True, exist_ok=True)

    # Process each chapter
    total_scenes = 0
    total_images = 0

    for xml_file in xml_files:
        try:
            stats = process_chapter(xml_file, output_dir, args.style)
            total_scenes += stats["scenes_found"]
            total_images += stats["images_generated"]
        except Exception as e:
            logger.error(f"Failed to process {xml_file.name}: {e}")
            continue

    # Summary
    logger.info("=" * 60)
    logger.info("Scene artwork generation complete!")
    logger.info(f"  Total scenes identified: {total_scenes}")
    logger.info(f"  Total images generated: {total_images}")
    logger.info(f"  Output directory: {output_dir}")
    logger.info("=" * 60)

    return 0


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="src/actors/extract_stat_blocks.py">
"""Extract stat blocks from generated XML files."""

import logging
from typing import List
from concurrent.futures import ThreadPoolExecutor, as_completed
from util.gemini import GeminiAPI
from .models import StatBlock
from .parse_stat_blocks import (
    parse_stat_block_with_gemini,
    extract_stat_blocks_from_xml_file
)

logger = logging.getLogger(__name__)

# Max parallel Gemini API calls
MAX_WORKERS = 5


def extract_and_parse_stat_blocks(
    xml_file: str,
    api: GeminiAPI = None
) -> List[StatBlock]:
    """
    Extract stat blocks from XML and parse into structured data.

    Uses XMLDocument model for parsing.

    Args:
        xml_file: Path to XML file
        api: Optional GeminiAPI instance (creates new one if not provided)

    Returns:
        List of parsed StatBlock objects

    Raises:
        FileNotFoundError: If XML file doesn't exist
        ValueError: If parsing fails
    """
    if api is None:
        api = GeminiAPI()

    # Extract raw stat blocks using XMLDocument
    raw_stat_blocks = extract_stat_blocks_from_xml_file(xml_file)

    if not raw_stat_blocks:
        logger.info(f"No stat blocks found in {xml_file}")
        return []

    logger.info(f"Parsing {len(raw_stat_blocks)} stat blocks in parallel (max {MAX_WORKERS} workers)...")

    # Parse each stat block in parallel using ThreadPoolExecutor
    parsed_stat_blocks = []

    def parse_single_stat_block(raw_block):
        """Parse a single stat block and return result or None."""
        try:
            return parse_stat_block_with_gemini(raw_block["raw_text"], api=api)
        except Exception as e:
            logger.error(f"Failed to parse stat block '{raw_block['name']}': {e}")
            return None

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit all parsing tasks
        future_to_block = {
            executor.submit(parse_single_stat_block, raw_block): raw_block
            for raw_block in raw_stat_blocks
        }

        # Collect results as they complete
        for future in as_completed(future_to_block):
            result = future.result()
            if result is not None:
                parsed_stat_blocks.append(result)

    logger.info(f"Successfully parsed {len(parsed_stat_blocks)}/{len(raw_stat_blocks)} stat blocks")
    return parsed_stat_blocks
</file>

<file path="src/actors/parse_stat_blocks.py">
"""Parse D&D 5e stat blocks using Gemini."""

import logging
import json
from typing import Optional, List, Dict
from pathlib import Path
from util.gemini import GeminiAPI
from .models import StatBlock

logger = logging.getLogger(__name__)


def parse_stat_block_with_gemini(raw_text: str, api: Optional[GeminiAPI] = None) -> StatBlock:
    """
    Parse a D&D 5e stat block using Gemini.

    Args:
        raw_text: Raw stat block text
        api: Optional GeminiAPI instance (creates new one if not provided)

    Returns:
        Parsed StatBlock object

    Raises:
        ValueError: If parsing fails or result is invalid
        RuntimeError: If Gemini API call fails
    """
    if api is None:
        api = GeminiAPI()

    logger.debug(f"Parsing stat block (length: {len(raw_text)} chars)")

    # Construct parsing prompt
    prompt = f"""Parse this D&D 5e stat block into structured JSON.

Extract the following fields:
- name (string): Creature name
- armor_class (integer): AC value only (not the parenthetical armor type)
- hit_points (integer): HP value only (not the dice formula)
- challenge_rating (float): CR as decimal (1/4 = 0.25, 1/2 = 0.5)
- size (string, optional): Creature size
- type (string, optional): Creature type
- alignment (string, optional): Alignment
- abilities (object, optional): {{STR: int, DEX: int, CON: int, INT: int, WIS: int, CHA: int}}
- speed (string, optional): Speed description
- senses (string, optional): Senses description
- languages (string, optional): Languages
- traits (array of strings, optional): Each special trait as a separate string in array. If no traits, use empty array [].
- actions (array of strings, optional): Each action as a separate string in array. If no actions, use empty array [].

Return ONLY valid JSON with these exact field names. Do not include any markdown formatting or explanation.
If a field has no value, use empty array [] for list fields or null for other optional fields.

Stat block:
{raw_text}"""

    try:
        # Call Gemini
        response = api.generate_content(prompt)
        response_text = response.text.strip()

        # Remove markdown code blocks if present
        if response_text.startswith("```"):
            lines = response_text.split("\n")
            # Remove first and last lines (``` markers)
            response_text = "\n".join(lines[1:-1])
            # Remove language identifier if present
            if response_text.startswith("json"):
                response_text = response_text[4:].strip()

        # Parse JSON
        parsed_data = json.loads(response_text)

        # Add raw_text to parsed data
        parsed_data["raw_text"] = raw_text

        # Create and validate StatBlock
        stat_block = StatBlock(**parsed_data)

        logger.info(f"Successfully parsed stat block: {stat_block.name} (CR {stat_block.challenge_rating})")
        return stat_block

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse Gemini response as JSON: {e}")
        logger.debug(f"Response text: {response_text}")
        raise ValueError(f"Invalid JSON from Gemini: {e}") from e

    except Exception as e:
        logger.error(f"Failed to parse stat block: {e}")
        raise RuntimeError(f"Stat block parsing failed: {e}") from e


def extract_stat_blocks_from_document(doc: 'XMLDocument') -> List[Dict[str, str]]:
    """
    Extract raw stat block text from XMLDocument.

    Finds all stat_block content elements across all pages.

    Args:
        doc: XMLDocument instance to extract from

    Returns:
        List of dicts with 'name' and 'raw_text' keys

    Example:
        >>> from models import XMLDocument
        >>> doc = XMLDocument.from_xml(xml_string)
        >>> stat_blocks = extract_stat_blocks_from_document(doc)
        >>> for block in stat_blocks:
        ...     print(f"{block['name']}: {len(block['raw_text'])} chars")
    """
    from models.xml_document import StatBlockRaw

    stat_blocks = []

    # Iterate through all pages and content
    for page in doc.pages:
        for content in page.content:
            # Check if this is a stat_block content type
            if content.type == "stat_block":
                # Extract StatBlockRaw data
                stat_block_raw = content.data
                if isinstance(stat_block_raw, StatBlockRaw):
                    # Parse the XML element to extract text content
                    import xml.etree.ElementTree as ET
                    elem = ET.fromstring(stat_block_raw.xml_element)
                    raw_text = elem.text.strip() if elem.text else ""

                    if not raw_text:
                        logger.warning(f"Stat block '{stat_block_raw.name}' has no text content, skipping")
                        continue

                    stat_blocks.append({
                        "name": stat_block_raw.name,
                        "raw_text": raw_text
                    })

    logger.info(f"Extracted {len(stat_blocks)} stat block(s) from XMLDocument")
    return stat_blocks


def extract_stat_blocks_from_xml_file(xml_file: str) -> List[Dict[str, str]]:
    """
    Extract raw stat block text from XML file using XMLDocument.

    Convenience wrapper that loads XML file, parses to XMLDocument,
    and extracts stat blocks.

    Args:
        xml_file: Path to XML file

    Returns:
        List of dicts with 'name' and 'raw_text' keys

    Raises:
        FileNotFoundError: If XML file doesn't exist
        xml.etree.ElementTree.ParseError: If XML is malformed

    Example:
        >>> stat_blocks = extract_stat_blocks_from_xml_file("chapter_1.xml")
        >>> for block in stat_blocks:
        ...     print(block['name'])
    """
    from models import XMLDocument

    xml_path = Path(xml_file)
    if not xml_path.exists():
        raise FileNotFoundError(f"XML file not found: {xml_file}")

    logger.debug(f"Extracting stat blocks from: {xml_file}")

    # Load and parse XML to XMLDocument
    with open(xml_path, 'r') as f:
        xml_string = f.read()
    doc = XMLDocument.from_xml(xml_string)

    # Extract stat blocks from document
    return extract_stat_blocks_from_document(doc)
</file>

<file path="src/foundry/actors/manager.py">
"""FoundryVTT Actor operations."""

import logging
import requests
from typing import Optional, Dict, Any, List

logger = logging.getLogger(__name__)


def _is_running_in_tests() -> bool:
    """Check if running in pytest."""
    import sys
    return "pytest" in sys.modules


class ActorManager:
    """Manages actor operations for FoundryVTT."""

    def __init__(
        self,
        relay_url: str,
        foundry_url: str,
        api_key: str,
        client_id: str,
        folder_manager: Optional[Any] = None
    ):
        """
        Initialize actor manager.

        Args:
            relay_url: URL of the relay server
            foundry_url: URL of the FoundryVTT instance
            api_key: API key for authentication
            client_id: Client ID for the FoundryVTT instance
            folder_manager: Optional FolderManager instance for organizing actors
        """
        self.relay_url = relay_url
        self.foundry_url = foundry_url
        self.api_key = api_key
        self.client_id = client_id
        self.folder_manager = folder_manager

    def search_all_compendiums(self, name: str) -> Optional[str]:
        """
        Search all user compendiums for actor by name.

        Args:
            name: Actor name to search for

        Returns:
            Actor UUID if found, None otherwise
        """
        url = f"{self.relay_url}/search"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        # Use filter parameter (not type) for Actor filtering
        params = {
            "clientId": self.client_id,
            "filter": "Actor",
            "query": name
        }

        logger.debug(f"Searching for actor: {name}")

        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.warning(f"Actor search failed: {response.status_code}")
                return None

            results = response.json()

            # Handle empty results
            if not results or (isinstance(results, dict) and results.get("error")):
                logger.debug(f"No actor found with name: {name}")
                return None

            # Handle both list and dict response formats
            search_results = results if isinstance(results, list) else results.get("results", [])

            # Find exact name match
            for actor in search_results:
                if actor.get("name") == name:
                    uuid = actor.get("uuid")
                    logger.debug(f"Found actor: {name} (UUID: {uuid})")
                    return uuid

            logger.debug(f"No exact match found for actor: {name}")
            return None

        except Exception as e:
            logger.warning(f"Actor search request failed: {e}")
            return None

    def create_creature_actor(self, stat_block) -> str:
        """
        Create a creature Actor from a stat block.

        Args:
            stat_block: Parsed StatBlock object

        Returns:
            Actor UUID

        Raises:
            RuntimeError: If creation fails
        """
        url = f"{self.relay_url}/create?clientId={self.client_id}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        # Map stat block to D&D 5e Actor structure
        actor_data = {
            "name": stat_block.name,
            "type": "npc",
            "system": {
                "attributes": {
                    "ac": {"value": stat_block.armor_class},
                    "hp": {
                        "value": stat_block.hit_points,
                        "max": stat_block.hit_points
                    }
                },
                "details": {
                    "cr": stat_block.challenge_rating,
                    "type": {
                        "value": stat_block.type or "",
                        "subtype": ""
                    },
                    "alignment": stat_block.alignment or "",
                    "biography": {
                        "value": f"<pre>{stat_block.raw_text}</pre>"
                    }
                }
            }
        }

        # Add abilities if present
        if stat_block.abilities:
            actor_data["system"]["abilities"] = {
                ability.lower(): {"value": value}
                for ability, value in stat_block.abilities.items()
            }

        payload = {
            "entityType": "Actor",
            "data": actor_data
        }

        logger.debug(f"Creating creature actor: {stat_block.name}")

        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Failed to create actor: {response.status_code} - {response.text}")
                raise RuntimeError(
                    f"Failed to create actor: {response.status_code} - {response.text}"
                )

            result = response.json()
            uuid = result.get("uuid")
            logger.info(f"Created creature actor: {stat_block.name} (UUID: {uuid})")
            return uuid

        except requests.exceptions.RequestException as e:
            logger.error(f"Actor creation request failed: {e}")
            raise RuntimeError(f"Failed to create actor: {e}") from e

    def create_npc_actor(self, npc, stat_block_uuid: Optional[str] = None) -> str:
        """
        Create an NPC Actor with biography and optional stat block link.

        NPCs are bio-only Actors with no stats. If stat_block_uuid provided,
        biography includes @UUID link to the creature's stat block.

        Args:
            npc: NPC object with description and plot info
            stat_block_uuid: Optional UUID of creature stat block actor

        Returns:
            Actor UUID

        Raises:
            RuntimeError: If creation fails
        """
        url = f"{self.relay_url}/create?clientId={self.client_id}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        # Build biography HTML
        bio_parts = [
            f"<h2>{npc.name}</h2>",
            f"<p><strong>Description:</strong> {npc.description}</p>",
            f"<p><strong>Plot Role:</strong> {npc.plot_relevance}</p>"
        ]

        if npc.location:
            bio_parts.append(f"<p><strong>Location:</strong> {npc.location}</p>")

        if stat_block_uuid:
            bio_parts.append(
                f'<p><strong>Creature Stats:</strong> '
                f'@UUID[{stat_block_uuid}]{{View {npc.creature_stat_block_name} stats}}</p>'
            )

        bio_html = "\n".join(bio_parts)

        # Create bio-only NPC actor (no stats)
        actor_data = {
            "name": npc.name,
            "type": "npc",
            "system": {
                "details": {
                    "biography": {
                        "value": bio_html
                    }
                }
            }
        }

        payload = {
            "entityType": "Actor",
            "data": actor_data
        }

        logger.debug(f"Creating NPC actor: {npc.name}")

        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Failed to create NPC: {response.status_code} - {response.text}")
                raise RuntimeError(
                    f"Failed to create NPC: {response.status_code} - {response.text}"
                )

            result = response.json()
            uuid = result.get("uuid")
            logger.info(f"Created NPC actor: {npc.name} (UUID: {uuid})")
            return uuid

        except requests.exceptions.RequestException as e:
            logger.error(f"NPC creation request failed: {e}")
            raise RuntimeError(f"Failed to create NPC: {e}") from e

    def create_actor(self, actor_data: Dict[str, Any], spell_uuids: list[str] = None) -> str:
        """
        Create an Actor from pre-built FoundryVTT JSON format.

        This method accepts a complete FoundryVTT actor JSON structure
        (as produced by convert_to_foundry) and uploads it to FoundryVTT.
        Optionally adds compendium spells via /give endpoint after creation.

        When running in pytest, automatically creates actors in a "tests" folder.

        Args:
            actor_data: Complete FoundryVTT actor JSON with 'name', 'type',
                       'system', 'items', etc.
            spell_uuids: Optional list of compendium spell UUIDs to add via /give
                        (recommended for full spell data)

        Returns:
            Actor UUID

        Raises:
            RuntimeError: If creation fails
        """
        # Auto-organize test actors into "tests" folder
        folder_id = None
        if _is_running_in_tests() and self.folder_manager:
            try:
                folder_id = self.folder_manager.get_or_create_folder("tests", "Actor")
                logger.debug("Adding actor to 'tests' folder (running in pytest)")
            except Exception as e:
                logger.warning(f"Failed to set test folder: {e}")

        url = f"{self.relay_url}/create?clientId={self.client_id}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        payload = {
            "entityType": "Actor",
            "data": actor_data
        }

        # Add folder at top level of payload if needed
        if folder_id:
            payload["folder"] = folder_id

        actor_name = actor_data.get("name", "Unknown")
        logger.debug(f"Creating actor: {actor_name}")

        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Failed to create actor: {response.status_code} - {response.text}")
                raise RuntimeError(
                    f"Failed to create actor: {response.status_code} - {response.text}"
                )

            result = response.json()
            uuid = result.get("uuid")
            logger.info(f"Created actor: {actor_name} (UUID: {uuid})")

            # Add compendium spells via /give if provided
            if spell_uuids:
                logger.info(f"Adding {len(spell_uuids)} compendium spells via /give...")
                self.add_compendium_items(uuid, spell_uuids)

            return uuid

        except requests.exceptions.RequestException as e:
            logger.error(f"Actor creation request failed: {e}")
            raise RuntimeError(f"Failed to create actor: {e}") from e

    def get_actor(self, actor_uuid: str) -> Dict[str, Any]:
        """
        Retrieve an Actor by UUID.

        Args:
            actor_uuid: UUID of the actor to retrieve

        Returns:
            Complete actor data as dict

        Raises:
            RuntimeError: If retrieval fails
        """
        url = f"{self.relay_url}/get?clientId={self.client_id}&uuid={actor_uuid}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        logger.debug(f"Retrieving actor: {actor_uuid}")

        try:
            response = requests.get(url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Failed to retrieve actor: {response.status_code} - {response.text}")
                raise RuntimeError(
                    f"Failed to retrieve actor: {response.status_code} - {response.text}"
                )

            response_data = response.json()

            # Extract actor data from response envelope
            actor_data = response_data.get("data", response_data)

            actor_name = actor_data.get("name", "Unknown")
            logger.info(f"Retrieved actor: {actor_name} (UUID: {actor_uuid})")
            return actor_data

        except requests.exceptions.RequestException as e:
            logger.error(f"Actor retrieval request failed: {e}")
            raise RuntimeError(f"Failed to retrieve actor: {e}") from e

    def add_compendium_items(self, actor_uuid: str, item_uuids: list[str]) -> None:
        """
        Add compendium items to an actor using the /give endpoint.

        This method adds items from compendiums (typically spells) to an existing
        actor. Items added this way retain full compendium data including
        descriptions, activities, and damage formulas.

        Args:
            actor_uuid: UUID of the actor to add items to
            item_uuids: List of compendium item UUIDs to add

        Raises:
            RuntimeError: If any item addition fails
        """
        url = f"{self.relay_url}/give"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        failed_items = []

        for item_uuid in item_uuids:
            payload = {
                "toUuid": actor_uuid,
                "itemUuid": item_uuid,
                "selected": False
            }

            try:
                response = requests.post(
                    f"{url}?clientId={self.client_id}",
                    json=payload,
                    headers=headers,
                    timeout=30
                )

                if response.status_code != 200:
                    logger.warning(f"Failed to add item {item_uuid}: {response.status_code}")
                    failed_items.append(item_uuid)
                else:
                    logger.debug(f"Added compendium item: {item_uuid}")

            except requests.exceptions.RequestException as e:
                logger.warning(f"Failed to add item {item_uuid}: {e}")
                failed_items.append(item_uuid)

        if failed_items:
            raise RuntimeError(
                f"Failed to add {len(failed_items)}/{len(item_uuids)} compendium items"
            )

        logger.info(f"Successfully added {len(item_uuids)} compendium items to actor {actor_uuid}")

    def get_all_actors(self) -> List[Dict[str, Any]]:
        """
        Get all world actors using multiple search queries.

        Note: The search API requires a non-empty query to return world actors.
        Empty queries only return compendium actors. This method searches with
        common letters and deduplicates by UUID.

        Returns:
            List of world actor data dictionaries (Actor.* UUIDs only)

        Raises:
            RuntimeError: If request fails
        """
        url = f"{self.relay_url}/search"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        all_actors = {}  # Use dict to deduplicate by UUID

        # Search with all letters of the alphabet to find all actors
        # API has 200 result limit, so we need multiple queries for comprehensive coverage
        import string
        search_queries = list(string.ascii_lowercase)  # a-z

        try:
            for query in search_queries:
                params = {
                    "clientId": self.client_id,
                    "filter": "Actor",
                    "query": query
                }

                response = requests.get(url, params=params, headers=headers, timeout=30)

                if response.status_code != 200:
                    logger.warning(f"Search for '{query}' failed: {response.status_code}")
                    continue

                results = response.json()
                actors = results if isinstance(results, list) else results.get("results", [])

                # Filter to only world actors and deduplicate
                for actor in actors:
                    uuid = actor.get("uuid", actor.get("_id", ""))
                    if uuid.startswith("Actor."):
                        all_actors[uuid] = actor

            actor_list = list(all_actors.values())
            logger.info(f"Retrieved {len(actor_list)} unique world actors")
            return actor_list

        except requests.exceptions.RequestException as e:
            logger.error(f"Get all actors request failed: {e}")
            raise RuntimeError(f"Failed to get all actors: {e}") from e

    def delete_actor(self, actor_uuid: str) -> Dict[str, Any]:
        """
        Delete an actor.

        Args:
            actor_uuid: UUID of the actor to delete (format: Actor.{id})

        Returns:
            Response data from API

        Raises:
            RuntimeError: If deletion fails
        """
        url = f"{self.relay_url}/delete?clientId={self.client_id}&uuid={actor_uuid}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        try:
            response = requests.delete(url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Delete failed: {response.status_code} - {response.text}")
                raise RuntimeError(f"Failed to delete actor: {response.status_code} - {response.text}")

            result = response.json()
            logger.info(f"Deleted actor: {actor_uuid}")
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Delete request failed: {e}")
            raise RuntimeError(f"Failed to delete actor: {e}") from e
</file>

<file path="src/foundry/items/fetch.py">
"""
Fetch items from FoundryVTT compendiums via REST API.

Uses alphabet-based querying to work around the 200-result search limit.
"""

import os
import logging
from typing import Dict, List, Optional
from string import ascii_lowercase
import requests

logger = logging.getLogger(__name__)


def fetch_items_by_type(
    item_subtype: str,
    relay_url: str = None,
    api_key: str = None,
    client_id: str = None,
    use_two_letter_fallback: bool = True
) -> List[Dict]:
    """
    Fetch all items of a specific subtype from FoundryVTT.

    Uses alphabet strategy (a-z) to bypass 200-result search limit.
    For queries that hit the limit, optionally uses two-letter combinations.

    Args:
        item_subtype: Item subtype to fetch (e.g., "spell", "weapon", "equipment")
        relay_url: Relay server URL (defaults to FOUNDRY_RELAY_URL env var)
        api_key: API key (defaults to FOUNDRY_API_KEY env var)
        client_id: Client ID (defaults to FOUNDRY_CLIENT_ID env var)
        use_two_letter_fallback: Use two-letter combos for queries that hit 200 limit

    Returns:
        List of item dicts with name, uuid, and other metadata

    Raises:
        ValueError: If required credentials are missing
        RuntimeError: If API request fails
    """
    # Use environment variables if not provided
    relay_url = relay_url or os.getenv("FOUNDRY_RELAY_URL")
    api_key = api_key or os.getenv("FOUNDRY_API_KEY")
    client_id = client_id or os.getenv("FOUNDRY_CLIENT_ID")

    if not all([relay_url, api_key, client_id]):
        raise ValueError(
            "Missing required credentials: FOUNDRY_RELAY_URL, "
            "FOUNDRY_API_KEY, FOUNDRY_CLIENT_ID"
        )

    logger.info(f"Fetching all items of subtype '{item_subtype}'...")

    all_items = {}  # Deduplicate by UUID
    letters_at_limit = []

    def search_query(query: str) -> List[Dict]:
        """Execute a search query and return results."""
        url = f"{relay_url}/search"
        headers = {"x-api-key": api_key}
        params = {
            "clientId": client_id,
            "filter": f"documentType:Item,subType:{item_subtype}",
            "query": query
        }

        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            return data.get('results', data) if isinstance(data, dict) else data
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to search for '{query}': {e}")
            raise RuntimeError(f"Failed to search: {e}") from e

    # Query with each letter
    for letter in ascii_lowercase:
        results = search_query(letter)

        # Deduplicate by UUID
        for item in results:
            uuid = item.get('uuid')
            if uuid:
                all_items[uuid] = item

        logger.debug(f"Letter '{letter}': {len(results)} results (total unique: {len(all_items)})")

        # Track letters that hit the 200 limit
        if len(results) == 200 and use_two_letter_fallback:
            letters_at_limit.append(letter)

    # For letters that hit 200, query with two-letter combinations
    if letters_at_limit:
        logger.info(f"‚ö† Letters at 200 limit: {', '.join(letters_at_limit)}")
        logger.info("Querying with two-letter combinations...")

        for letter in letters_at_limit:
            for second in ascii_lowercase:
                query = f"{letter}{second}"
                results = search_query(query)

                for item in results:
                    uuid = item.get('uuid')
                    if uuid:
                        all_items[uuid] = item

                if results:
                    logger.debug(f"  '{query}': {len(results)} results (total unique: {len(all_items)})")

                # Warn if even two-letter combo hits 200
                if len(results) == 200:
                    logger.warning(f"‚ö† '{query}' hit 200 limit - may need 3-letter combos!")

    # Also try empty query to catch items that don't match letters
    results = search_query("")
    for item in results:
        uuid = item.get('uuid')
        if uuid:
            all_items[uuid] = item

    logger.debug(f"Empty query: {len(results)} results (total unique: {len(all_items)})")

    items_list = list(all_items.values())
    items_sorted = sorted(items_list, key=lambda i: i.get('name', ''))

    logger.info(f"‚úì Fetched {len(items_sorted)} unique items of subtype '{item_subtype}'")

    return items_sorted


def fetch_all_spells(**kwargs) -> List[Dict]:
    """
    Convenience function to fetch all spells.

    Args:
        **kwargs: Passed to fetch_items_by_type()

    Returns:
        List of spell dicts
    """
    return fetch_items_by_type('spell', **kwargs)
</file>

<file path="src/foundry/client.py">
"""FoundryVTT REST API client."""

import os
import logging
import requests
from typing import Dict, Any, Optional, List

from .journals import JournalManager
from .items.manager import ItemManager
from .actors import ActorManager
from .icon_cache import IconCache

logger = logging.getLogger(__name__)


class FoundryClient:
    """Client for interacting with FoundryVTT via REST API."""

    def __init__(self):
        """
        Initialize FoundryVTT API client.

        Raises:
            ValueError: If required environment variables are not set
        """
        self.relay_url = os.getenv("FOUNDRY_RELAY_URL")
        self.foundry_url = os.getenv("FOUNDRY_URL")
        self.api_key = os.getenv("FOUNDRY_API_KEY")
        self.client_id = os.getenv("FOUNDRY_CLIENT_ID")

        if not self.relay_url:
            raise ValueError("FOUNDRY_RELAY_URL not set in environment")
        if not self.foundry_url:
            raise ValueError("FOUNDRY_URL not set in environment")
        if not self.api_key:
            raise ValueError("FOUNDRY_API_KEY not set in environment")
        if not self.client_id:
            raise ValueError("FOUNDRY_CLIENT_ID not set in environment")

        # Initialize managers
        self.journals = JournalManager(
            relay_url=self.relay_url,
            foundry_url=self.foundry_url,
            api_key=self.api_key,
            client_id=self.client_id
        )

        self.items = ItemManager(
            relay_url=self.relay_url,
            foundry_url=self.foundry_url,
            api_key=self.api_key,
            client_id=self.client_id
        )

        self.actors = ActorManager(
            relay_url=self.relay_url,
            foundry_url=self.foundry_url,
            api_key=self.api_key,
            client_id=self.client_id
        )

        self.icons = IconCache()

        logger.info(f"Initialized FoundryClient at {self.foundry_url}")

    # Journal operations (delegated to JournalManager)

    def create_journal_entry(
        self,
        name: str,
        pages: list = None,
        content: str = None,
        folder: str = None
    ) -> Dict[str, Any]:
        """Create a new journal entry in FoundryVTT."""
        return self.journals.create_journal_entry(name, pages, content, folder)

    def get_all_journals_by_name(self, name: str) -> list[Dict[str, Any]]:
        """Get all journals matching the given name."""
        return self.journals.get_all_journals_by_name(name)

    def get_journal_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        """Get first journal matching the given name."""
        return self.journals.get_journal_by_name(name)

    def get_journal(self, journal_uuid: str) -> Dict[str, Any]:
        """Get a journal by UUID."""
        return self.journals.get_journal(journal_uuid)

    def update_journal_entry(
        self,
        journal_uuid: str,
        pages: list = None,
        content: str = None,
        name: str = None
    ) -> Dict[str, Any]:
        """Update an existing journal entry."""
        return self.journals.update_journal_entry(journal_uuid, pages, content, name)

    def delete_journal_entry(self, journal_uuid: str) -> Dict[str, Any]:
        """Delete a journal entry."""
        return self.journals.delete_journal_entry(journal_uuid)

    def create_or_replace_journal(
        self,
        name: str,
        pages: list = None,
        content: str = None,
        folder: str = None
    ) -> Dict[str, Any]:
        """Create or replace a journal entry."""
        return self.journals.create_or_replace_journal(name, pages, content, folder)

    # Item operations (delegated to ItemManager)

    def get_all_items_by_name(self, name: str) -> list[Dict[str, Any]]:
        """Get all items matching the given name."""
        return self.items.get_all_items_by_name(name)

    def get_item_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        """Get first item matching the given name."""
        return self.items.get_item_by_name(name)

    def get_item(self, item_uuid: str) -> Dict[str, Any]:
        """Get an item by UUID."""
        return self.items.get_item(item_uuid)

    # File operations

    def upload_file(self, local_path: str, target_path: str, overwrite: bool = True) -> Dict[str, Any]:
        """
        Upload a file to FoundryVTT.

        Args:
            local_path: Path to local file
            target_path: Target path in FoundryVTT (e.g., "worlds/my-world/assets/image.png")
            overwrite: Whether to overwrite existing files (default: True)

        Returns:
            Upload response dict

        Raises:
            RuntimeError: If upload fails
        """
        from pathlib import Path
        import mimetypes

        endpoint = f"{self.relay_url}/upload"

        # Split target_path into directory path and filename
        path_obj = Path(target_path)
        directory = str(path_obj.parent)
        filename = path_obj.name

        # Detect MIME type
        mime_type, _ = mimetypes.guess_type(filename)
        if not mime_type:
            mime_type = "application/octet-stream"

        # API expects x-api-key header, binary data in body, and separate path/filename parameters
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/octet-stream"
        }

        params = {
            "clientId": self.client_id,
            "path": directory,
            "filename": filename,
            "mimeType": mime_type,
            "overwrite": str(overwrite).lower()
        }

        logger.debug(f"Uploading {local_path} ‚Üí {target_path}")

        # Read file as binary data for request body
        try:
            with open(local_path, 'rb') as f:
                file_data = f.read()
        except IOError as e:
            logger.error(f"Failed to read local file '{local_path}': {e}")
            raise RuntimeError(f"Failed to read file: {e}") from e

        try:
            response = requests.post(endpoint, headers=headers, params=params, data=file_data, timeout=60)
            response.raise_for_status()
            result = response.json()
            logger.debug(f"Upload successful: {filename}")
            return result
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to upload file '{filename}': {e}")
            raise RuntimeError(f"Failed to upload file: {e}") from e

    def download_file(self, target_path: str, local_path: str) -> None:
        """
        Download a file from FoundryVTT.

        Args:
            target_path: Full path to file in FoundryVTT (e.g., "worlds/my-world/assets/image.png")
            local_path: Local path to save downloaded file

        Raises:
            RuntimeError: If download fails
        """
        from pathlib import Path

        endpoint = f"{self.relay_url}/download"

        headers = {
            "x-api-key": self.api_key
        }

        # Download endpoint expects full file path in "path" parameter (no separate filename)
        params = {
            "clientId": self.client_id,
            "path": target_path
        }

        logger.debug(f"Downloading {target_path} ‚Üí {local_path}")

        try:
            response = requests.get(endpoint, headers=headers, params=params, timeout=60)
            response.raise_for_status()

            # Write binary response to file
            with open(local_path, 'wb') as f:
                f.write(response.content)

            logger.debug(f"Download successful: {target_path}")

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to download file '{target_path}': {e}")
            raise RuntimeError(f"Failed to download file: {e}") from e
        except IOError as e:
            logger.error(f"Failed to write to local file '{local_path}': {e}")
            raise RuntimeError(f"Failed to write file: {e}") from e

    def is_world_active(self) -> bool:
        """
        Check if any world is currently running in FoundryVTT.

        This works regardless of whether the world was launched via browser
        or headless API session. Makes a lightweight search API call to verify
        the server can respond.

        Returns:
            True if a world is active and responding, False otherwise
        """
        endpoint = f"{self.relay_url}/search"

        headers = {
            "x-api-key": self.api_key
        }

        params = {
            "clientId": self.client_id,
            "query": "",
            "filter": "Item"
        }

        try:
            response = requests.get(endpoint, headers=headers, params=params, timeout=5)
            response.raise_for_status()
            # If we get a successful response, a world is active
            return True
        except requests.exceptions.RequestException:
            # Any error means no world is active or server isn't responding
            return False

    def get_active_sessions(self) -> List[Dict[str, Any]]:
        """
        Get currently active headless FoundryVTT sessions.

        Note: This only returns headless API sessions, not browser sessions.
        Use is_world_active() to check if any world is running.

        Returns:
            List of active session dictionaries with keys:
                - id: Session ID
                - clientId: Client ID
                - lastActivity: Timestamp of last activity
                - idleMinutes: Minutes since last activity

        Raises:
            RuntimeError: If API call fails
        """
        endpoint = f"{self.relay_url}/session"

        headers = {
            "x-api-key": self.api_key
        }

        try:
            response = requests.get(endpoint, headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            return data.get('activeSessions', [])
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to get active sessions: {e}")
            raise RuntimeError(f"Failed to get active sessions: {e}") from e

    # Actor operations (delegated to ActorManager)

    def search_actor(self, name: str) -> Optional[str]:
        """Search for actor by name in all compendiums."""
        return self.actors.search_all_compendiums(name)

    def create_creature_actor(self, stat_block) -> str:
        """Create creature actor from stat block."""
        return self.actors.create_creature_actor(stat_block)

    def create_npc_actor(self, npc, stat_block_uuid: Optional[str] = None) -> str:
        """Create NPC actor with optional stat block link."""
        return self.actors.create_npc_actor(npc, stat_block_uuid)
</file>

<file path="src/foundry/folders.py">
"""FoundryVTT Folder operations."""

import logging
import requests
import sys
from typing import Dict, Any, Optional, Literal

logger = logging.getLogger(__name__)


class FolderManager:
    """Manages folder operations for FoundryVTT."""

    def __init__(self, relay_url: str, foundry_url: str, api_key: str, client_id: str):
        """
        Initialize folder manager.

        Args:
            relay_url: URL of the relay server
            foundry_url: URL of the FoundryVTT instance
            api_key: API key for authentication
            client_id: Client ID for the FoundryVTT instance
        """
        self.relay_url = relay_url
        self.foundry_url = foundry_url
        self.api_key = api_key
        self.client_id = client_id
        self._folder_cache: Dict[tuple[str, str], str] = {}  # (type, name) -> folder_id

    def get_or_create_folder(
        self,
        name: str,
        folder_type: Literal["Actor", "JournalEntry", "Item", "Scene"]
    ) -> str:
        """
        Get existing folder by name or create it if it doesn't exist.

        Args:
            name: Folder name
            folder_type: Type of entities this folder will contain
                        ("Actor", "JournalEntry", "Item", "Scene")

        Returns:
            Folder ID

        Raises:
            RuntimeError: If folder operations fail
        """
        # Check cache first
        cache_key = (folder_type, name)
        if cache_key in self._folder_cache:
            logger.debug(f"Using cached folder ID for '{name}' ({folder_type})")
            return self._folder_cache[cache_key]

        # Search for existing folder
        folder_id = self.search_folder(name, folder_type)
        if folder_id:
            logger.info(f"Found existing folder: '{name}' ({folder_type}) - {folder_id}")
            self._folder_cache[cache_key] = folder_id
            return folder_id

        # Create new folder
        folder_id = self.create_folder(name, folder_type)
        logger.info(f"Created new folder: '{name}' ({folder_type}) - {folder_id}")
        self._folder_cache[cache_key] = folder_id
        return folder_id

    def search_folder(
        self,
        name: str,
        folder_type: Literal["Actor", "JournalEntry", "Item", "Scene"]
    ) -> Optional[str]:
        """
        Get a folder by name and type using the /get-folder endpoint.

        Args:
            name: Folder name to search for
            folder_type: Type of entities the folder contains

        Returns:
            Folder ID if found, None otherwise
        """
        url = f"{self.relay_url}/get-folder"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        params = {
            "clientId": self.client_id,
            "name": name
        }

        logger.debug(f"Getting folder: {name} ({folder_type})")

        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)

            if response.status_code == 404:
                logger.debug(f"Folder not found: {name}")
                return None

            if response.status_code != 200:
                logger.warning(f"Get folder failed: {response.status_code} - {response.text}")
                return None

            result = response.json()
            data = result.get("data", {})

            # Check if the folder type matches
            if data.get("type") == folder_type:
                folder_id = data.get("id")
                logger.debug(f"Found folder: {name} ({folder_type}) - {folder_id}")
                return folder_id
            else:
                logger.debug(f"Folder found but type mismatch: expected {folder_type}, got {data.get('type')}")
                return None

        except Exception as e:
            logger.warning(f"Get folder failed: {e}")
            return None

    def create_folder(
        self,
        name: str,
        folder_type: Literal["Actor", "JournalEntry", "Item", "Scene"]
    ) -> str:
        """
        Create a new folder using the /create-folder endpoint.

        Args:
            name: Folder name
            folder_type: Type of entities this folder will contain

        Returns:
            Folder ID

        Raises:
            RuntimeError: If creation fails
        """
        url = f"{self.relay_url}/create-folder"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        payload = {
            "clientId": self.client_id,
            "name": name,
            "folderType": folder_type
        }

        logger.debug(f"Creating folder: {name} ({folder_type})")

        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Folder creation failed: {response.status_code} - {response.text}")
                raise RuntimeError(
                    f"Failed to create folder: {response.status_code} - {response.text}"
                )

            result = response.json()
            folder_id = result.get("data", {}).get("id")

            if not folder_id:
                logger.error(f"No folder ID in response: {result}")
                raise RuntimeError("Failed to get folder ID from response")

            logger.info(f"Created folder: {name} ({folder_type}) - {folder_id}")
            return folder_id

        except requests.exceptions.RequestException as e:
            logger.error(f"Folder creation request failed: {e}")
            raise RuntimeError(f"Failed to create folder: {e}") from e


def is_running_in_tests() -> bool:
    """
    Detect if code is running in pytest test environment.

    Returns:
        True if running in pytest, False otherwise
    """
    return "pytest" in sys.modules


def get_test_folder_name() -> str:
    """
    Get the name for the test folder.

    Returns:
        "tests" folder name
    """
    return "tests"
</file>

<file path="src/foundry/journals.py">
"""FoundryVTT Journal operations."""

import logging
import requests
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


def _is_running_in_tests() -> bool:
    """Check if running in pytest."""
    import sys
    return "pytest" in sys.modules


class JournalManager:
    """Manages journal entry operations for FoundryVTT."""

    def __init__(
        self,
        relay_url: str,
        foundry_url: str,
        api_key: str,
        client_id: str,
        folder_manager: Optional[Any] = None
    ):
        """
        Initialize journal manager.

        Args:
            relay_url: URL of the relay server
            foundry_url: URL of the FoundryVTT instance
            api_key: API key for authentication
            client_id: Client ID for the FoundryVTT instance
            folder_manager: Optional FolderManager instance for organizing journals
        """
        self.relay_url = relay_url
        self.foundry_url = foundry_url
        self.api_key = api_key
        self.client_id = client_id
        self.folder_manager = folder_manager

    def create_journal_entry(
        self,
        name: str,
        pages: list = None,
        content: str = None,
        folder: str = None
    ) -> Dict[str, Any]:
        """
        Create a new journal entry in FoundryVTT.

        Args:
            name: Name of the journal entry
            pages: List of page dicts with 'name' and 'content' keys (preferred)
            content: HTML content for single-page journal (legacy, use pages instead)
            folder: Optional folder ID to organize the journal

        Returns:
            Dict containing created journal entry data

        Raises:
            RuntimeError: If API request fails
            ValueError: If neither pages nor content provided
        """
        url = f"{self.relay_url}/create?clientId={self.client_id}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        # Build pages array
        if pages:
            # Multiple pages provided
            pages_data = [
                {
                    "name": page["name"],
                    "type": "text",
                    "text": {
                        "content": page["content"]
                    }
                }
                for page in pages
            ]
        elif content is not None:
            # Legacy single-page mode
            pages_data = [
                {
                    "name": name,
                    "type": "text",
                    "text": {
                        "content": content
                    }
                }
            ]
        else:
            raise ValueError("Must provide either 'pages' or 'content'")

        # Auto-organize test journals into "tests" folder
        if not folder and _is_running_in_tests() and self.folder_manager:
            try:
                folder = self.folder_manager.get_or_create_folder("tests", "JournalEntry")
                logger.debug("Adding journal to 'tests' folder (running in pytest)")
            except Exception as e:
                logger.warning(f"Failed to set test folder: {e}")

        payload = {
            "entityType": "JournalEntry",
            "data": {
                "name": name,
                "pages": pages_data
            }
        }

        # Add folder at top level of payload if needed
        if folder:
            payload["folder"] = folder

        page_count = len(pages_data)
        logger.debug(f"Creating journal entry: {name} with {page_count} page(s)")

        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Failed to create journal: {response.status_code} - {response.text}")
                raise RuntimeError(
                    f"Failed to create journal entry: {response.status_code} - {response.text}"
                )

            result = response.json()
            # Response format: {"entity": {"_id": "..."}, "uuid": "JournalEntry.xxx"}
            entity_id = result.get('entity', {}).get('_id') or result.get('uuid', 'unknown')
            logger.info(f"Created journal entry: {name} with {page_count} page(s) (ID: {entity_id})")
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Request failed: {e}")
            raise RuntimeError(f"Failed to create journal entry: {e}") from e

    def get_all_journals_by_name(self, name: str) -> list[Dict[str, Any]]:
        """
        Get all journals matching the given name.

        Args:
            name: Name of the journal to search for

        Returns:
            List of matching journal dicts (empty list if none found)
        """
        url = f"{self.relay_url}/search"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        params = {
            "clientId": self.client_id,
            "type": "JournalEntry",
            "query": name
        }

        logger.debug(f"Searching for journal: {name}")

        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.warning(f"Search failed: {response.status_code} - search not available")
                return []  # Search failed, return empty list

            results = response.json()

            # Check for QuickInsert error (search module not available)
            if isinstance(results, dict) and results.get("error"):
                logger.warning(f"Search error: {results['error']}")
                return []

            # Check for empty results
            if not results or (isinstance(results, dict) and not results.get("results")):
                logger.debug(f"No journals found with name: {name}")
                return []

            # Handle both list and dict response formats
            search_results = results if isinstance(results, list) else results.get("results", [])

            # Normalize all results (convert 'id' to '_id')
            normalized = []
            for journal in search_results:
                if 'id' in journal and '_id' not in journal:
                    journal['_id'] = journal['id']
                normalized.append(journal)

            logger.debug(f"Found {len(normalized)} journal(s) matching: {name}")
            return normalized

        except requests.exceptions.RequestException as e:
            logger.warning(f"Search request failed: {e}")
            return []  # Network error, return empty list

    def get_journal_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        """
        Get first journal matching the given name.

        Args:
            name: Name of the journal to find

        Returns:
            Journal dict if found, None otherwise
        """
        results = self.get_all_journals_by_name(name)

        if not results:
            logger.debug(f"No journal found with name: {name}")
            return None

        # Return first exact name match
        for journal in results:
            if journal.get("name") == name:
                logger.debug(f"Found journal: {name} (ID: {journal.get('_id') or journal.get('id')})")
                return journal

        return None

    def get_journal(self, journal_uuid: str) -> Dict[str, Any]:
        """
        Get a journal entry by UUID.

        Args:
            journal_uuid: UUID of the journal entry (format: JournalEntry.{id})

        Returns:
            Dict containing journal entry data including pages

        Raises:
            RuntimeError: If API request fails
        """
        url = f"{self.relay_url}/get?clientId={self.client_id}&uuid={journal_uuid}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        logger.debug(f"Getting journal entry: {journal_uuid}")

        try:
            response = requests.get(url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Get failed: {response.status_code} - {response.text}")
                raise RuntimeError(f"Failed to get journal: {response.status_code} - {response.text}")

            result = response.json()
            logger.debug(f"Retrieved journal entry: {journal_uuid}")
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Get request failed: {e}")
            raise RuntimeError(f"Failed to get journal: {e}") from e

    def update_journal_entry(
        self,
        journal_uuid: str,
        pages: list = None,
        content: str = None,
        name: str = None
    ) -> Dict[str, Any]:
        """
        Update an existing journal entry.

        Args:
            journal_uuid: UUID of the journal entry (format: JournalEntry.{id})
            pages: List of page dicts with 'name' and 'content' keys (preferred)
            content: New HTML content for single page (legacy, use pages instead)
            name: New name (optional)

        Returns:
            Dict containing updated journal entry data

        Raises:
            RuntimeError: If API request fails
        """
        url = f"{self.relay_url}/update?clientId={self.client_id}&uuid={journal_uuid}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        payload = {
            "data": {}
        }

        # Build pages array if content provided
        if pages:
            # Multiple pages provided
            payload["data"]["pages"] = [
                {
                    "name": page["name"],
                    "type": "text",
                    "text": {
                        "content": page["content"]
                    }
                }
                for page in pages
            ]
        elif content is not None:
            # Legacy single-page mode
            payload["data"]["pages"] = [
                {
                    "name": name or "Content",
                    "type": "text",
                    "text": {
                        "content": content
                    }
                }
            ]

        if name is not None:
            payload["data"]["name"] = name

        page_info = f" with {len(pages)} page(s)" if pages else ""
        logger.debug(f"Updating journal entry: {journal_uuid}{page_info}")

        try:
            response = requests.put(url, json=payload, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Update failed: {response.status_code} - {response.text}")
                raise RuntimeError(f"Failed to update journal: {response.status_code} - {response.text}")

            result = response.json()
            logger.info(f"Updated journal entry: {journal_uuid}{page_info}")
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Update request failed: {e}")
            raise RuntimeError(f"Failed to update journal: {e}") from e

    def delete_journal_entry(self, journal_uuid: str) -> Dict[str, Any]:
        """
        Delete a journal entry.

        Args:
            journal_uuid: UUID of the journal entry (format: JournalEntry.{id})

        Returns:
            Dict with success status

        Raises:
            RuntimeError: If API request fails
        """
        url = f"{self.relay_url}/delete?clientId={self.client_id}&uuid={journal_uuid}"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        logger.debug(f"Deleting journal entry: {journal_uuid}")

        try:
            response = requests.delete(url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"Delete failed: {response.status_code} - {response.text}")
                raise RuntimeError(f"Failed to delete journal: {response.status_code} - {response.text}")

            result = response.json()
            logger.info(f"Deleted journal entry: {journal_uuid}")
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Delete request failed: {e}")
            raise RuntimeError(f"Failed to delete journal: {e}") from e

    def create_or_replace_journal(
        self,
        name: str,
        pages: list = None,
        content: str = None,
        folder: str = None
    ) -> Dict[str, Any]:
        """
        Create or replace a journal entry.

        Searches for existing journal by name. If found, deletes it and creates a new one
        (to ensure pages are replaced, not appended).
        If not found, creates a new journal entry.

        Args:
            name: Name of the journal entry
            pages: List of page dicts with 'name' and 'content' keys (preferred)
            content: HTML content for single-page journal (legacy, use pages instead)
            folder: Optional folder ID

        Returns:
            Dict containing journal entry data

        Raises:
            ValueError: If neither pages nor content provided
        """
        if not pages and content is None:
            raise ValueError("Must provide either 'pages' or 'content'")

        # Try to find existing journal
        existing = self.get_journal_by_name(name)

        if existing:
            # Extract UUID for deletion
            # Search results may have 'uuid' field or we construct from 'id'/'_id'
            journal_uuid = existing.get('uuid')
            if not journal_uuid:
                journal_id = existing.get('_id') or existing.get('id')
                if journal_id:
                    journal_uuid = f"JournalEntry.{journal_id}"

            if journal_uuid:
                # Delete old journal to ensure clean replacement
                logger.info(f"Deleting existing journal for replacement: {name} (UUID: {journal_uuid})")
                try:
                    self.delete_journal_entry(journal_uuid)
                except RuntimeError as e:
                    logger.warning(f"Failed to delete old journal, will try creating new one: {e}")
            else:
                logger.warning(f"Found journal but no UUID available, creating new: {name}")

        # Create new journal (either fresh or replacement)
        logger.info(f"Creating journal entry: {name}")
        return self.create_journal_entry(
            name=name,
            pages=pages,
            content=content,
            folder=folder
        )
</file>

<file path="src/pdf_processing/pdf_to_html.py">
#!/usr/bin/env python3
"""Complete pipeline: PDF -> XML -> Maps -> HTML.

Orchestrates the full workflow for processing a D&D module PDF chapter into
a standalone HTML journal with positioned maps.
"""

import sys
import os
from pathlib import Path
from typing import Optional

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pdf_processing.pdf_to_xml import process_chapter
from pdf_processing.image_asset_processing.extract_map_assets import extract_maps_from_pdf
from foundry.upload_journal_to_foundry import load_and_position_images
from logging_config import setup_logging

logger = setup_logging(__name__)


def process_pdf_to_html(
    pdf_path: str,
    output_dir: Optional[Path] = None,
    map_positioning_mode: str = "semantic",
    extract_maps: bool = True,
    open_html: bool = False
) -> Path:
    """Process a PDF chapter through the complete pipeline.

    Pipeline stages:
    1. PDF to XML (Gemini extraction)
    2. Map extraction (optional)
    3. Image positioning and HTML export

    Args:
        pdf_path: Path to PDF file (relative to data/pdf_sections/Lost_Mine_of_Phandelver/)
        output_dir: Optional output directory (defaults to output/runs/<timestamp>)
        map_positioning_mode: "semantic" (use Gemini to match map names) or "page" (use page numbers)
        extract_maps: Whether to extract maps (default: True)
        open_html: Whether to open HTML in browser (default: False)

    Returns:
        Path to generated HTML file

    Example:
        >>> html_path = process_pdf_to_html("02_Part_1_Goblin_Arrows.pdf")
        >>> print(f"Generated: {html_path}")
    """
    logger.info("=== PIPELINE START ===")
    logger.info(f"PDF: {pdf_path}")
    logger.info(f"Map positioning mode: {map_positioning_mode}")

    # Step 1: PDF to XML
    logger.info("Step 1: Converting PDF to XML...")
    from datetime import datetime
    from pdf_processing.pdf_to_xml import main as pdf_to_xml_main, configure_gemini, PROJECT_ROOT

    configure_gemini()

    pdf_sections_dir = os.path.join(PROJECT_ROOT, "data", "pdf_sections", "Lost_Mine_of_Phandelver")
    runs_output_dir = os.path.join(PROJECT_ROOT, "output", "runs")

    # Generate timestamp and run directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = Path(runs_output_dir) / timestamp

    # Run PDF to XML conversion
    pdf_to_xml_main(pdf_sections_dir, runs_output_dir, single_file=pdf_path)

    logger.info(f"XML saved to: {run_dir / 'documents'}")

    # Step 2: Extract maps (optional)
    if extract_maps:
        logger.info("Step 2: Extracting maps from PDF...")
        import asyncio
        from pdf_processing.image_asset_processing.extract_map_assets import save_metadata

        full_pdf_path = Path("data/pdf_sections/Lost_Mine_of_Phandelver") / pdf_path
        maps_output_dir = run_dir / "map_assets"

        maps = asyncio.run(extract_maps_from_pdf(
            pdf_path=str(full_pdf_path),
            output_dir=str(maps_output_dir)
        ))

        if maps:
            save_metadata(maps, str(maps_output_dir))
            logger.info(f"Extracted {len(maps)} maps")

        logger.info(f"Maps saved to: {maps_output_dir}")

    # Step 3: Position images and export HTML
    logger.info("Step 3: Positioning images and exporting HTML...")
    journal = load_and_position_images(run_dir, map_positioning_mode=map_positioning_mode)
    html_file = journal.export_standalone_html(run_dir / "standalone_export")
    logger.info(f"HTML saved to: {html_file}")

    # Open in browser if requested
    if open_html:
        import subprocess
        subprocess.run(["open", str(html_file)])

    logger.info("=== PIPELINE COMPLETE ===")
    return Path(html_file)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Process D&D PDF chapter to HTML")
    parser.add_argument("--file", required=True, help="PDF filename (e.g., 02_Part_1_Goblin_Arrows.pdf)")
    parser.add_argument("--output-dir", help="Output directory (default: auto-generated)")
    parser.add_argument("--mode", choices=["semantic", "page"], default="semantic",
                        help="Map positioning mode (default: semantic)")
    parser.add_argument("--no-maps", action="store_true", help="Skip map extraction")
    parser.add_argument("--open", action="store_true", help="Open HTML in browser")

    args = parser.parse_args()

    html_path = process_pdf_to_html(
        pdf_path=args.file,
        output_dir=Path(args.output_dir) if args.output_dir else None,
        map_positioning_mode=args.mode,
        extract_maps=not args.no_maps,
        open_html=args.open
    )

    print(f"\n‚úÖ Success! HTML: {html_path}")
</file>

<file path="src/scene_extraction/extract_context.py">
"""Extract environmental context from chapter XML using Gemini."""

import logging
import json
import os
from dotenv import load_dotenv
from google import genai

from .models import ChapterContext

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

GEMINI_MODEL_NAME = "gemini-2.0-flash"


def extract_chapter_context(xml_content: str) -> ChapterContext:
    """
    Use Gemini to analyze chapter XML and extract environmental context.

    Args:
        xml_content: Full chapter XML content

    Returns:
        ChapterContext with environment type, weather, atmosphere, etc.

    Raises:
        ValueError: If Gemini response is invalid or unparseable
        RuntimeError: If Gemini API call fails
    """
    logger.info("Extracting chapter environmental context with Gemini")

    prompt = f"""
Analyze this D&D module chapter XML and extract environmental context.

Return ONLY a JSON object with these fields (use null for unknown):
- environment_type: (e.g., "underground", "forest", "urban", "coastal", "dungeon")
- weather: (e.g., "rainy", "foggy", "clear", "stormy") or null if indoors/underground
- atmosphere: (e.g., "oppressive", "peaceful", "tense", "mysterious")
- lighting: (e.g., "dim torchlight", "bright sunlight", "darkness", "well-lit")
- terrain: (e.g., "rocky caverns", "dense forest", "cobblestone streets")
- additional_notes: Any other relevant environmental details

Focus on the PHYSICAL ENVIRONMENT only. Ignore NPCs, monsters, and plot.

XML:
{xml_content}

Return ONLY valid JSON, no markdown formatting:
"""

    try:
        client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))
        response = client.models.generate_content(
            model=GEMINI_MODEL_NAME,
            contents=prompt
        )

        logger.debug(f"Gemini response: {response.text}")

        # Parse JSON response
        response_text = response.text.strip()

        # Remove markdown code blocks if present
        if response_text.startswith("```"):
            # Remove opening ```json or ``` and closing ```
            lines = response_text.split("\n")
            # Remove first line (``` or ```json)
            lines = lines[1:]
            # Remove last line (```)
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            # If first line is "json", remove it
            if lines and lines[0].strip() == "json":
                lines = lines[1:]
            response_text = "\n".join(lines).strip()

        try:
            context_data = json.loads(response_text)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Gemini response as JSON: {response_text}")
            raise ValueError(f"Failed to parse Gemini response as JSON: {e}") from e

        # Create ChapterContext (Pydantic will validate)
        context = ChapterContext(**context_data)
        logger.info(f"Extracted context: {context.environment_type} environment")

        return context

    except ValueError:
        # Re-raise ValueError without wrapping
        raise
    except Exception as e:
        logger.error(f"Failed to extract chapter context: {e}")
        raise RuntimeError(f"Failed to extract chapter context: {e}") from e
</file>

<file path="src/scene_extraction/identify_scenes.py">
"""Identify physical location scenes from chapter XML using Gemini."""

import logging
import json
import os
from typing import List
from dotenv import load_dotenv
from google import genai

from .models import Scene, ChapterContext

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

# Use gemini-2.0-flash for structured extraction - fast and reliable JSON parsing
GEMINI_MODEL_NAME = "gemini-2.0-flash"


def identify_scene_locations(xml_content: str, chapter_context: ChapterContext) -> List[Scene]:
    """
    Use Gemini to analyze XML and identify physical location scenes.

    Args:
        xml_content: Full chapter XML content
        chapter_context: Environmental context for the chapter

    Returns:
        List of Scene objects (empty list if no scenes found)

    Raises:
        ValueError: If Gemini response is invalid
        RuntimeError: If Gemini API call fails
    """
    logger.info("Identifying scene locations with Gemini")

    context_summary = f"""
Environment: {chapter_context.environment_type}
Weather: {chapter_context.weather or 'N/A'}
Atmosphere: {chapter_context.atmosphere or 'N/A'}
Lighting: {chapter_context.lighting or 'N/A'}
Terrain: {chapter_context.terrain or 'N/A'}
"""

    prompt = f"""
Analyze this D&D module chapter XML and identify physical location scenes (rooms, areas, outdoor locations).

Chapter Environmental Context:
{context_summary}

For each physical location, extract:
- section_path: Full section hierarchy (e.g., "Chapter 2 ‚Üí The Cragmaw Hideout ‚Üí Area 1")
- name: Short descriptive name for the location
- description: Physical environment description ONLY (no NPCs, no monsters, no plot - just the physical space)
- location_type: Specific location type - must be one of: "underground" (caves, dungeons, tunnels), "outdoor" (forest, plains, mountains), "interior" (buildings, structures), "underwater", or "other"
- xml_section_id: The 'id' attribute from the XML section element (or null if not present)

Return ONLY a JSON array of scene objects. If no physical locations found, return [].

EXCLUDE from descriptions:
- Named NPCs or monsters
- Combat encounters
- Plot events
- Treasure or items

INCLUDE in descriptions:
- Physical layout and dimensions
- Architectural features
- Natural features (if outdoor)
- Lighting and atmosphere
- Furniture and fixtures
- Environmental hazards (physical only, like pits or traps)

XML:
{xml_content}

Return ONLY valid JSON array, no markdown formatting:
"""

    try:
        client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))
        response = client.models.generate_content(
            model=GEMINI_MODEL_NAME,
            contents=prompt
        )

        logger.debug(f"Gemini response: {response.text}")

        # Parse JSON response
        response_text = response.text.strip()

        # Remove markdown code blocks if present
        if response_text.startswith("```"):
            lines = response_text.split("\n")
            response_text = "\n".join(lines[1:-1])
            if response_text.startswith("json"):
                response_text = response_text[4:].strip()

        try:
            scenes_data = json.loads(response_text)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Gemini response as JSON: {response_text}")
            raise ValueError(f"Failed to parse Gemini response as JSON: {e}") from e

        # Create Scene objects (Pydantic will validate)
        scenes = [Scene(**scene_data) for scene_data in scenes_data]
        logger.info(f"Identified {len(scenes)} scene locations")

        return scenes

    except Exception as e:
        logger.error(f"Failed to identify scene locations: {e}")
        raise RuntimeError(f"Failed to identify scene locations: {e}") from e
</file>

<file path="tests/actors/test_orchestrate_integration.py">
"""Integration tests for actor orchestration pipeline."""

import pytest
import os
from pathlib import Path

from actors.orchestrate import create_actor_from_description
from actors.models import ActorCreationResult


class TestActorOrchestrationIntegration:
    """Integration tests for full actor creation pipeline."""

    @pytest.mark.smoke
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_full_pipeline_end_to_end(self, tmp_path):
        """
        Smoke test: End-to-end actor creation from description

        Test complete pipeline from description to FoundryVTT.

        NOTE: This test currently expects generate_actor_description() to be implemented.
        Since it's a stub, we skip if NotImplementedError is raised.
        """
        # Skip if API key not available
        if not os.getenv("GeminiImageAPI") and not os.getenv("GEMINI_API_KEY"):
            pytest.skip("Gemini API key not available")

        # Skip if FoundryVTT not available (connection may fail)
        # The test will handle FoundryVTT connection errors gracefully

        description = "A small goblin warrior with a rusty short sword"
        challenge_rating = 0.25

        try:
            result = await create_actor_from_description(
                description=description,
                challenge_rating=challenge_rating,
                output_dir_base=str(tmp_path)
            )

            # Verify result structure
            assert isinstance(result, ActorCreationResult)
            assert result.description == description
            assert result.challenge_rating == challenge_rating

            # Verify intermediate outputs exist
            assert result.raw_stat_block_text
            assert result.stat_block
            assert result.stat_block.name
            assert result.stat_block.armor_class > 0
            assert result.stat_block.hit_points > 0
            assert result.stat_block.challenge_rating == challenge_rating

            assert result.parsed_actor_data
            assert result.parsed_actor_data.name

            # Verify FoundryVTT UUID
            assert result.foundry_uuid
            assert result.foundry_uuid.startswith("Actor.")

            # Verify files were saved
            assert result.output_dir.exists()
            assert result.raw_text_file and result.raw_text_file.exists()
            assert result.stat_block_file and result.stat_block_file.exists()
            assert result.parsed_data_file and result.parsed_data_file.exists()
            assert result.foundry_json_file and result.foundry_json_file.exists()

            # Verify file contents can be read
            assert result.raw_text_file.read_text()
            assert result.stat_block_file.read_text()
            assert result.parsed_data_file.read_text()
            assert result.foundry_json_file.read_text()

            print(f"‚úì Integration test passed!")
            print(f"  Actor: {result.stat_block.name}")
            print(f"  UUID: {result.foundry_uuid}")
            print(f"  Output: {result.output_dir}")

        except NotImplementedError as e:
            if "generate_actor_description" in str(e):
                pytest.skip("generate_actor_description() not yet implemented")
            else:
                raise
        except Exception as e:
            # Log the error for debugging but allow test to see it
            print(f"Integration test failed: {e}")
            raise

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_pipeline_creates_output_directory(self, tmp_path):
        """Test that pipeline creates properly structured output directory."""
        description = "A simple test goblin"

        try:
            result = await create_actor_from_description(
                description=description,
                challenge_rating=0.25,
                output_dir_base=str(tmp_path)
            )

            # Verify directory structure
            assert result.output_dir.exists()
            assert result.output_dir.parent.parent == tmp_path
            assert result.output_dir.name == "actors"

            # Verify timestamp directory exists
            timestamp_dir = result.output_dir.parent
            assert len(timestamp_dir.name) == 15  # YYYYMMDD_HHMMSS

        except NotImplementedError as e:
            if "generate_actor_description" in str(e):
                pytest.skip("generate_actor_description() not yet implemented")
            else:
                raise
</file>

<file path="tests/actors/test_orchestrate.py">
"""Tests for actor orchestration pipeline."""

import json
import pytest
from pathlib import Path
import tempfile
import shutil
from unittest.mock import AsyncMock, MagicMock, patch

from actors.orchestrate import (
    _create_output_directory,
    _save_intermediate_file,
    create_actor_from_description,
    create_actor_from_description_sync,
    create_actors_batch,
    create_actors_batch_sync
)
from actors.models import StatBlock, ActorCreationResult
from foundry.actors.models import ParsedActorData


class TestCreateOutputDirectory:
    """Test output directory creation."""

    def test_creates_directory_with_timestamp(self, tmp_path):
        """Test that directory is created with timestamp format."""
        base_dir = tmp_path / "test_runs"
        output_dir = _create_output_directory(str(base_dir))

        assert output_dir.exists()
        assert output_dir.is_dir()
        assert str(output_dir).startswith(str(base_dir))
        assert output_dir.name == "actors"

    def test_directory_structure(self, tmp_path):
        """Test that directory structure is output/runs/<timestamp>/actors/."""
        base_dir = tmp_path / "test_runs"
        output_dir = _create_output_directory(str(base_dir))

        # Should be: base_dir/<timestamp>/actors/
        assert output_dir.parent.parent == base_dir
        assert output_dir.name == "actors"

    def test_timestamp_format(self, tmp_path):
        """Test that timestamp follows YYYYMMDD_HHMMSS format."""
        base_dir = tmp_path / "test_runs"
        output_dir = _create_output_directory(str(base_dir))

        timestamp_dir = output_dir.parent.name
        # Should match format like: 20241103_143022
        assert len(timestamp_dir) == 15  # YYYYMMDD_HHMMSS
        assert timestamp_dir[8] == "_"

    def test_creates_parents(self, tmp_path):
        """Test that parent directories are created if they don't exist."""
        base_dir = tmp_path / "nonexistent" / "nested" / "path"
        output_dir = _create_output_directory(str(base_dir))

        assert output_dir.exists()
        assert base_dir.exists()


class TestSaveIntermediateFile:
    """Test saving intermediate files."""

    def test_save_text_file(self, tmp_path):
        """Test saving plain text content."""
        filepath = tmp_path / "test.txt"
        content = "Goblin\nSmall humanoid, neutral evil"

        result = _save_intermediate_file(content, filepath, "test text")

        assert result == filepath
        assert filepath.exists()
        assert filepath.read_text() == content

    def test_save_dict_as_json(self, tmp_path):
        """Test saving dict as JSON."""
        filepath = tmp_path / "test.json"
        content = {"name": "Goblin", "cr": 0.25, "hp": 7}

        result = _save_intermediate_file(content, filepath, "test dict")

        assert result == filepath
        assert filepath.exists()

        loaded = json.loads(filepath.read_text())
        assert loaded == content

    def test_save_pydantic_model_as_json(self, tmp_path):
        """Test saving Pydantic model as JSON."""
        filepath = tmp_path / "stat_block.json"
        stat_block = StatBlock(
            name="Goblin",
            raw_text="Goblin stat block...",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25
        )

        result = _save_intermediate_file(stat_block, filepath, "StatBlock")

        assert result == filepath
        assert filepath.exists()

        loaded = json.loads(filepath.read_text())
        assert loaded["name"] == "Goblin"
        assert loaded["armor_class"] == 15

    def test_creates_parent_directory(self, tmp_path):
        """Test that parent directories are created if needed."""
        filepath = tmp_path / "nested" / "dir" / "test.txt"
        content = "test content"

        result = _save_intermediate_file(content, filepath, "nested file")

        assert filepath.exists()
        assert filepath.parent.exists()

    def test_invalid_content_type_raises_error(self, tmp_path):
        """Test that invalid content type raises IOError."""
        filepath = tmp_path / "test.txt"

        with pytest.raises(IOError, match="Failed to save list"):
            _save_intermediate_file([1, 2, 3], filepath, "list")


class TestCreateActorPipeline:
    """Test main actor creation pipeline."""

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_pipeline_calls_all_steps(self, tmp_path):
        """Test that pipeline calls all 5 steps in order."""

        # Mock all the async functions
        with patch('actors.orchestrate.generate_actor_description', new_callable=AsyncMock) as mock_gen, \
             patch('actors.orchestrate.parse_raw_text_to_statblock', new_callable=AsyncMock) as mock_parse_sb, \
             patch('actors.orchestrate.parse_stat_block_parallel', new_callable=AsyncMock) as mock_parse_actor, \
             patch('actors.orchestrate.convert_to_foundry') as mock_convert, \
             patch('actors.orchestrate.FoundryClient') as mock_client_class, \
             patch('actors.orchestrate._create_output_directory') as mock_create_dir:

            # Setup mocks
            mock_gen.return_value = "Goblin\nSmall humanoid..."

            mock_stat_block = StatBlock(
                name="Goblin",
                raw_text="Goblin\nSmall humanoid...",
                armor_class=15,
                hit_points=7,
                challenge_rating=0.25
            )
            mock_parse_sb.return_value = mock_stat_block

            mock_parsed = ParsedActorData(
                source_statblock_name="Goblin",
                name="Goblin",
                armor_class=15,
                hit_points=7,
                challenge_rating=0.25,
                abilities={"str": 8, "dex": 14, "con": 10, "int": 10, "wis": 8, "cha": 8}
            )
            mock_parse_actor.return_value = mock_parsed

            mock_convert.return_value = ({"name": "Goblin"}, [])

            mock_client = MagicMock()
            mock_client.actors.create_actor.return_value = "Actor.abc123"
            mock_client_class.return_value = mock_client

            mock_create_dir.return_value = tmp_path / "test_output"
            (tmp_path / "test_output").mkdir()

            # Mock SpellCache
            mock_spell_cache = MagicMock()

            # Call the function
            result = await create_actor_from_description(
                description="A sneaky goblin",
                challenge_rating=0.25,
                output_dir_base=str(tmp_path),
                spell_cache=mock_spell_cache,
                foundry_client=mock_client
            )

            # Verify all steps were called
            assert mock_gen.called
            assert mock_parse_sb.called
            assert mock_parse_actor.called
            assert mock_convert.called
            assert mock_client.actors.create_actor.called

            # Verify result
            assert result.description == "A sneaky goblin"
            assert result.foundry_uuid == "Actor.abc123"
            assert result.stat_block.name == "Goblin"


class TestSyncWrapper:
    """Test synchronous wrapper function."""

    def test_sync_wrapper_calls_async_function(self, tmp_path):
        """Test that sync wrapper properly calls the async function."""

        # Mock asyncio.run to capture the call
        with patch('actors.orchestrate.asyncio.run') as mock_run:
            mock_result = MagicMock()
            mock_run.return_value = mock_result

            result = create_actor_from_description_sync(
                description="Test goblin",
                challenge_rating=0.25,
                output_dir_base=str(tmp_path)
            )

            # Verify asyncio.run was called
            assert mock_run.called
            assert result == mock_result

            # Verify the async function was passed to asyncio.run
            call_args = mock_run.call_args
            assert call_args is not None


class TestBatchCreation:
    """Test batch actor creation."""

    @pytest.mark.asyncio
    async def test_batch_validates_input_length(self):
        """Test that mismatched list lengths raise error."""
        descriptions = ["Goblin", "Dragon"]
        crs = [0.25]  # Wrong length

        with pytest.raises(ValueError, match="same length"):
            await create_actors_batch(descriptions, challenge_ratings=crs)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_batch_creates_tasks_for_all_descriptions(self, tmp_path):
        """Test that batch creates one task per description."""
        descriptions = ["Goblin 1", "Goblin 2", "Goblin 3"]

        with patch('actors.orchestrate.create_actor_from_description', new_callable=AsyncMock) as mock_create, \
             patch('actors.orchestrate.asyncio.gather', new_callable=AsyncMock) as mock_gather, \
             patch('actors.orchestrate.SpellCache') as mock_cache_class, \
             patch('actors.orchestrate.FoundryClient') as mock_client_class:

            mock_cache = MagicMock()
            mock_cache_class.return_value = mock_cache

            mock_client = MagicMock()
            mock_client_class.return_value = mock_client

            mock_gather.return_value = [MagicMock(), MagicMock(), MagicMock()]

            results = await create_actors_batch(
                descriptions,
                output_dir_base=str(tmp_path)
            )

            # Verify create_actor_from_description was called 3 times
            assert mock_create.call_count == 3

            # Verify gather was called with return_exceptions=True
            assert mock_gather.called
            call_kwargs = mock_gather.call_args.kwargs
            assert call_kwargs.get('return_exceptions') is True

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_batch_returns_exceptions_for_failures(self, tmp_path):
        """Test that individual failures are captured as exceptions."""
        descriptions = ["Good", "Bad", "Ugly"]

        with patch('actors.orchestrate.create_actor_from_description', new_callable=AsyncMock) as mock_create, \
             patch('actors.orchestrate.SpellCache') as mock_cache_class, \
             patch('actors.orchestrate.FoundryClient') as mock_client_class:

            # Setup mocks
            mock_cache = MagicMock()
            mock_cache_class.return_value = mock_cache
            mock_client = MagicMock()
            mock_client_class.return_value = mock_client

            # Mock results: success, failure, success
            mock_result1 = MagicMock(spec=ActorCreationResult)
            mock_error = ValueError("API failed")
            mock_result3 = MagicMock(spec=ActorCreationResult)

            # Use side_effect to return different values
            mock_create.side_effect = [mock_result1, mock_error, mock_result3]

            # Manually simulate asyncio.gather behavior with exceptions
            with patch('actors.orchestrate.asyncio.gather', new_callable=AsyncMock) as mock_gather:
                mock_gather.return_value = [mock_result1, mock_error, mock_result3]

                results = await create_actors_batch(
                    descriptions,
                    output_dir_base=str(tmp_path)
                )

                assert len(results) == 3
                assert results[0] == mock_result1
                assert isinstance(results[1], ValueError)
                assert results[2] == mock_result3

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_batch_uses_provided_resources(self, tmp_path):
        """Test that batch uses pre-loaded resources if provided."""
        descriptions = ["Goblin", "Dragon"]
        mock_cache = MagicMock()
        mock_client = MagicMock()

        with patch('actors.orchestrate.create_actor_from_description', new_callable=AsyncMock) as mock_create, \
             patch('actors.orchestrate.asyncio.gather', new_callable=AsyncMock) as mock_gather:

            mock_gather.return_value = [MagicMock(), MagicMock()]

            await create_actors_batch(
                descriptions,
                output_dir_base=str(tmp_path),
                spell_cache=mock_cache,
                foundry_client=mock_client
            )

            # Verify both calls used the same cache and client
            assert mock_create.call_count == 2
            for call in mock_create.call_args_list:
                kwargs = call.kwargs
                assert kwargs['spell_cache'] is mock_cache
                assert kwargs['foundry_client'] is mock_client

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_batch_creates_resources_if_not_provided(self, tmp_path):
        """Test that batch creates resources if not provided."""
        descriptions = ["Goblin"]

        with patch('actors.orchestrate.create_actor_from_description', new_callable=AsyncMock) as mock_create, \
             patch('actors.orchestrate.asyncio.gather', new_callable=AsyncMock) as mock_gather, \
             patch('actors.orchestrate.SpellCache') as mock_cache_class, \
             patch('actors.orchestrate.FoundryClient') as mock_client_class:

            mock_cache = MagicMock()
            mock_cache_class.return_value = mock_cache

            mock_client = MagicMock()
            mock_client_class.return_value = mock_client

            mock_gather.return_value = [MagicMock()]

            await create_actors_batch(
                descriptions,
                output_dir_base=str(tmp_path)
            )

            # Verify resources were created
            assert mock_cache_class.called
            assert mock_cache.load.called
            assert mock_client_class.called

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_batch_with_challenge_ratings(self, tmp_path):
        """Test batch processing with challenge ratings."""
        descriptions = ["Goblin", "Dragon"]
        crs = [0.25, 10.0]

        with patch('actors.orchestrate.create_actor_from_description', new_callable=AsyncMock) as mock_create, \
             patch('actors.orchestrate.asyncio.gather', new_callable=AsyncMock) as mock_gather, \
             patch('actors.orchestrate.SpellCache') as mock_cache_class, \
             patch('actors.orchestrate.FoundryClient') as mock_client_class:

            mock_cache = MagicMock()
            mock_cache_class.return_value = mock_cache
            mock_client = MagicMock()
            mock_client_class.return_value = mock_client

            mock_gather.return_value = [MagicMock(), MagicMock()]

            await create_actors_batch(
                descriptions,
                challenge_ratings=crs,
                output_dir_base=str(tmp_path)
            )

            # Verify both calls were made with correct CRs
            assert mock_create.call_count == 2
            call1_kwargs = mock_create.call_args_list[0].kwargs
            call2_kwargs = mock_create.call_args_list[1].kwargs

            assert call1_kwargs['description'] == "Goblin"
            assert call1_kwargs['challenge_rating'] == 0.25
            assert call2_kwargs['description'] == "Dragon"
            assert call2_kwargs['challenge_rating'] == 10.0

    def test_sync_batch_wrapper(self, tmp_path):
        """Test synchronous batch wrapper."""
        descriptions = ["Goblin"]

        with patch('actors.orchestrate.asyncio.run') as mock_run:
            mock_result = [MagicMock()]
            mock_run.return_value = mock_result

            result = create_actors_batch_sync(
                descriptions,
                output_dir_base=str(tmp_path)
            )

            assert mock_run.called
            assert result == mock_result
</file>

<file path="tests/actors/test_parse_stat_blocks.py">
"""Tests for stat block parsing with Gemini."""

import pytest
from pathlib import Path
from actors.parse_stat_blocks import (
    parse_stat_block_with_gemini,
    extract_stat_blocks_from_document,
    extract_stat_blocks_from_xml_file
)
from actors.models import StatBlock
from models import XMLDocument


@pytest.mark.integration
@pytest.mark.requires_api
class TestStatBlockParsing:
    """Test stat block parsing with real Gemini API calls."""

    def test_parse_goblin_stat_block(self, check_api_key):
        """Test parsing a complete goblin stat block."""
        # Load sample stat block
        fixture_path = Path(__file__).parent / "fixtures" / "sample_stat_block.txt"
        with open(fixture_path, 'r') as f:
            raw_text = f.read()

        # Parse with Gemini
        stat_block = parse_stat_block_with_gemini(raw_text)

        # Verify structured data
        assert isinstance(stat_block, StatBlock)
        assert stat_block.name.upper() == "GOBLIN"  # Case may vary
        assert stat_block.armor_class == 15
        assert stat_block.hit_points == 7
        assert stat_block.challenge_rating == 0.25
        assert stat_block.size == "Small"
        assert "humanoid" in stat_block.type.lower()  # May include subtype
        assert stat_block.raw_text == raw_text

        # Verify abilities parsed
        assert stat_block.abilities is not None
        assert stat_block.abilities["STR"] == 8
        assert stat_block.abilities["DEX"] == 14


@pytest.mark.unit
class TestStatBlockParsingUnit:
    """Unit tests for stat block parsing (mocked)."""

    def test_parse_returns_stat_block_model(self):
        """Test parser returns StatBlock model (integration test required for full test)."""
        # This is a placeholder - real testing requires API
        # Just verify the function exists and has correct signature
        from actors.parse_stat_blocks import parse_stat_block_with_gemini
        import inspect

        sig = inspect.signature(parse_stat_block_with_gemini)
        assert 'raw_text' in sig.parameters


@pytest.mark.unit
class TestStatBlockExtractionFromXMLDocument:
    """Unit tests for extracting stat blocks from XMLDocument objects."""

    def test_extract_stat_blocks_from_document(self):
        """Test extracting stat blocks from XMLDocument."""
        # Load fixture
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_stat_blocks.xml"
        with open(fixture_path, 'r') as f:
            xml_string = f.read()

        # Parse to XMLDocument
        doc = XMLDocument.from_xml(xml_string)

        # Extract stat blocks
        stat_blocks = extract_stat_blocks_from_document(doc)

        # Verify extraction
        assert len(stat_blocks) == 2
        assert stat_blocks[0]["name"] == "Goblin"
        assert "GOBLIN" in stat_blocks[0]["raw_text"]
        assert "Small humanoid" in stat_blocks[0]["raw_text"]

        assert stat_blocks[1]["name"] == "Goblin Boss"
        assert "GOBLIN BOSS" in stat_blocks[1]["raw_text"]
        assert "chain shirt" in stat_blocks[1]["raw_text"]

    def test_extract_stat_blocks_from_document_empty(self):
        """Test extracting stat blocks from document with no stat blocks."""
        xml_string = """<Chapter>
            <page number="1">
                <section>Introduction</section>
                <p>No stat blocks here.</p>
            </page>
        </Chapter>"""

        doc = XMLDocument.from_xml(xml_string)
        stat_blocks = extract_stat_blocks_from_document(doc)

        assert len(stat_blocks) == 0

    def test_extract_stat_blocks_from_xml_file(self):
        """Test convenience wrapper for extracting from XML file."""
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_stat_blocks.xml"

        # Extract stat blocks directly from file
        stat_blocks = extract_stat_blocks_from_xml_file(str(fixture_path))

        # Verify extraction
        assert len(stat_blocks) == 2
        assert stat_blocks[0]["name"] == "Goblin"
        assert stat_blocks[1]["name"] == "Goblin Boss"

    def test_extract_stat_blocks_from_xml_file_not_found(self):
        """Test that FileNotFoundError is raised for missing files."""
        with pytest.raises(FileNotFoundError):
            extract_stat_blocks_from_xml_file("/nonexistent/path.xml")
</file>

<file path="tests/foundry/actors/test_innate_spellcasting.py">
"""Tests for innate spellcasting."""

import pytest
from foundry.actors.models import ParsedActorData, InnateSpellcasting, InnateSpell
from foundry.actors.converter import convert_to_foundry


class TestInnateSpellcastingModel:
    """Tests for innate spellcasting models."""

    def test_innate_spell(self):
        """Should create innate spell."""
        spell = InnateSpell(
            name="Fireball",
            frequency="at will"
        )

        assert spell.name == "Fireball"
        assert spell.frequency == "at will"
        assert spell.uses is None

    def test_innate_spell_with_uses(self):
        """Should handle limited-use spells."""
        spell = InnateSpell(
            name="Hold Monster",
            frequency="3/day",
            uses=3
        )

        assert spell.frequency == "3/day"
        assert spell.uses == 3

    def test_innate_spellcasting(self):
        """Should group spells by frequency."""
        innate = InnateSpellcasting(
            ability="charisma",
            save_dc=21,
            spells=[
                InnateSpell(name="Detect Magic", frequency="at will"),
                InnateSpell(name="Fireball", frequency="at will"),
                InnateSpell(name="Hold Monster", frequency="3/day", uses=3),
                InnateSpell(name="Wall of Fire", frequency="3/day", uses=3),
            ]
        )

        assert innate.ability == "charisma"
        assert innate.save_dc == 21
        assert len(innate.spells) == 4

    def test_parsed_actor_with_innate_spellcasting(self):
        """Should include innate spellcasting in ParsedActorData."""
        actor = ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend",
            armor_class=19,
            hit_points=300,
            challenge_rating=20,
            abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
            innate_spellcasting=InnateSpellcasting(
                ability="charisma",
                save_dc=21,
                spells=[
                    InnateSpell(name="Fireball", frequency="at will"),
                ]
            )
        )

        assert actor.innate_spellcasting is not None
        assert len(actor.innate_spellcasting.spells) == 1


class TestInnateSpellcastingConversion:
    """Tests for converting innate spellcasting to FoundryVTT format."""

    async def test_converts_innate_spellcasting_to_feat(self):
        """Should create Innate Spellcasting feat."""
        actor = ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend",
            armor_class=19,
            hit_points=300,
            challenge_rating=20,
            abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
            innate_spellcasting=InnateSpellcasting(
                ability="charisma",
                save_dc=21,
                spells=[
                    InnateSpell(name="Detect Magic", frequency="at will"),
                    InnateSpell(name="Fireball", frequency="at will"),
                ]
            )
        )

        result, spell_uuids = await convert_to_foundry(actor)

        # Should have Innate Spellcasting feat
        feats = [item for item in result["items"] if item["type"] == "feat"]
        innate_feat = next((f for f in feats if "Innate Spellcasting" in f["name"]), None)

        assert innate_feat is not None
        assert "charisma" in innate_feat["system"]["description"]["value"].lower()

    async def test_converts_innate_spells_to_spell_items(self):
        """Should create spell items for innate spells."""
        actor = ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend",
            armor_class=19,
            hit_points=300,
            challenge_rating=20,
            abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
            innate_spellcasting=InnateSpellcasting(
                ability="charisma",
                save_dc=21,
                spells=[
                    InnateSpell(name="Fireball", frequency="at will"),
                    InnateSpell(name="Hold Monster", frequency="3/day", uses=3),
                ]
            )
        )

        result, spell_uuids = await convert_to_foundry(actor, include_spells_in_payload=True)

        # Should have spell items in payload
        spells = [item for item in result["items"] if item["type"] == "spell"]

        assert len(spells) >= 2

        fireball = next((s for s in spells if s["name"] == "Fireball"), None)
        hold_monster = next((s for s in spells if s["name"] == "Hold Monster"), None)

        assert fireball is not None
        assert hold_monster is not None

        # Limited-use spell should have uses
        assert hold_monster["system"]["uses"]["max"] == 3

    @pytest.mark.integration
    @pytest.mark.requires_foundry
    @pytest.mark.asyncio
    async def test_looks_up_spell_uuids_from_cache(self):
        """Should use SpellCache to get proper spell UUIDs."""
        from foundry.actors.spell_cache import SpellCache
        from dotenv import load_dotenv
        import os

        load_dotenv()

        # Skip if FoundryVTT not available
        if not os.getenv("FOUNDRY_RELAY_URL"):
            pytest.skip("FoundryVTT not configured")

        spell_cache = SpellCache()
        spell_cache.load()

        actor = ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend",
            armor_class=19,
            hit_points=300,
            challenge_rating=20,
            abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
            innate_spellcasting=InnateSpellcasting(
                ability="charisma",
                save_dc=21,
                spells=[
                    InnateSpell(name="Fireball", frequency="at will"),
                ]
            )
        )

        # Convert with spell cache
        result, spell_uuids = await convert_to_foundry(actor, spell_cache=spell_cache)

        # NEW behavior: spells NOT in payload, returned as UUIDs
        spells = [item for item in result["items"] if item["type"] == "spell"]
        assert len(spells) == 0, "Spells should NOT be in payload by default"

        # Check spell UUIDs returned separately
        assert len(spell_uuids) == 1
        fireball_uuid = spell_uuids[0]
        assert fireball_uuid.startswith("Compendium.")

        # Also test backward compatibility with include_spells_in_payload=True
        result_compat, _ = await convert_to_foundry(actor, spell_cache=spell_cache, include_spells_in_payload=True)
        spells_compat = [item for item in result_compat["items"] if item["type"] == "spell"]
        fireball = next((s for s in spells_compat if s["name"] == "Fireball"), None)

        assert fireball is not None
        # Should have proper level (Fireball is 3rd level)
        assert fireball["system"]["level"] == 3
</file>

<file path="tests/foundry/actors/test_parser.py">
"""Tests for parallel StatBlock ‚Üí ParsedActorData parser."""

import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch

from actors.models import StatBlock
from foundry.actors.models import (
    ParsedActorData, Attack, Trait, Multiattack,
    InnateSpellcasting, InnateSpell, DamageFormula, AttackSave
)
from foundry.actors.parser import (
    parse_single_action_async,
    parse_single_trait_async,
    parse_innate_spellcasting_async,
    parse_stat_block_parallel,
    parse_multiple_stat_blocks
)
from foundry.actors.spell_cache import SpellCache


# Test fixtures

@pytest.fixture
def mock_spell_cache():
    """Mock spell cache with common spells."""
    cache = Mock(spec=SpellCache)
    cache.get_spell_uuid = Mock(side_effect=lambda name: {
        "detect magic": "Compendium.dnd5e.spells.Item.detect_magic_uuid",
        "fireball": "Compendium.dnd5e.spells.Item.fireball_uuid",
        "hold monster": "Compendium.dnd5e.spells.Item.hold_monster_uuid",
        "wish": "Compendium.dnd5e.spells.Item.wish_uuid"
    }.get(name))
    return cache


@pytest.fixture
def goblin_statblock():
    """Simple Goblin stat block for testing."""
    return StatBlock(
        name="Goblin",
        raw_text="Goblin stat block...",
        armor_class=15,
        hit_points=7,
        challenge_rating=0.25,
        size="Small",
        type="humanoid",
        alignment="neutral evil",
        abilities={"STR": 8, "DEX": 14, "CON": 10, "INT": 10, "WIS": 8, "CHA": 8},
        traits=["Nimble Escape. The goblin can take the Disengage or Hide action as a bonus action on each of its turns."],
        actions=[
            "Scimitar. Melee Weapon Attack: +4 to hit, reach 5 ft., one target. Hit: 5 (1d6 + 2) slashing damage.",
            "Shortbow. Ranged Weapon Attack: +4 to hit, range 80/320 ft., one target. Hit: 5 (1d6 + 2) piercing damage."
        ],
        reactions=[]
    )


@pytest.fixture
def pit_fiend_statblock():
    """Complex Pit Fiend stat block for testing parallel processing."""
    return StatBlock(
        name="Pit Fiend",
        raw_text="Pit Fiend stat block...",
        armor_class=19,
        hit_points=300,
        challenge_rating=20,
        size="Large",
        type="fiend",
        alignment="lawful evil",
        abilities={"STR": 26, "DEX": 14, "CON": 24, "INT": 22, "WIS": 18, "CHA": 24},
        traits=[
            "Fear Aura. Any creature hostile to the pit fiend that starts its turn within 20 feet of the pit fiend must make a DC 21 Wisdom saving throw, unless the pit fiend is incapacitated. On a failed save, the creature is frightened until the start of its next turn. If a creature's saving throw is successful, the creature is immune to the pit fiend's Fear Aura for the next 24 hours.",
            "Magic Resistance. The pit fiend has advantage on saving throws against spells and other magical effects.",
            "Magic Weapons. The pit fiend's weapon attacks are magical.",
            "Innate Spellcasting. The pit fiend's spellcasting ability is Charisma (spell save DC 21). The pit fiend can innately cast the following spells, requiring no material components:\nAt will: detect magic, fireball\n3/day each: hold monster\n1/day: wish"
        ],
        actions=[
            "Multiattack. The pit fiend makes four attacks: one with its bite, one with its claw, one with its mace, and one with its tail.",
            "Bite. Melee Weapon Attack: +14 to hit, reach 5 ft., one target. Hit: 22 (4d6 + 8) piercing damage. The target must succeed on a DC 21 Constitution saving throw or become poisoned. While poisoned in this way, the target can't regain hit points, and it takes 21 (6d6) poison damage at the start of each of its turns. The poisoned target can repeat the saving throw at the end of each of its turns, ending the effect on itself on a success.",
            "Claw. Melee Weapon Attack: +14 to hit, reach 10 ft., one target. Hit: 17 (2d8 + 8) slashing damage.",
            "Mace. Melee Weapon Attack: +14 to hit, reach 10 ft., one target. Hit: 15 (2d6 + 8) bludgeoning damage plus 21 (6d6) fire damage.",
            "Tail. Melee Weapon Attack: +14 to hit, reach 10 ft., one target. Hit: 24 (3d10 + 8) piercing damage."
        ],
        reactions=[]
    )


# Unit tests for individual parsing functions

@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_single_action_simple_melee():
    """Test parsing a simple melee attack (Scimitar)."""
    action_text = "Scimitar. Melee Weapon Attack: +4 to hit, reach 5 ft., one target. Hit: 5 (1d6 + 2) slashing damage."

    result = await parse_single_action_async(action_text)

    assert isinstance(result, Attack)
    assert result.name == "Scimitar"
    assert result.attack_type == "melee"
    assert result.attack_bonus == 4
    assert result.reach == 5
    assert len(result.damage) == 1
    assert result.damage[0].number == 1
    assert result.damage[0].denomination == 6
    assert result.damage[0].bonus == "+2"
    assert result.damage[0].type == "slashing"


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_single_action_ranged():
    """Test parsing a ranged attack (Shortbow)."""
    action_text = "Shortbow. Ranged Weapon Attack: +4 to hit, range 80/320 ft., one target. Hit: 5 (1d6 + 2) piercing damage."

    result = await parse_single_action_async(action_text)

    assert isinstance(result, Attack)
    assert result.name == "Shortbow"
    assert result.attack_type == "ranged"
    assert result.attack_bonus == 4
    # Ranged weapons have separate short and long range fields
    assert result.range_short == 80
    assert result.range_long == 320
    assert len(result.damage) == 1
    assert result.damage[0].type == "piercing"


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_single_action_with_save():
    """Test parsing attack with saving throw (Poison Bite)."""
    action_text = """Bite. Melee Weapon Attack: +14 to hit, reach 5 ft., one target. Hit: 22 (4d6 + 8) piercing damage. The target must succeed on a DC 21 Constitution saving throw or become poisoned. While poisoned in this way, the target can't regain hit points, and it takes 21 (6d6) poison damage at the start of each of its turns. The poisoned target can repeat the saving throw at the end of each of its turns, ending the effect on itself on a success."""

    result = await parse_single_action_async(action_text)

    assert isinstance(result, Attack)
    assert result.name == "Bite"
    assert result.attack_bonus == 14
    assert len(result.damage) == 1
    assert result.damage[0].number == 4
    assert result.damage[0].denomination == 6

    # Check saving throw
    assert result.attack_save is not None
    assert result.attack_save.ability == "con"
    assert result.attack_save.dc == 21
    assert len(result.attack_save.ongoing_damage) > 0
    assert result.attack_save.ongoing_damage[0].number == 6
    assert result.attack_save.ongoing_damage[0].denomination == 6
    assert result.attack_save.ongoing_damage[0].type == "poison"


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_single_action_multiattack():
    """Test parsing multiattack."""
    action_text = "Multiattack. The pit fiend makes four attacks: one with its bite, one with its claw, one with its mace, and one with its tail."

    result = await parse_single_action_async(action_text)

    assert isinstance(result, Multiattack)
    assert result.name == "Multiattack"
    assert result.num_attacks == 4
    assert "four attacks" in result.description.lower()


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_single_trait_passive():
    """Test parsing passive trait (Nimble Escape)."""
    trait_text = "Nimble Escape. The goblin can take the Disengage or Hide action as a bonus action on each of its turns."

    result = await parse_single_trait_async(trait_text)

    assert isinstance(result, Trait)
    assert result.name == "Nimble Escape"
    assert result.activation in ["passive", "bonus"]  # Either is acceptable
    assert "Disengage" in result.description or "Hide" in result.description


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_single_trait_action():
    """Test parsing action trait (Fear Aura)."""
    trait_text = "Fear Aura. Any creature hostile to the pit fiend that starts its turn within 20 feet of the pit fiend must make a DC 21 Wisdom saving throw, unless the pit fiend is incapacitated. On a failed save, the creature is frightened until the start of its next turn."

    result = await parse_single_trait_async(trait_text)

    assert isinstance(result, Trait)
    assert result.name == "Fear Aura"
    assert result.activation in ["passive", "action"]  # Could be either
    assert "DC 21" in result.description or "frightened" in result.description.lower()


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_innate_spellcasting(mock_spell_cache):
    """Test parsing innate spellcasting with spell UUID resolution."""
    trait_text = """Innate Spellcasting. The pit fiend's spellcasting ability is Charisma (spell save DC 21). The pit fiend can innately cast the following spells, requiring no material components:
At will: detect magic, fireball
3/day each: hold monster
1/day: wish"""

    result = await parse_innate_spellcasting_async(trait_text, spell_cache=mock_spell_cache)

    assert isinstance(result, InnateSpellcasting)
    assert result.ability == "charisma"
    assert result.save_dc == 21
    assert len(result.spells) == 4  # 2 at will + 1 from 3/day + 1 from 1/day

    # Check at will spells
    at_will_spells = [s for s in result.spells if s.frequency == "at will"]
    assert len(at_will_spells) == 2
    assert any(s.name == "detect magic" for s in at_will_spells)
    assert any(s.name == "fireball" for s in at_will_spells)

    # Check limited use spells
    limited_spells = [s for s in result.spells if "day" in s.frequency]
    assert len(limited_spells) == 2
    hold_monster = next(s for s in limited_spells if s.name == "hold monster")
    assert hold_monster.frequency == "3/day"
    assert hold_monster.uses == 3

    wish = next(s for s in limited_spells if s.name == "wish")
    assert wish.frequency == "1/day"
    assert wish.uses == 1

    # Check UUIDs were resolved
    for spell in result.spells:
        assert spell.uuid is not None
        assert spell.uuid.startswith("Compendium.")


# Integration tests for full stat block parsing

@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_stat_block_parallel_goblin(goblin_statblock, mock_spell_cache):
    """Test full Goblin parsing (2 attacks + 1 trait)."""
    result = await parse_stat_block_parallel(goblin_statblock, spell_cache=mock_spell_cache)

    assert isinstance(result, ParsedActorData)
    assert result.name == "Goblin"
    assert result.armor_class == 15
    assert result.hit_points == 7
    assert result.challenge_rating == 0.25

    # Check attacks
    assert len(result.attacks) == 2
    scimitar = next(a for a in result.attacks if a.name == "Scimitar")
    assert scimitar.attack_type == "melee"
    shortbow = next(a for a in result.attacks if a.name == "Shortbow")
    assert shortbow.attack_type == "ranged"

    # Check traits
    assert len(result.traits) == 1
    assert result.traits[0].name == "Nimble Escape"

    # No multiattack or spellcasting
    assert result.multiattack is None
    assert result.innate_spellcasting is None


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_stat_block_parallel_pit_fiend(pit_fiend_statblock, mock_spell_cache):
    """Test full Pit Fiend parsing (9 parallel calls: 4 attacks + 4 traits + 1 multiattack)."""
    result = await parse_stat_block_parallel(pit_fiend_statblock, spell_cache=mock_spell_cache)

    assert isinstance(result, ParsedActorData)
    assert result.name == "Pit Fiend"
    assert result.armor_class == 19
    assert result.hit_points == 300
    assert result.challenge_rating == 20

    # Check multiattack (separated from regular attacks)
    assert result.multiattack is not None
    assert result.multiattack.num_attacks == 4

    # Check attacks (4 regular attacks, multiattack excluded)
    assert len(result.attacks) == 4
    attack_names = {a.name for a in result.attacks}
    assert attack_names == {"Bite", "Claw", "Mace", "Tail"}

    # Check Bite has saving throw
    bite = next(a for a in result.attacks if a.name == "Bite")
    assert bite.attack_save is not None
    assert bite.attack_save.ability == "con"
    assert bite.attack_save.dc == 21

    # Check innate spellcasting (separated from regular traits)
    assert result.innate_spellcasting is not None
    assert result.innate_spellcasting.ability == "charisma"
    assert result.innate_spellcasting.save_dc == 21
    assert len(result.innate_spellcasting.spells) == 4

    # Check regular traits (3 traits: Fear Aura, Magic Resistance, Magic Weapons)
    # Innate Spellcasting should be excluded
    assert len(result.traits) == 3
    trait_names = {t.name for t in result.traits}
    assert "Fear Aura" in trait_names
    assert "Magic Resistance" in trait_names
    assert "Magic Weapons" in trait_names
    assert "Innate Spellcasting" not in trait_names  # Should be in innate_spellcasting


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_multiple_stat_blocks(goblin_statblock, pit_fiend_statblock, mock_spell_cache):
    """Test batch processing of multiple stat blocks."""
    results = await parse_multiple_stat_blocks(
        [goblin_statblock, pit_fiend_statblock],
        spell_cache=mock_spell_cache
    )

    assert len(results) == 2
    assert all(isinstance(r, ParsedActorData) for r in results)

    # Check that both were parsed correctly
    goblin_result = next(r for r in results if r.name == "Goblin")
    assert len(goblin_result.attacks) == 2

    pit_fiend_result = next(r for r in results if r.name == "Pit Fiend")
    assert len(pit_fiend_result.attacks) == 4
    assert pit_fiend_result.multiattack is not None


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_stat_block_with_no_spells(goblin_statblock):
    """Test parsing without spell cache (spells should still parse, just no UUIDs)."""
    result = await parse_stat_block_parallel(goblin_statblock, spell_cache=None)

    assert isinstance(result, ParsedActorData)
    assert result.name == "Goblin"
    # Should work fine without spell cache since Goblin has no spells


@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_stat_block_empty_lists():
    """Test parsing stat block with empty action/trait lists."""
    minimal_statblock = StatBlock(
        name="Minimal Creature",
        raw_text="Minimal creature...",
        armor_class=10,
        hit_points=5,
        challenge_rating=0,
        traits=[],
        actions=[],
        reactions=[]
    )

    result = await parse_stat_block_parallel(minimal_statblock)

    assert isinstance(result, ParsedActorData)
    assert result.name == "Minimal Creature"
    assert len(result.attacks) == 0
    assert len(result.traits) == 0
    assert result.multiattack is None
    assert result.innate_spellcasting is None


# Performance test

@pytest.mark.integration
@pytest.mark.asyncio
async def test_parallel_processing_performance(pit_fiend_statblock, mock_spell_cache):
    """Verify parallel processing is actually faster than sequential."""
    import time

    # Parallel processing (current implementation)
    start = time.time()
    result = await parse_stat_block_parallel(pit_fiend_statblock, spell_cache=mock_spell_cache)
    parallel_time = time.time() - start

    # Should complete in ~3-5 seconds (9 parallel calls)
    assert parallel_time < 10  # Generous upper bound

    # Verify correctness
    assert len(result.attacks) == 4
    assert result.multiattack is not None
    assert len(result.traits) == 3
    assert result.innate_spellcasting is not None

    print(f"\nParallel processing time: {parallel_time:.2f}s for 9 Gemini calls")
</file>

<file path="tests/foundry/items/test_fetch.py">
"""Tests for foundry.items.fetch module."""

import pytest
from unittest.mock import patch, MagicMock
from foundry.items.fetch import fetch_items_by_type, fetch_all_spells


class TestFetchItemsByType:
    """Tests for fetch_items_by_type function."""

    @patch('foundry.items.fetch.requests.get')
    @patch('foundry.items.fetch.os.getenv')
    def test_basic_fetch(self, mock_getenv, mock_get):
        """Should fetch items successfully."""
        # Setup environment variables
        mock_getenv.side_effect = lambda key: {
            'FOUNDRY_RELAY_URL': 'http://localhost:3010',
            'FOUNDRY_API_KEY': 'test-key',
            'FOUNDRY_CLIENT_ID': 'test-client'
        }.get(key)

        # Setup mock response
        mock_response = MagicMock()
        mock_response.json.return_value = {
            'results': [
                {'uuid': 'Compendium.dnd5e.spells.1', 'name': 'Fireball'},
                {'uuid': 'Compendium.dnd5e.spells.2', 'name': 'Lightning Bolt'},
            ]
        }
        mock_get.return_value = mock_response

        # Execute
        items = fetch_items_by_type('spell', use_two_letter_fallback=False)

        # Verify
        assert len(items) >= 2  # May have more from empty query
        assert any(item['name'] == 'Fireball' for item in items)

    @patch('foundry.items.fetch.os.getenv')
    def test_missing_credentials(self, mock_getenv):
        """Should raise ValueError if credentials missing."""
        mock_getenv.return_value = None

        with pytest.raises(ValueError, match="Missing required credentials"):
            fetch_items_by_type('spell')

    @patch('foundry.items.fetch.requests.get')
    @patch('foundry.items.fetch.os.getenv')
    def test_deduplication(self, mock_getenv, mock_get):
        """Should deduplicate items by UUID."""
        mock_getenv.side_effect = lambda key: {
            'FOUNDRY_RELAY_URL': 'http://localhost:3010',
            'FOUNDRY_API_KEY': 'test-key',
            'FOUNDRY_CLIENT_ID': 'test-client'
        }.get(key)

        # Setup mock to return same item multiple times
        mock_response = MagicMock()
        mock_response.json.return_value = {
            'results': [
                {'uuid': 'Compendium.dnd5e.spells.1', 'name': 'Fireball'},
                {'uuid': 'Compendium.dnd5e.spells.1', 'name': 'Fireball'},  # Duplicate UUID
            ]
        }
        mock_get.return_value = mock_response

        items = fetch_items_by_type('spell', use_two_letter_fallback=False)

        # Should only have one item despite duplicate in results
        uuids = [item['uuid'] for item in items if item['uuid'] == 'Compendium.dnd5e.spells.1']
        assert len(uuids) == 1

    @patch('foundry.items.fetch.requests.get')
    @patch('foundry.items.fetch.os.getenv')
    def test_two_letter_fallback_triggered(self, mock_getenv, mock_get):
        """Should use two-letter combos when hitting 200 limit."""
        mock_getenv.side_effect = lambda key: {
            'FOUNDRY_RELAY_URL': 'http://localhost:3010',
            'FOUNDRY_API_KEY': 'test-key',
            'FOUNDRY_CLIENT_ID': 'test-client'
        }.get(key)

        # Setup mock: 'a' returns exactly 200 items, triggering fallback
        def mock_response_side_effect(*args, **kwargs):
            query = kwargs.get('params', {}).get('query', '')
            response = MagicMock()

            if query == 'a':
                # Return exactly 200 items to trigger fallback
                response.json.return_value = {
                    'results': [{'uuid': f'Compendium.dnd5e.spells.{i}', 'name': f'Spell {i}'}
                               for i in range(200)]
                }
            elif query in ['aa', 'ab', 'ac']:
                # Two-letter queries return fewer items
                response.json.return_value = {'results': [
                    {'uuid': f'Compendium.dnd5e.spells.{query}1', 'name': f'{query.upper()} Spell'}
                ]}
            else:
                response.json.return_value = {'results': []}

            return response

        mock_get.side_effect = mock_response_side_effect

        items = fetch_items_by_type('spell', use_two_letter_fallback=True)

        # Should have items from both single-letter and two-letter queries
        assert len(items) > 200

    @patch('foundry.items.fetch.requests.get')
    @patch('foundry.items.fetch.os.getenv')
    def test_custom_credentials(self, mock_getenv, mock_get):
        """Should use provided credentials over environment variables."""
        mock_response = MagicMock()
        mock_response.json.return_value = {'results': []}
        mock_get.return_value = mock_response

        fetch_items_by_type(
            'spell',
            relay_url='http://custom:3010',
            api_key='custom-key',
            client_id='custom-client',
            use_two_letter_fallback=False
        )

        # Verify custom credentials were used
        call_args = mock_get.call_args
        assert call_args[1]['headers']['x-api-key'] == 'custom-key'
        assert call_args[1]['params']['clientId'] == 'custom-client'

    @patch('foundry.items.fetch.requests.get')
    @patch('foundry.items.fetch.os.getenv')
    def test_api_error_handling(self, mock_getenv, mock_get):
        """Should raise RuntimeError on API failure."""
        import requests

        mock_getenv.side_effect = lambda key: {
            'FOUNDRY_RELAY_URL': 'http://localhost:3010',
            'FOUNDRY_API_KEY': 'test-key',
            'FOUNDRY_CLIENT_ID': 'test-client'
        }.get(key)

        mock_get.side_effect = requests.exceptions.RequestException("API Error")

        with pytest.raises(RuntimeError, match="Failed to search"):
            fetch_items_by_type('spell', use_two_letter_fallback=False)


class TestFetchAllSpells:
    """Tests for fetch_all_spells convenience function."""

    @patch('foundry.items.fetch.fetch_items_by_type')
    def test_calls_fetch_items_by_type(self, mock_fetch):
        """Should call fetch_items_by_type with 'spell' subtype."""
        mock_fetch.return_value = []

        result = fetch_all_spells(relay_url='http://test', api_key='key', client_id='id')

        mock_fetch.assert_called_once()
        call_args = mock_fetch.call_args
        assert call_args[0][0] == 'spell'
        assert call_args[1]['relay_url'] == 'http://test'


@pytest.mark.integration
class TestFetchItemsIntegration:
    """Integration tests that make real API calls."""

    def test_fetch_real_spells(self, check_api_key):
        """Fetch real spells from FoundryVTT (requires running server)."""
        import os

        # Skip if relay not configured
        if not os.getenv('FOUNDRY_RELAY_URL'):
            pytest.skip("FOUNDRY_RELAY_URL not configured")

        items = fetch_items_by_type('spell', use_two_letter_fallback=False)

        # Should return some spells
        assert len(items) > 0

        # Verify structure
        for item in items[:5]:
            assert 'uuid' in item
            assert 'name' in item
            assert 'Compendium.' in item['uuid']

    def test_fetch_real_weapons(self, check_api_key):
        """Fetch real weapons from FoundryVTT (requires running server)."""
        import os

        if not os.getenv('FOUNDRY_RELAY_URL'):
            pytest.skip("FOUNDRY_RELAY_URL not configured")

        items = fetch_items_by_type('weapon', use_two_letter_fallback=False)

        assert len(items) > 0

        # Verify these are weapons
        for item in items[:5]:
            assert 'uuid' in item
            assert 'name' in item
</file>

<file path="tests/foundry/test_journals.py">
"""Tests for JournalManager class."""

import pytest
from unittest.mock import Mock, patch
from src.foundry.journals import JournalManager


class TestJournalManagerInit:
    """Tests for JournalManager initialization."""

    def test_journal_manager_initialization(self):
        """Test JournalManager initializes with correct attributes."""
        manager = JournalManager(
            relay_url="https://relay.example.com",
            foundry_url="http://localhost:30000",
            api_key="test-api-key",
            client_id="test-client-id"
        )

        assert manager.relay_url == "https://relay.example.com"
        assert manager.foundry_url == "http://localhost:30000"
        assert manager.api_key == "test-api-key"
        assert manager.client_id == "test-client-id"


class TestJournalManagerOperations:
    """Tests for journal operations via JournalManager."""

    @pytest.fixture
    def manager(self):
        """Create a JournalManager instance for testing."""
        return JournalManager(
            relay_url="https://relay.example.com",
            foundry_url="http://localhost:30000",
            api_key="test-key",
            client_id="test-client-id"
        )

    @patch('requests.post')
    def test_create_journal_entry_with_pages(self, mock_post, manager):
        """Test creating a journal entry with multiple pages."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "journal123"},
            "uuid": "JournalEntry.journal123"
        }
        mock_post.return_value = mock_response

        pages = [
            {"name": "Chapter 1", "content": "<h1>Chapter 1</h1>"},
            {"name": "Chapter 2", "content": "<h1>Chapter 2</h1>"}
        ]

        result = manager.create_journal_entry(
            name="Test Module",
            pages=pages
        )

        assert result["entity"]["_id"] == "journal123"
        mock_post.assert_called_once()

        # Verify payload structure
        call_kwargs = mock_post.call_args[1]
        payload = call_kwargs["json"]
        assert payload["entityType"] == "JournalEntry"
        assert payload["data"]["name"] == "Test Module"
        assert len(payload["data"]["pages"]) == 2
        assert payload["data"]["pages"][0]["name"] == "Chapter 1"
        assert payload["data"]["pages"][0]["type"] == "text"
        assert payload["data"]["pages"][0]["text"]["content"] == "<h1>Chapter 1</h1>"

    @patch('requests.post')
    def test_create_journal_entry_with_content(self, mock_post, manager):
        """Test creating a journal entry with legacy content parameter."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "journal123"},
            "uuid": "JournalEntry.journal123"
        }
        mock_post.return_value = mock_response

        result = manager.create_journal_entry(
            name="Test Journal",
            content="<p>Test content</p>"
        )

        assert result["entity"]["_id"] == "journal123"

        # Verify it creates a single-page journal
        call_kwargs = mock_post.call_args[1]
        payload = call_kwargs["json"]
        assert len(payload["data"]["pages"]) == 1
        assert payload["data"]["pages"][0]["name"] == "Test Journal"
        assert payload["data"]["pages"][0]["text"]["content"] == "<p>Test content</p>"

    @pytest.mark.integration
    @patch('requests.post')
    def test_create_journal_entry_with_folder(self, mock_post, manager):
        """Test creating a journal entry with folder parameter."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"entity": {"_id": "journal123"}}
        mock_post.return_value = mock_response

        result = manager.create_journal_entry(
            name="Test Journal",
            content="<p>Content</p>",
            folder="folder123"
        )

        # Verify folder is in payload
        call_kwargs = mock_post.call_args[1]
        payload = call_kwargs["json"]
        assert payload["data"]["folder"] == "folder123"

    def test_create_journal_entry_requires_content_or_pages(self, manager):
        """Test that create_journal_entry requires either pages or content."""
        with pytest.raises(ValueError, match="Must provide either 'pages' or 'content'"):
            manager.create_journal_entry(name="Test Journal")

    @patch('requests.post')
    def test_create_journal_entry_handles_api_error(self, mock_post, manager):
        """Test that create_journal_entry raises on API error."""
        mock_response = Mock()
        mock_response.status_code = 500
        mock_response.text = "Internal server error"
        mock_post.return_value = mock_response

        with pytest.raises(RuntimeError, match="Failed to create journal entry"):
            manager.create_journal_entry(
                name="Test Journal",
                content="<p>Content</p>"
            )

    @patch('requests.get')
    def test_get_journal_by_name_success(self, mock_get, manager):
        """Test finding a journal entry by name."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = [
            {"id": "journal123", "name": "Test Journal"},
            {"id": "journal456", "name": "Other Journal"}
        ]
        mock_get.return_value = mock_response

        result = manager.get_journal_by_name("Test Journal")

        assert result is not None
        assert result["_id"] == "journal123"  # id normalized to _id
        assert result["name"] == "Test Journal"

    @patch('requests.get')
    def test_get_journal_by_name_not_found(self, mock_get, manager):
        """Test that get_journal_by_name returns None when not found."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = []
        mock_get.return_value = mock_response

        result = manager.get_journal_by_name("Nonexistent")

        assert result is None

    @patch('requests.get')
    def test_get_journal_by_name_handles_dict_response(self, mock_get, manager):
        """Test that get_journal_by_name handles dict response format."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "results": [
                {"id": "journal123", "name": "Test Journal"}
            ]
        }
        mock_get.return_value = mock_response

        result = manager.get_journal_by_name("Test Journal")

        assert result is not None
        assert result["_id"] == "journal123"

    @patch('requests.get')
    def test_get_journal_by_name_handles_search_error(self, mock_get, manager):
        """Test that get_journal_by_name returns None on search error."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"error": "QuickInsert module not available"}
        mock_get.return_value = mock_response

        result = manager.get_journal_by_name("Test")

        assert result is None

    @patch('requests.get')
    def test_get_journal_success(self, mock_get, manager):
        """Test getting a journal entry by UUID."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "data": {
                "name": "Test Journal",
                "pages": [{"name": "Page 1", "text": {"content": "<p>Content</p>"}}]
            }
        }
        mock_get.return_value = mock_response

        result = manager.get_journal("JournalEntry.journal123")

        assert result["data"]["name"] == "Test Journal"
        assert len(result["data"]["pages"]) == 1

        # Verify URL construction
        call_args = mock_get.call_args
        url = call_args[0][0]
        assert "uuid=JournalEntry.journal123" in url

    @patch('requests.get')
    def test_get_journal_handles_error(self, mock_get, manager):
        """Test that get_journal raises on API error."""
        mock_response = Mock()
        mock_response.status_code = 404
        mock_response.text = "Journal not found"
        mock_get.return_value = mock_response

        with pytest.raises(RuntimeError, match="Failed to get journal"):
            manager.get_journal("JournalEntry.nonexistent")

    @patch('requests.put')
    def test_update_journal_entry_with_pages(self, mock_put, manager):
        """Test updating a journal entry with multiple pages."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"_id": "journal123"}
        mock_put.return_value = mock_response

        pages = [
            {"name": "Updated Page 1", "content": "<h1>Updated</h1>"},
            {"name": "Updated Page 2", "content": "<h2>Updated</h2>"}
        ]

        result = manager.update_journal_entry(
            journal_uuid="JournalEntry.journal123",
            pages=pages,
            name="Updated Journal"
        )

        assert result["_id"] == "journal123"

        # Verify payload
        call_kwargs = mock_put.call_args[1]
        payload = call_kwargs["json"]
        assert len(payload["data"]["pages"]) == 2
        assert payload["data"]["name"] == "Updated Journal"

    @patch('requests.put')
    def test_update_journal_entry_with_content(self, mock_put, manager):
        """Test updating a journal entry with legacy content parameter."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"_id": "journal123"}
        mock_put.return_value = mock_response

        result = manager.update_journal_entry(
            journal_uuid="JournalEntry.journal123",
            content="<p>Updated content</p>",
            name="Updated Name"
        )

        # Verify single-page update
        call_kwargs = mock_put.call_args[1]
        payload = call_kwargs["json"]
        assert len(payload["data"]["pages"]) == 1
        assert payload["data"]["pages"][0]["name"] == "Updated Name"

    @patch('requests.delete')
    def test_delete_journal_entry_success(self, mock_delete, manager):
        """Test deleting a journal entry."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"success": True}
        mock_delete.return_value = mock_response

        result = manager.delete_journal_entry("JournalEntry.journal123")

        assert result["success"] is True

        # Verify URL construction
        call_args = mock_delete.call_args
        url = call_args[0][0]
        assert "uuid=JournalEntry.journal123" in url

    @patch('requests.delete')
    def test_delete_journal_entry_handles_error(self, mock_delete, manager):
        """Test that delete_journal_entry raises on API error."""
        mock_response = Mock()
        mock_response.status_code = 500
        mock_response.text = "Server error"
        mock_delete.return_value = mock_response

        with pytest.raises(RuntimeError, match="Failed to delete journal"):
            manager.delete_journal_entry("JournalEntry.journal123")

    @patch('requests.post')
    @patch('requests.get')
    def test_create_or_replace_creates_when_not_found(self, mock_get, mock_post, manager):
        """Test create_or_replace creates new journal when not found."""
        # Search returns no results
        mock_search_response = Mock()
        mock_search_response.status_code = 200
        mock_search_response.json.return_value = []
        mock_get.return_value = mock_search_response

        # Create succeeds
        mock_create_response = Mock()
        mock_create_response.status_code = 200
        mock_create_response.json.return_value = {
            "entity": {"_id": "new123"},
            "uuid": "JournalEntry.new123"
        }
        mock_post.return_value = mock_create_response

        result = manager.create_or_replace_journal(
            name="New Journal",
            content="<p>Content</p>"
        )

        assert result["entity"]["_id"] == "new123"
        mock_get.assert_called_once()
        mock_post.assert_called_once()

    @patch('requests.delete')
    @patch('requests.post')
    @patch('requests.get')
    def test_create_or_replace_deletes_and_creates_when_found(self, mock_get, mock_post, mock_delete, manager):
        """Test create_or_replace deletes existing journal and creates new one."""
        # Search returns existing journal
        mock_search_response = Mock()
        mock_search_response.status_code = 200
        mock_search_response.json.return_value = [
            {
                "name": "Existing Journal",
                "id": "existing123",
                "uuid": "JournalEntry.existing123"
            }
        ]
        mock_get.return_value = mock_search_response

        # Delete succeeds
        mock_delete_response = Mock()
        mock_delete_response.status_code = 200
        mock_delete_response.json.return_value = {"success": True}
        mock_delete.return_value = mock_delete_response

        # Create succeeds
        mock_create_response = Mock()
        mock_create_response.status_code = 200
        mock_create_response.json.return_value = {
            "entity": {"_id": "new123"},
            "uuid": "JournalEntry.new123"
        }
        mock_post.return_value = mock_create_response

        result = manager.create_or_replace_journal(
            name="Existing Journal",
            content="<p>Updated content</p>"
        )

        assert result["entity"]["_id"] == "new123"
        mock_get.assert_called_once()
        mock_delete.assert_called_once()
        mock_post.assert_called_once()

        # Verify delete was called with correct UUID
        delete_call_args = mock_delete.call_args
        delete_url = delete_call_args[0][0]
        assert "uuid=JournalEntry.existing123" in delete_url

    @patch('requests.post')
    @patch('requests.get')
    def test_create_or_replace_constructs_uuid_from_id(self, mock_get, mock_post, manager):
        """Test create_or_replace constructs UUID when not provided in search results."""
        # Search returns journal with id but no uuid
        mock_search_response = Mock()
        mock_search_response.status_code = 200
        mock_search_response.json.return_value = [
            {
                "name": "Test Journal",
                "id": "test456",
                "_id": "test456"
            }
        ]
        mock_get.return_value = mock_search_response

        # Delete succeeds
        with patch('requests.delete') as mock_delete:
            mock_delete_response = Mock()
            mock_delete_response.status_code = 200
            mock_delete_response.json.return_value = {"success": True}
            mock_delete.return_value = mock_delete_response

            # Create succeeds
            mock_create_response = Mock()
            mock_create_response.status_code = 200
            mock_create_response.json.return_value = {
                "entity": {"_id": "new123"}
            }
            mock_post.return_value = mock_create_response

            manager.create_or_replace_journal(
                name="Test Journal",
                content="<p>Content</p>"
            )

            # Verify constructed UUID is used
            delete_call_args = mock_delete.call_args
            delete_url = delete_call_args[0][0]
            assert "uuid=JournalEntry.test456" in delete_url

    def test_create_or_replace_requires_content_or_pages(self, manager):
        """Test that create_or_replace requires either pages or content."""
        with pytest.raises(ValueError, match="Must provide either 'pages' or 'content'"):
            manager.create_or_replace_journal(name="Test Journal")
</file>

<file path="tests/foundry/test_upload_script.py">
"""Tests for upload_journal_to_foundry script."""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from src.foundry.upload_journal_to_foundry import (
    find_latest_run,
    find_xml_directory,
    upload_run_to_foundry,
    upload_scene_gallery
)


class TestUploadScript:
    """Tests for upload script functions."""

    def test_find_latest_run(self, tmp_path):
        """Test finding the most recent run directory."""
        output_dir = tmp_path / "output" / "runs"
        output_dir.mkdir(parents=True)

        # Create mock run directories
        (output_dir / "20250101_120000").mkdir()
        (output_dir / "20250102_120000").mkdir()
        (output_dir / "20250103_120000").mkdir()

        latest = find_latest_run(str(tmp_path / "output" / "runs"))

        assert latest == str(output_dir / "20250103_120000")

    def test_find_xml_directory(self, tmp_path):
        """Test finding XML directory in run directory."""
        # Create run directory with documents folder
        run_dir = tmp_path / "run_20250101_120000"
        xml_dir = run_dir / "documents"
        xml_dir.mkdir(parents=True)

        # Create test XML files
        (xml_dir / "01_Chapter_One.xml").write_text("<chapter><title>Chapter 1</title></chapter>")
        (xml_dir / "02_Chapter_Two.xml").write_text("<chapter><title>Chapter 2</title></chapter>")

        found_dir = find_xml_directory(str(run_dir))

        assert found_dir == str(xml_dir)

    def test_find_xml_directory_no_xml_files(self, tmp_path):
        """Test finding XML directory raises error when no XML files exist."""
        run_dir = tmp_path / "run_20250101_120000"
        run_dir.mkdir(parents=True)

        with pytest.raises(ValueError, match="No XML files found"):
            find_xml_directory(str(run_dir))

    # NOTE: Upload integration tests moved to test_upload_journal.py
    # These tests now use the new XMLDocument -> Journal -> HTML workflow


class TestSceneGalleryUpload:
    """Tests for scene gallery upload function."""

    def test_upload_scene_gallery_no_gallery_file(self, tmp_path):
        """Test that upload_scene_gallery returns None when no gallery file exists."""
        run_dir = tmp_path / "run_20250101_120000"
        run_dir.mkdir()

        mock_client = Mock()
        mock_client.client_id = "test_client"

        result = upload_scene_gallery(mock_client, run_dir)

        assert result is None

    def test_upload_scene_gallery_with_images(self, tmp_path):
        """Test uploading scene gallery with images."""
        # Create run directory structure
        run_dir = tmp_path / "run_20250101_120000"
        scene_dir = run_dir / "scene_artwork"
        images_dir = scene_dir / "images"
        images_dir.mkdir(parents=True)

        # Create test image file
        test_image = images_dir / "scene_001_cave_entrance.png"
        test_image.write_bytes(b"fake_png_data")

        # Create gallery HTML with image reference
        gallery_html = """
<h1>Scene Gallery</h1>
<h2>Cave Entrance</h2>
<img src="images/scene_001_cave_entrance.png" alt="Cave Entrance" />
<p>A dark cave entrance.</p>
"""
        gallery_file = scene_dir / "scene_gallery.html"
        gallery_file.write_text(gallery_html)

        # Mock client
        mock_client = Mock()
        mock_client.client_id = "test_client"
        mock_client.upload_file = Mock()

        # Call function
        result = upload_scene_gallery(mock_client, run_dir)

        # Verify upload was called
        mock_client.upload_file.assert_called_once_with(
            str(test_image),
            "worlds/test_client/images/scene_001_cave_entrance.png"
        )

        # Verify result structure
        assert result is not None
        assert result["name"] == "Scene Gallery"
        assert result["type"] == "text"
        assert result["text"]["format"] == 1

        # Verify HTML paths were updated
        updated_html = result["text"]["content"]
        assert "worlds/test_client/images/scene_001_cave_entrance.png" in updated_html
        assert 'src="images/scene_001_cave_entrance.png"' not in updated_html

    def test_upload_scene_gallery_multiple_images(self, tmp_path):
        """Test uploading scene gallery with multiple images."""
        # Create run directory structure
        run_dir = tmp_path / "run_20250101_120000"
        scene_dir = run_dir / "scene_artwork"
        images_dir = scene_dir / "images"
        images_dir.mkdir(parents=True)

        # Create multiple test image files
        test_image_1 = images_dir / "scene_001_cave.png"
        test_image_1.write_bytes(b"fake_png_data_1")
        test_image_2 = images_dir / "scene_002_forest.jpg"
        test_image_2.write_bytes(b"fake_jpg_data_2")

        # Create gallery HTML with multiple image references
        gallery_html = """
<h1>Scene Gallery</h1>
<h2>Cave</h2>
<img src="images/scene_001_cave.png" alt="Cave" />
<h2>Forest</h2>
<img src="images/scene_002_forest.jpg" alt="Forest" />
"""
        gallery_file = scene_dir / "scene_gallery.html"
        gallery_file.write_text(gallery_html)

        # Mock client
        mock_client = Mock()
        mock_client.client_id = "test_client"
        mock_client.upload_file = Mock()

        # Call function
        result = upload_scene_gallery(mock_client, run_dir)

        # Verify both uploads were called
        assert mock_client.upload_file.call_count == 2

        # Verify result
        assert result is not None
        updated_html = result["text"]["content"]
        assert "worlds/test_client/images/scene_001_cave.png" in updated_html
        assert "worlds/test_client/images/scene_002_forest.jpg" in updated_html

    def test_upload_scene_gallery_no_images(self, tmp_path):
        """Test uploading scene gallery when images directory is empty."""
        # Create run directory structure (no images)
        run_dir = tmp_path / "run_20250101_120000"
        scene_dir = run_dir / "scene_artwork"
        images_dir = scene_dir / "images"
        images_dir.mkdir(parents=True)

        # Create gallery HTML
        gallery_html = "<h1>Scene Gallery</h1><p>No scenes found.</p>"
        gallery_file = scene_dir / "scene_gallery.html"
        gallery_file.write_text(gallery_html)

        # Mock client
        mock_client = Mock()
        mock_client.client_id = "test_client"

        # Call function
        result = upload_scene_gallery(mock_client, run_dir)

        # Verify no uploads
        mock_client.upload_file.assert_not_called()

        # Verify result still created
        assert result is not None
        assert result["name"] == "Scene Gallery"
        assert result["text"]["content"] == gallery_html

    # NOTE: Scene gallery integration test moved to test_upload_journal.py
</file>

<file path="tests/integration/__init__.py">
"""Integration tests for end-to-end workflows."""
</file>

<file path="tests/integration/test_image_insertion_e2e.py">
"""End-to-end test for automatic image insertion."""

import pytest
import json
from pathlib import Path
from PIL import Image


@pytest.mark.integration
def test_full_pipeline_with_image_insertion(tmp_path):
    """Test complete workflow: XML ‚Üí Journal ‚Üí positioned images ‚Üí HTML."""
    # Setup run directory
    run_dir = tmp_path / "run"
    docs_dir = run_dir / "documents"
    maps_dir = run_dir / "map_assets" / "images"
    scenes_dir = run_dir / "scene_artwork" / "images"

    docs_dir.mkdir(parents=True)
    maps_dir.mkdir(parents=True)
    scenes_dir.mkdir(parents=True)

    # Copy XML fixture
    import shutil
    shutil.copy("tests/fixtures/sample_chapter.xml", docs_dir / "chapter_01.xml")

    # Create map metadata
    maps_metadata = {
        "maps": [
            {
                "name": "Goblin Ambush",
                "page_num": 5,
                "type": "battle_map",
                "source": "extracted"
            }
        ]
    }
    with open(run_dir / "map_assets" / "maps_metadata.json", "w") as f:
        json.dump(maps_metadata, f)

    # Create scene metadata
    scenes_metadata = {
        "scenes": [
            {
                "section_path": "Chapter 1: Goblin Arrows ‚Üí Goblin Ambush",
                "name": "Forest Road",
                "description": "Dense forest",
                "location_type": "outdoor",
                "image_file": "images/scene_001_forest_road.png"
            },
            {
                "section_path": "Chapter 1: Goblin Arrows ‚Üí The Cragmaw Hideout ‚Üí Area 1: Cave Entrance",
                "name": "Cave Entrance",
                "description": "Rocky cave",
                "location_type": "underground",
                "image_file": "images/scene_002_cave_entrance.png"
            }
        ]
    }
    with open(run_dir / "scene_artwork" / "scenes_metadata.json", "w") as f:
        json.dump(scenes_metadata, f)

    # Create mock images
    img = Image.new('RGB', (100, 100), color='red')
    img.save(maps_dir / "page_005_goblin_ambush.png")

    img = Image.new('RGB', (100, 100), color='blue')
    img.save(scenes_dir / "scene_001_forest_road.png")
    img.save(scenes_dir / "scene_002_cave_entrance.png")

    # Load and position images
    from foundry.upload_journal_to_foundry import load_and_position_images
    journal = load_and_position_images(run_dir)

    # Verify all images in registry
    assert "page_005_goblin_ambush" in journal.image_registry
    assert "scene_forest_road" in journal.image_registry
    assert "scene_cave_entrance" in journal.image_registry

    # Verify positioned images have insert locations
    # (Note: Original image_ref elements from XML may not have positions)
    assert journal.image_registry["page_005_goblin_ambush"].insert_before_content_id is not None
    assert journal.image_registry["scene_forest_road"].insert_before_content_id is not None
    assert journal.image_registry["scene_cave_entrance"].insert_before_content_id is not None

    # Render HTML
    image_mapping = {
        "page_005_goblin_ambush": "https://example.com/map.png",
        "scene_forest_road": "https://example.com/scene1.png",
        "scene_cave_entrance": "https://example.com/scene2.png"
    }

    html = journal.to_foundry_html(image_mapping)

    # Verify images appear in HTML
    assert "https://example.com/map.png" in html
    assert "https://example.com/scene1.png" in html
    assert "https://example.com/scene2.png" in html

    # Verify structure (headings before images)
    assert "<h1>Chapter 1: Goblin Arrows</h1>" in html
    assert "<h2>Goblin Ambush</h2>" in html
</file>

<file path="tests/models/__init__.py">
"""Tests for model modules."""
</file>

<file path="tests/pdf_processing/test_pdf_to_xml.py">
"""
Tests for src/pdf_processing/pdf_to_xml.py

Tests PDF to XML conversion including:
- XML element name sanitization
- Word counting and validation
- Text extraction (embedded and OCR)
- XML structure validation
- Error handling and retry logic
- Gemini API integration (actual API calls)

NOTE: These tests make REAL Gemini API calls and will consume API quota.
Run with caution and ensure you have API key configured.
"""

import pytest
import xml.etree.ElementTree as ET
import fitz
from pathlib import Path
import sys
import os

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from pdf_processing.pdf_to_xml import (
    sanitize_xml_element_name,
    count_words,
    get_word_frequencies,
    is_text_legible,
    configure_gemini,
    get_legible_text_from_page,
    get_xml_for_page,
    validate_xml_with_model
)
from models.xml_document import XMLDocument


class TestXMLSanitization:
    """Test XML element name sanitization."""

    def test_sanitize_numeric_prefix(self):
        """Test that numeric prefixes are handled."""
        assert sanitize_xml_element_name("01_Introduction") == "Chapter_01_Introduction"
        assert sanitize_xml_element_name("08_Appendix_B_Monsters") == "Chapter_08_Appendix_B_Monsters"
        assert sanitize_xml_element_name("123_Test") == "Chapter_123_Test"

    def test_sanitize_valid_names(self):
        """Test that valid names are not modified."""
        assert sanitize_xml_element_name("Introduction") == "Introduction"
        assert sanitize_xml_element_name("Appendix_A") == "Appendix_A"
        assert sanitize_xml_element_name("Part_1") == "Part_1"
        assert sanitize_xml_element_name("_Test") == "_Test"

    def test_sanitize_empty_string(self):
        """Test handling of edge cases."""
        # Empty string or None should be handled gracefully
        result = sanitize_xml_element_name("")
        assert isinstance(result, str)


class TestWordCounting:
    """Test word counting functionality."""

    def test_count_simple_text(self):
        """Test counting words in simple text."""
        assert count_words("hello world") == 2
        assert count_words("one two three four five") == 5
        assert count_words("") == 0

    def test_count_with_xml_tags(self):
        """Test that XML tags are excluded from word count."""
        xml_text = "<paragraph>hello world</paragraph>"
        assert count_words(xml_text) == 2

        xml_text = "<page><heading>Test</heading><paragraph>one two three</paragraph></page>"
        # "Test" + "one" + "two" + "three" but regex counts differently
        word_count = count_words(xml_text)
        assert word_count >= 3  # At minimum, should count the main words

    def test_count_with_punctuation(self):
        """Test word counting with punctuation."""
        assert count_words("Hello, world!") == 2
        # Note: \b\w+\b regex treats "It's" as "It" and "s" (2 words)
        assert count_words("It's a test.") == 4  # "It", "s", "a", "test"

    def test_word_frequencies(self):
        """Test word frequency counting."""
        text = "the quick brown fox jumps over the lazy dog the"
        frequencies = get_word_frequencies(text)

        assert frequencies["the"] == 3
        assert frequencies["quick"] == 1
        assert frequencies["brown"] == 1

    def test_word_frequencies_case_insensitive(self):
        """Test that word frequencies are case-insensitive."""
        text = "The THE the"
        frequencies = get_word_frequencies(text)

        assert frequencies["the"] == 3


class TestTextLegibility:
    """Test text legibility checking."""

    def test_legible_text(self):
        """Test that normal text is considered legible."""
        normal_text = "This is a normal paragraph with reasonable word lengths."
        assert is_text_legible(normal_text) == True

    def test_illegible_text(self):
        """Test that corrupted text is detected."""
        # Create text with many overly long "words" (simulating corrupted text)
        corrupted_text = " ".join(["x" * 25 for _ in range(15)])
        assert is_text_legible(corrupted_text) == False

    def test_mixed_legibility(self):
        """Test text with some long words."""
        # Text with a few long words should still be legible
        mixed_text = "Normal words here pneumonoultramicroscopicsilicovolcanoconiosis and more"
        assert is_text_legible(mixed_text) == True


class TestTextExtraction:
    """Test text extraction from PDF pages."""

    @pytest.mark.requires_pdf
    def test_extract_embedded_text(self, test_pdf_path, test_output_dir):
        """Test extraction of embedded text from PDF."""
        doc = fitz.open(test_pdf_path)

        # Extract first page
        page_bytes = fitz.open()
        page_bytes.insert_pdf(doc, from_page=0, to_page=0)
        page_data = page_bytes.write()
        page_bytes.close()

        # Test extraction
        os.makedirs(test_output_dir / "pages", exist_ok=True)
        text, source = get_legible_text_from_page(page_data, 1, str(test_output_dir))

        assert isinstance(text, str)
        assert len(text) > 0
        assert source in ["embedded", "ocr", "ocr_failed"]

        doc.close()

    @pytest.mark.requires_pdf
    def test_text_extraction_creates_logs(self, test_pdf_path, test_output_dir):
        """Test that text extraction creates log files."""
        doc = fitz.open(test_pdf_path)

        page_bytes = fitz.open()
        page_bytes.insert_pdf(doc, from_page=0, to_page=0)
        page_data = page_bytes.write()
        page_bytes.close()

        os.makedirs(test_output_dir / "pages", exist_ok=True)
        text, source = get_legible_text_from_page(page_data, 1, str(test_output_dir))

        # Check that output files were created
        if source == "embedded":
            assert (test_output_dir / "pages" / "page_1_embedded.txt").exists()
        elif source == "ocr":
            assert (test_output_dir / "pages" / "page_1_ocr.txt").exists()

        doc.close()


@pytest.mark.integration
@pytest.mark.slow
@pytest.mark.requires_api
class TestGeminiIntegration:
    """Test Gemini API integration (makes real API calls)."""

    def test_gemini_configuration(self, check_api_key):
        """Test that Gemini API can be configured."""
        configure_gemini()
        # If this doesn't raise an exception, API key is configured

    @pytest.mark.slow
    def test_single_page_xml_generation(self, test_pdf_path, test_output_dir, check_api_key):
        """
        Test XML generation for a single page using Gemini API.

        WARNING: This test makes a REAL API call and will consume quota.
        """
        from util.gemini import GeminiFileContext
        gemini_api = configure_gemini()

        doc = fitz.open(test_pdf_path)

        # Extract first page
        page_bytes = fitz.open()
        page_bytes.insert_pdf(doc, from_page=0, to_page=0)
        page_data = page_bytes.write()
        page_bytes.close()

        # Create required directory structure
        log_dir = test_output_dir / "test_chapter"
        os.makedirs(log_dir / "pages", exist_ok=True)

        # Upload PDF and generate XML for this page
        with GeminiFileContext(gemini_api, test_pdf_path, "test_pdf") as uploaded_pdf:
            page_info = (page_data, 1, str(log_dir), uploaded_pdf)
            xml_result = get_xml_for_page(page_info)

        # Validate result
        assert xml_result is not None
        assert len(xml_result) > 0

        # Parse XML to verify it's valid
        root = ET.fromstring(xml_result)
        assert root.tag == "page"

        # Check that XML file was created
        assert (log_dir / "pages" / "page_1.xml").exists()

        doc.close()

    @pytest.mark.slow
    def test_xml_generation_word_count_validation(self, test_pdf_path, test_output_dir, check_api_key):
        """
        Test that generated XML word count is validated.

        WARNING: This test makes a REAL API call.
        """
        from util.gemini import GeminiFileContext
        gemini_api = configure_gemini()

        doc = fitz.open(test_pdf_path)

        # Extract first page
        page_bytes = fitz.open()
        page_bytes.insert_pdf(doc, from_page=0, to_page=0)
        page_data = page_bytes.write()
        page_bytes.close()

        log_dir = test_output_dir / "test_chapter_wordcount"
        os.makedirs(log_dir / "pages", exist_ok=True)

        # Extract text to get word count
        text, _ = get_legible_text_from_page(page_data, 1, str(log_dir))
        pdf_word_count = count_words(text)

        # Upload PDF and generate XML
        with GeminiFileContext(gemini_api, test_pdf_path, "test_pdf") as uploaded_pdf:
            page_info = (page_data, 1, str(log_dir), uploaded_pdf)
            xml_result = get_xml_for_page(page_info)

        # Check XML word count
        xml_word_count = count_words(xml_result)

        # Word counts should be reasonably close (within 15% threshold if > 30 words)
        if pdf_word_count >= 30:
            difference = abs(pdf_word_count - xml_word_count)
            percentage_diff = (difference / pdf_word_count) * 100
            assert percentage_diff <= 20, \
                f"Word count difference too large: PDF={pdf_word_count}, XML={xml_word_count}"

        doc.close()


class TestErrorHandling:
    """Test error handling and edge cases."""

    def test_word_count_with_empty_string(self):
        """Test word counting with empty strings."""
        assert count_words("") == 0
        assert count_words("   ") == 0

    def test_sanitize_xml_handles_unicode(self):
        """Test XML sanitization with Unicode characters."""
        unicode_name = "01_Introducci√≥n"
        result = sanitize_xml_element_name(unicode_name)
        assert result.startswith("Chapter_")
        assert "Introducci√≥n" in result or "Introducci" in result  # Allow encoding variations


class TestXMLDocumentValidation:
    """Test XMLDocument model validation."""

    def test_validate_valid_xml(self):
        """Test that valid XML passes XMLDocument validation."""
        valid_xml = """<Chapter_01>
  <page number="1">
    <chapter_title>Introduction</chapter_title>
    <p>This is a paragraph with **bold** and *italic* text.</p>
    <section>A Section</section>
    <p>More content here.</p>
  </page>
</Chapter_01>"""

        # Should not raise any exception
        is_valid, error_msg = validate_xml_with_model(valid_xml)
        assert is_valid == True
        assert error_msg is None

    def test_validate_invalid_xml_malformed(self):
        """Test that malformed XML fails validation."""
        invalid_xml = """<Chapter_01>
  <page number="1">
    <p>Unclosed paragraph
  </page>
</Chapter_01>"""

        is_valid, error_msg = validate_xml_with_model(invalid_xml)
        assert is_valid == False
        assert error_msg is not None
        assert "parse" in error_msg.lower() or "xml" in error_msg.lower()

    def test_validate_xml_with_table(self):
        """Test validation with table structure."""
        xml_with_table = """<Chapter_Test>
  <page number="1">
    <section>Data Table</section>
    <table>
      <row>
        <cell>Header 1</cell>
        <cell>Header 2</cell>
      </row>
      <row>
        <cell>Data 1</cell>
        <cell>Data 2</cell>
      </row>
    </table>
  </page>
</Chapter_Test>"""

        is_valid, error_msg = validate_xml_with_model(xml_with_table)
        assert is_valid == True
        assert error_msg is None

    def test_validate_xml_with_list(self):
        """Test validation with list structure."""
        xml_with_list = """<Chapter_Test>
  <page number="1">
    <section>Items</section>
    <list type="unordered">
      <item>First item</item>
      <item>Second item</item>
      <item>Third item</item>
    </list>
  </page>
</Chapter_Test>"""

        is_valid, error_msg = validate_xml_with_model(xml_with_list)
        assert is_valid == True
        assert error_msg is None

    def test_validate_xml_with_stat_block(self):
        """Test validation with stat block."""
        xml_with_stat_block = """<Chapter_Test>
  <page number="1">
    <section>Monsters</section>
    <stat_block name="Goblin">
GOBLIN
Small humanoid (goblinoid), neutral evil

Armor Class 15 (leather armor, shield)
Hit Points 7 (2d6)
Speed 30 ft.

STR     DEX     CON     INT     WIS     CHA
8 (-1)  14 (+2) 10 (+0) 10 (+0) 8 (-1)  8 (-1)

Challenge 1/4 (50 XP)
    </stat_block>
  </page>
</Chapter_Test>"""

        is_valid, error_msg = validate_xml_with_model(xml_with_stat_block)
        assert is_valid == True
        assert error_msg is None

    def test_validate_xml_missing_page_number(self):
        """Test validation when page number is missing."""
        xml_missing_page_num = """<Chapter_Test>
  <page>
    <p>Content without page number</p>
  </page>
</Chapter_Test>"""

        # This should still be valid - page number defaults to "1"
        is_valid, error_msg = validate_xml_with_model(xml_missing_page_num)
        assert is_valid == True
        assert error_msg is None
</file>

<file path="tests/scene_extraction/test_extract_context.py">
"""Tests for chapter context extraction."""

import pytest
from unittest.mock import patch, MagicMock
from src.scene_extraction.extract_context import extract_chapter_context
from src.scene_extraction.models import ChapterContext


@pytest.fixture
def sample_xml_content():
    """Sample XML content for testing."""
    return """
    <chapter name="The Cragmaw Hideout">
        <section name="Overview">
            <p>Deep in the forest, a cave system serves as the hideout for Cragmaw goblins.</p>
        </section>
        <section name="Area 1">
            <p>The cave entrance is dark and foreboding, with rough stone walls.</p>
        </section>
    </chapter>
    """


class TestExtractChapterContext:
    """Tests for extract_chapter_context function."""

    @pytest.mark.integration
    def test_extract_context_calls_gemini(self, sample_xml_content):
        """Test that extract_chapter_context calls Gemini API with correct prompt."""
        with patch('src.scene_extraction.extract_context.genai.Client') as mock_client_class:
            # Mock Gemini response
            mock_response = MagicMock()
            mock_response.text = """
            {
                "environment_type": "underground",
                "weather": "dry",
                "atmosphere": "oppressive",
                "lighting": "dim",
                "terrain": "rocky caverns",
                "additional_notes": "Forest cave system"
            }
            """
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            # Call function
            context = extract_chapter_context(sample_xml_content)

            # Verify Gemini was called
            mock_client.models.generate_content.assert_called_once()

            # Verify result is ChapterContext
            assert isinstance(context, ChapterContext)
            assert context.environment_type == "underground"
            assert context.terrain == "rocky caverns"

    def test_extract_context_handles_json_parsing(self):
        """Test that extract_context properly parses Gemini JSON response."""
        with patch('src.scene_extraction.extract_context.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '{"environment_type": "forest", "lighting": "dappled sunlight"}'
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            context = extract_chapter_context("<chapter></chapter>")

            assert context.environment_type == "forest"
            assert context.lighting == "dappled sunlight"

    def test_extract_context_raises_on_invalid_json(self):
        """Test that extract_context raises error on malformed JSON."""
        with patch('src.scene_extraction.extract_context.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = "Not valid JSON at all"
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            with pytest.raises(ValueError, match="Failed to parse.*JSON"):
                extract_chapter_context("<chapter></chapter>")

    def test_extract_context_handles_markdown_json_same_line(self):
        """Test parsing when json identifier is on same line as backticks: ```json"""
        with patch('src.scene_extraction.extract_context.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '''```json
{"environment_type": "forest", "lighting": "dappled sunlight"}
```'''
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            context = extract_chapter_context("<chapter></chapter>")

            assert context.environment_type == "forest"
            assert context.lighting == "dappled sunlight"

    def test_extract_context_handles_markdown_json_separate_line(self):
        """Test parsing when json identifier is on separate line after backticks."""
        with patch('src.scene_extraction.extract_context.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '''```
json
{"environment_type": "underground", "atmosphere": "oppressive"}
```'''
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            context = extract_chapter_context("<chapter></chapter>")

            assert context.environment_type == "underground"
            assert context.atmosphere == "oppressive"

    def test_extract_context_handles_markdown_no_json_identifier(self):
        """Test parsing when markdown blocks have no json identifier."""
        with patch('src.scene_extraction.extract_context.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '''```
{"environment_type": "coastal", "weather": "foggy"}
```'''
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            context = extract_chapter_context("<chapter></chapter>")

            assert context.environment_type == "coastal"
            assert context.weather == "foggy"
</file>

<file path="tests/scene_extraction/test_full_workflow.py">
"""End-to-end tests for scene extraction workflow."""

import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock
from src.scene_extraction import (
    extract_chapter_context,
    identify_scene_locations,
    generate_scene_image,
    save_scene_image,
    create_scene_gallery_html
)


@pytest.fixture
def test_xml_file(tmp_path):
    """Create test XML file."""
    xml_content = """
<chapter name="Test Chapter">
    <section name="Area 1" id="area_1">
        <p>A dark forest clearing with ancient trees.</p>
    </section>
</chapter>
"""
    xml_file = tmp_path / "test_chapter.xml"
    xml_file.write_text(xml_content)
    return xml_file


class TestSceneProcessingWorkflow:
    """End-to-end workflow tests."""

    @pytest.mark.integration
    def test_full_workflow_with_mocked_gemini(self, test_xml_file, tmp_path):
        """Test complete workflow with mocked Gemini calls."""
        xml_content = test_xml_file.read_text()
        output_dir = tmp_path / "output"
        output_dir.mkdir()

        # Patch genai.Client constructor (new API pattern)
        # NOTE: All scene_extraction modules share the same genai import,
        # so we only need to patch once
        with patch('google.genai.Client') as mock_client_class:

            # Create shared mock client that handles all three operations
            mock_client = MagicMock()

            # Set up context extraction response
            mock_context_response = MagicMock()
            mock_context_response.text = '{"environment_type": "forest", "lighting": "dappled sunlight"}'

            # Set up scene identification response
            mock_scenes_response = MagicMock()
            mock_scenes_response.text = '[{"section_path": "Test Chapter ‚Üí Area 1", "name": "Forest Clearing", "description": "A dark forest clearing", "location_type": "outdoor", "xml_section_id": "area_1"}]'

            # Mock generate_content to return appropriate response based on call count
            mock_client.models.generate_content.side_effect = [mock_context_response, mock_scenes_response]

            # Set up image generation response
            mock_pil_image = MagicMock()
            mock_generated_image = MagicMock()
            mock_generated_image.image._pil_image = mock_pil_image
            mock_gen_response = MagicMock()
            mock_gen_response.generated_images = [mock_generated_image]
            mock_client.models.generate_images.return_value = mock_gen_response

            mock_client_class.return_value = mock_client

            # Run workflow
            context = extract_chapter_context(xml_content)
            assert context.environment_type == "forest"

            scenes = identify_scene_locations(xml_content, context)
            assert len(scenes) == 1
            assert scenes[0].name == "Forest Clearing"

            # Mock BytesIO for image generation
            with patch('src.scene_extraction.generate_artwork.BytesIO') as mock_bytesio_class:
                mock_buffer = MagicMock()
                mock_buffer.getvalue.return_value = b"fake_png_data"
                mock_bytesio_class.return_value = mock_buffer

                image_bytes, prompt = generate_scene_image(scenes[0], context)
                assert image_bytes == b"fake_png_data"
                assert isinstance(prompt, str)

            image_path = output_dir / "scene_001_forest_clearing.png"
            save_scene_image(image_bytes, str(image_path))

            assert image_path.exists()
            assert image_path.read_bytes() == b"fake_png_data"

            # Create gallery HTML
            image_paths = {"Forest Clearing": str(image_path)}
            html = create_scene_gallery_html(scenes, image_paths)

            assert "Scene Gallery" in html
            assert "Forest Clearing" in html
            assert str(image_path) in html
</file>

<file path="tests/scene_extraction/test_identify_scenes.py">
"""Tests for scene location identification."""

import pytest
from unittest.mock import patch, MagicMock
from src.scene_extraction.identify_scenes import identify_scene_locations
from src.scene_extraction.models import Scene, ChapterContext


@pytest.fixture
def sample_xml_content():
    """Sample XML for testing."""
    return """
    <chapter name="The Cragmaw Hideout">
        <section name="Area 1" id="area_1">
            <p>The cave entrance is dark and foreboding.</p>
        </section>
    </chapter>
    """


@pytest.fixture
def sample_context():
    """Sample chapter context."""
    return ChapterContext(
        environment_type="underground",
        lighting="dim",
        terrain="rocky caverns"
    )


class TestIdentifySceneLocations:
    """Tests for identify_scene_locations function."""

    @pytest.mark.integration
    def test_identify_scenes_calls_gemini(self, sample_xml_content, sample_context):
        """Test that identify_scene_locations calls Gemini with XML and context."""
        with patch('src.scene_extraction.identify_scenes.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = """
            [
                {
                    "section_path": "Chapter 1 ‚Üí Area 1",
                    "name": "Cave Entrance",
                    "description": "Dark cave entrance with rough stone walls",
                    "location_type": "underground",
                    "xml_section_id": "area_1"
                }
            ]
            """
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            scenes = identify_scene_locations(sample_xml_content, sample_context)

            # Verify Gemini called
            mock_client.models.generate_content.assert_called_once()

            # Verify result
            assert len(scenes) == 1
            assert isinstance(scenes[0], Scene)
            assert scenes[0].name == "Cave Entrance"

    def test_identify_scenes_parses_json_array(self, sample_context):
        """Test parsing of JSON array response."""
        with patch('src.scene_extraction.identify_scenes.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '[{"section_path": "Ch1", "name": "Room", "description": "A room", "location_type": "interior"}]'
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            scenes = identify_scene_locations("<xml></xml>", sample_context)

            assert len(scenes) == 1
            assert scenes[0].name == "Room"

    def test_identify_scenes_returns_empty_list_on_no_scenes(self, sample_context):
        """Test that function returns empty list when no scenes found."""
        with patch('src.scene_extraction.identify_scenes.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = "[]"
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            scenes = identify_scene_locations("<xml></xml>", sample_context)

            assert scenes == []

    def test_identify_scenes_handles_markdown_json_same_line(self, sample_context):
        """Test parsing when json identifier is on same line as backticks: ```json"""
        with patch('src.scene_extraction.identify_scenes.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '''```json
[{"section_path": "Ch1", "name": "Forest Clearing", "description": "A sunlit clearing", "location_type": "outdoor"}]
```'''
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            scenes = identify_scene_locations("<xml></xml>", sample_context)

            assert len(scenes) == 1
            assert scenes[0].name == "Forest Clearing"
            assert scenes[0].description == "A sunlit clearing"

    def test_identify_scenes_handles_markdown_json_separate_line(self, sample_context):
        """Test parsing when json identifier is on separate line after backticks."""
        with patch('src.scene_extraction.identify_scenes.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '''```
json
[{"section_path": "Ch2", "name": "Underground Chamber", "description": "A dark stone chamber", "location_type": "underground"}]
```'''
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            scenes = identify_scene_locations("<xml></xml>", sample_context)

            assert len(scenes) == 1
            assert scenes[0].name == "Underground Chamber"
            assert scenes[0].description == "A dark stone chamber"

    def test_identify_scenes_handles_markdown_no_json_identifier(self, sample_context):
        """Test parsing when there's no json identifier, just backticks."""
        with patch('src.scene_extraction.identify_scenes.genai.Client') as mock_client_class:
            mock_response = MagicMock()
            mock_response.text = '''```
[{"section_path": "Ch3", "name": "City Street", "description": "Cobblestone street", "location_type": "outdoor"}]
```'''
            mock_client = MagicMock()
            mock_client.models.generate_content.return_value = mock_response
            mock_client_class.return_value = mock_client

            scenes = identify_scene_locations("<xml></xml>", sample_context)

            assert len(scenes) == 1
            assert scenes[0].name == "City Street"
            assert scenes[0].description == "Cobblestone street"
</file>

<file path="tests/scene_extraction/test_scene_metadata.py">
"""Tests for scene metadata preservation."""

import pytest
import json
from pathlib import Path


def test_generate_scene_art_saves_metadata(tmp_path):
    """Test that scene artwork generation saves metadata JSON."""
    from scripts.generate_scene_art import process_chapter

    # Use real XML fixture
    xml_file = Path("tests/fixtures/sample_chapter.xml")
    output_dir = tmp_path / "scene_artwork"

    # Process chapter
    result = process_chapter(xml_file, output_dir)

    # Verify metadata file exists
    metadata_file = output_dir / "scenes_metadata.json"
    assert metadata_file.exists()

    # Verify structure
    with open(metadata_file) as f:
        data = json.load(f)

    assert "scenes" in data
    assert "generated_at" in data
    assert len(data["scenes"]) > 0

    # Verify scene structure
    scene = data["scenes"][0]
    assert "section_path" in scene
    assert "name" in scene
    assert "description" in scene
    assert "image_file" in scene
</file>

<file path="ui/backend/app/tools/__init__.py">
"""Tool system initialization."""
from .base import BaseTool, ToolSchema, ToolResponse
from .registry import ToolRegistry, registry
from .image_generator import ImageGeneratorTool
from .actor_creator import ActorCreatorTool

# Auto-register tools
registry.register(ImageGeneratorTool())
registry.register(ActorCreatorTool())

__all__ = [
    'BaseTool',
    'ToolSchema',
    'ToolResponse',
    'ToolRegistry',
    'registry',
    'ImageGeneratorTool',
    'ActorCreatorTool'
]
</file>

<file path="src/actors/generate_actor_biography.py">
"""Generate flavorful biographies for D&D 5e creatures using Gemini."""

import asyncio
import logging
import os
from typing import Optional
from google import genai

from foundry.actors.models import ParsedActorData
from util.gemini import generate_content_async

logger = logging.getLogger(__name__)


async def generate_actor_biography(
    parsed_actor: ParsedActorData,
    model_name: str = "gemini-2.0-flash"
) -> str:
    """
    Generate a flavorful biography for a D&D 5e creature.

    Creates a descriptive paragraph about what the creature is like based on its
    stats, abilities, and special traits. This is NOT mechanical information, but
    rather flavor text describing the creature's nature, behavior, and appearance.

    Args:
        parsed_actor: The fully parsed actor data with stats, abilities, traits
        model_name: Gemini model to use (default: "gemini-2.0-flash")

    Returns:
        A 2-4 sentence biography/description of the creature

    Example:
        >>> bio = await generate_actor_biography(goblin_parsed_data)
        >>> print(bio)
        "Goblins are small, black-hearted, selfish humanoids that lair in caves,
         abandoned mines, despoiled dungeons, and other dismal settings. Individually
         weak, they gather in large numbers to torment other creatures."
    """
    logger.info(f"Generating biography for {parsed_actor.name}...")

    # Build a summary of the creature's key features
    summary_parts = []

    # Size and type
    if parsed_actor.size and parsed_actor.creature_type:
        summary_parts.append(f"Size: {parsed_actor.size}, Type: {parsed_actor.creature_type}")

    # Alignment
    if parsed_actor.alignment:
        summary_parts.append(f"Alignment: {parsed_actor.alignment}")

    # CR
    summary_parts.append(f"Challenge Rating: {parsed_actor.challenge_rating}")

    # Notable abilities
    ability_scores = []
    for ability, score in parsed_actor.abilities.items():
        modifier = (score - 10) // 2
        if modifier >= 3:  # Notable positive
            ability_scores.append(f"{ability.upper()} {score} (+{modifier})")
        elif modifier <= -2:  # Notable negative
            ability_scores.append(f"{ability.upper()} {score} ({modifier})")

    if ability_scores:
        summary_parts.append(f"Notable abilities: {', '.join(ability_scores)}")

    # Special traits (limit to 3)
    if parsed_actor.traits:
        trait_names = [t.name for t in parsed_actor.traits[:3]]
        summary_parts.append(f"Special traits: {', '.join(trait_names)}")

    # Attacks (limit to 3)
    if parsed_actor.attacks:
        attack_names = [a.name for a in parsed_actor.attacks[:3]]
        summary_parts.append(f"Attacks: {', '.join(attack_names)}")

    # Spellcasting
    if parsed_actor.spells:
        summary_parts.append(f"Spellcaster with {len(parsed_actor.spells)} spells")

    if parsed_actor.innate_spellcasting:
        summary_parts.append(f"Innate spellcasting ({parsed_actor.innate_spellcasting.ability})")

    # Build the prompt
    summary = "\n".join(f"- {part}" for part in summary_parts)

    prompt = f"""You are writing flavor text for a D&D 5e virtual tabletop.

Generate a SHORT biography (2-4 sentences) for the following creature. The biography should:
1. Describe what this creature IS (appearance, nature, behavior)
2. Be evocative and flavorful
3. NOT include mechanical stats or numbers
4. Sound like official D&D 5e monster manual descriptions

CREATURE NAME: {parsed_actor.name}

KEY FEATURES:
{summary}

Write ONLY the biography text (2-4 sentences), nothing else:"""

    try:
        # Initialize client and call Gemini
        client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))
        response = await generate_content_async(
            client=client,
            model=model_name,
            contents=prompt,
            config={
                'temperature': 0.7,  # Slightly creative
                'max_output_tokens': 200  # Keep it short
            }
        )

        biography = response.text.strip()
        logger.info(f"‚úì Generated biography for {parsed_actor.name} ({len(biography)} chars)")
        return biography

    except Exception as e:
        logger.error(f"Failed to generate biography for {parsed_actor.name}: {e}")
        # Return a generic fallback
        return f"A {parsed_actor.size or ''} {parsed_actor.creature_type or 'creature'} of challenge rating {parsed_actor.challenge_rating}."
</file>

<file path="src/actors/statblock_parser.py">
"""Gemini-powered parser for converting raw D&D 5e stat block text to StatBlock."""

import asyncio
import json
import logging
import os
from pathlib import Path
from typing import Optional

from google import genai
from dotenv import load_dotenv

from actors.models import StatBlock
from util.gemini import generate_content_async

# Load environment
PROJECT_ROOT = Path(__file__).parent.parent.parent
load_dotenv(PROJECT_ROOT / ".env")

logger = logging.getLogger(__name__)

# Model configuration
DEFAULT_MODEL = "gemini-2.0-flash"
PARSE_TEMPERATURE = 0.0  # Deterministic parsing


async def parse_raw_text_to_statblock(
    raw_text: str,
    model_name: str = DEFAULT_MODEL
) -> StatBlock:
    """
    Parse raw D&D 5e stat block text into StatBlock using Gemini.

    Args:
        raw_text: Complete stat block text
        model_name: Gemini model to use

    Returns:
        StatBlock with all fields extracted

    Example input:
        Giant Octopus
        Large Beast, Unaligned
        Armor Class 11
        Hit Points 52 (8d10 + 8)
        Speed 10 ft., swim 60 ft.
        STR 17 (+3)
        DEX 13 (+1)
        ...
        Skills Perception +4, Stealth +5
        ...
        Traits
        Hold Breath. While out of water...
        Actions
        Tentacles. Melee Weapon Attack: +5 to hit...
    """
    prompt = f"""Parse this D&D 5e stat block into JSON format.

STAT BLOCK TEXT:
{raw_text}

OUTPUT JSON SCHEMA:
{{
  "name": "string (creature name, e.g., 'Giant Octopus')",
  "armor_class": integer,
  "hit_points": integer,
  "challenge_rating": float (use 0.125 for CR 1/8, 0.25 for CR 1/4, 0.5 for CR 1/2),

  "size": "string (tiny, small, medium, large, huge, gargantuan) LOWERCASE",
  "type": "string (beast, humanoid, fiend, etc.) LOWERCASE",
  "alignment": "string (lawful evil, unaligned, etc.) LOWERCASE or null",

  "abilities": {{
    "STR": integer,
    "DEX": integer,
    "CON": integer,
    "INT": integer,
    "WIS": integer,
    "CHA": integer
  }},

  "speed": "string (e.g., '30 ft., fly 60 ft.') or null",
  "senses": "string (e.g., 'Darkvision 60 ft., Passive Perception 14') or null",
  "languages": "string (e.g., 'Infernal, Telepathy 120 ft.' or '--' for none) or null",

  "damage_resistances": "string (e.g., 'Fire, Cold; Bludgeoning from Nonmagical Attacks') or null",
  "damage_immunities": "string (e.g., 'Poison, Fire') or null",
  "damage_vulnerabilities": "string or null",
  "condition_immunities": "string (e.g., 'Poisoned, Charmed') or null",

  "saving_throws": {{
    "dex": integer,
    "con": integer,
    "wis": integer
  }} or null,

  "skills": {{
    "perception": integer,
    "stealth": integer,
    "intimidation": integer
  }} or null,

  "traits": [
    "string (complete trait entry from Traits section, e.g., 'Hold Breath. While out of water, the octopus can hold its breath for 1 hour.')"
  ],

  "actions": [
    "string (complete action entry from Actions section - includes both attacks AND special actions)",
    "string (e.g., 'Tentacles. Melee Weapon Attack: +5 to hit...' OR 'Ink Cloud (Recharges after a Short or Long Rest). A 20-foot-radius cloud...')"
  ],

  "reactions": [
    "string (complete reaction entry)"
  ],

  "legendary_actions": [
    "string (complete legendary action entry)"
  ]
}}

PARSING RULES:
1. Extract name from the first line
2. Extract size and type from line like "Large Beast, Unaligned"
3. Parse abilities from ability score block (STR, DEX, CON, INT, WIS, CHA)
4. For saving_throws: Extract from "Saving Throws DEX +8, CON +13" ‚Üí {{"dex": 8, "con": 13}}
   - Use LOWERCASE ability names (dex, con, wis, str, int, cha)
   - Include ONLY abilities listed in saving throws line
5. For skills: Extract from "Skills Perception +4, Stealth +5" ‚Üí {{"perception": 4, "stealth": 5}}
   - Use LOWERCASE skill names
   - Include ONLY skills explicitly listed
6. For traits/actions/reactions: Preserve the D&D 5e structure exactly as it appears:
   - traits: Everything listed under "Traits" section
   - actions: Everything listed under "Actions" section (both attacks AND special actions like Ink Cloud, Breath Weapon)
   - Each entry should start with the ability name followed by period and full description
   - Example: "Hold Breath. While out of water, the octopus can hold its breath for 1 hour."
7. If a section is missing (no Reactions, no Legendary Actions), use empty list []
8. If a field doesn't exist in the stat block, use null

OUTPUT ONLY VALID JSON. No explanations.
"""

    # Initialize client
    client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))

    response = await generate_content_async(
        client=client,
        model=model_name,
        contents=prompt,
        config={
            'temperature': PARSE_TEMPERATURE,
            'response_mime_type': 'application/json'
        }
    )

    # Parse response
    parsed_json = json.loads(response.text)
    logger.debug(f"Gemini parsed stat block: {parsed_json.get('name')}")

    # Handle case where Gemini returns a list instead of dict
    if isinstance(parsed_json, list):
        if len(parsed_json) == 0:
            raise ValueError(f"Gemini returned empty list for stat block")
        parsed_json = parsed_json[0]
        logger.warning(f"Gemini returned list instead of dict, using first element")

    # Add raw_text to the JSON
    parsed_json["raw_text"] = raw_text

    # Create StatBlock
    return StatBlock(**parsed_json)
</file>

<file path="src/pdf_processing/pdf_to_xml.py">
import os
from datetime import datetime
import concurrent.futures
import fitz  # PyMuPDF
import xml.etree.ElementTree as ET
import re
from typing import List, Tuple, Optional
import tempfile
import time
import io
from PIL import Image
import pytesseract
import json
from collections import Counter
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from logging_config import setup_logging, get_run_logger
from util.gemini import GeminiAPI, GeminiFileContext
from pdf_processing.valid_xml_tags import APPROVED_XML_TAGS, get_approved_tags_text
from models.xml_document import XMLDocument

# Project root is three levels up from the script's directory (pdf_processing -> src -> root)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Initialize logger (will be reconfigured in main() with run directory)
logger = setup_logging(__name__)

# Global Gemini API instance
gemini_api = None

def validate_xml_tags(xml_content: str, page_number: int = None) -> Tuple[bool, List[str]]:
    """
    Validates that all tags in the XML content are in the approved list.
    Returns (is_valid, list_of_unknown_tags).
    """
    try:
        root = ET.fromstring(xml_content)
        unknown_tags = set()

        for elem in root.iter():
            if elem.tag not in APPROVED_XML_TAGS:
                unknown_tags.add(elem.tag)

        if unknown_tags:
            page_info = f" on page {page_number}" if page_number else ""
            logger.error(f"Unknown XML tags found{page_info}: {', '.join(sorted(unknown_tags))}")
            return False, list(unknown_tags)

        return True, []
    except ET.ParseError as e:
        logger.error(f"XML parsing error during validation: {e}")
        return False, ["PARSE_ERROR"]

def validate_xml_with_model(xml_content: str) -> Tuple[bool, Optional[str]]:
    """
    Validates XML content by attempting to parse it with the XMLDocument model.
    This catches schema errors early before downstream processing.

    Args:
        xml_content: The XML content string to validate

    Returns:
        Tuple of (is_valid, error_message)
        - is_valid: True if XML is valid and can be parsed by XMLDocument model
        - error_message: None if valid, otherwise contains error details
    """
    try:
        # Attempt to parse with XMLDocument model
        XMLDocument.from_xml(xml_content)
        logger.debug("XML content successfully validated with XMLDocument model")
        return True, None
    except ET.ParseError as e:
        error_msg = f"XML parsing error: {str(e)}"
        logger.error(f"XMLDocument validation failed: {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = f"XMLDocument model validation error: {str(e)}"
        logger.error(f"XMLDocument validation failed: {error_msg}")
        return False, error_msg

def configure_gemini():
    """Configures the Gemini API with the API key from environment variables."""
    global gemini_api
    gemini_api = GeminiAPI()
    return gemini_api

def sanitize_xml_element_name(name: str) -> str:
    """
    Sanitizes a string to be a valid XML element name.
    XML element names must start with a letter or underscore, not a digit.
    """
    # If the name starts with a digit, prefix with 'Chapter_'
    if name and name[0].isdigit():
        return f"Chapter_{name}"
    return name

def count_words(text: str) -> int:
    """Counts the words in a given string, excluding common XML tags."""
    # Remove XML tags from the text
    text_no_tags = re.sub(r'<[^>]+>', '', text)
    words = re.findall(r'\b\w+\b', text_no_tags)
    return len(words)

def get_word_frequencies(text: str) -> dict:
    """Calculates the frequency of each word in a given string, excluding common XML tags."""
    # Remove XML tags from the text
    text_no_tags = re.sub(r'<[^>]+>', '', text)
    words = re.findall(r'\b\w+\b', text_no_tags.lower())
    return Counter(words)

def verify_and_correct_xml(xml_content: str, original_text: str, chapter_name: str, log_dir: str) -> str:
    """
    Uses Gemini to verify the XML content against the original text and correct it if necessary.
    """
    logger.info(f"Verifying and correcting XML for chapter: {chapter_name}...")
    try:
        prompt = f"""
        Please verify that the following XML content is a faithful and complete representation of the original text.
        If there are any discrepancies, please correct the XML.
        The corrected output should be only the valid XML, starting with the <{chapter_name}> tag.

        Original Text:
        ---
        {original_text}
        ---

        XML Content:
        ---
        {xml_content}
        ---
        """

        response = gemini_api.generate_content(prompt)
        
        if response and response.text:
            # Use regex to find the chapter block, stripping everything else
            escaped_chapter = re.escape(chapter_name)
            chapter_pattern = rf'<{escaped_chapter}(?:\s[^>]*)?>.*?</{escaped_chapter}>'
            match = re.search(chapter_pattern, response.text, re.DOTALL)
            if not match:
                raise ValueError("Verification failed: Could not find valid XML in response.")
            
            corrected_xml = match.group(0)
            ET.fromstring(corrected_xml)  # Validate the corrected XML
            
            corrected_xml_path = os.path.join(log_dir, f"{chapter_name}_corrected.xml")
            with open(corrected_xml_path, "w") as f:
                f.write(corrected_xml)
            logger.info(f"Successfully verified and corrected XML for chapter: {chapter_name}.")
            return corrected_xml
        else:
            raise ValueError("Verification failed: No response from model.")

    except Exception as e:
        logger.error(f"Failed to verify and correct XML for chapter {chapter_name}: {e}")
        return xml_content # Return the original XML if correction fails


def correct_xml_with_gemini(malformed_xml: str, original_text: str, page_number: int, log_dir: str) -> str:
    """
    Uses Gemini to correct malformed XML, using the original text as a reference.
    """
    logger.info(f"Attempting to correct malformed XML for page {page_number} with Gemini...")
    try:
        prompt = f"""
        The following XML is malformed. Please correct it based on the original text provided below.
        Ensure all tags are properly closed and the structure is valid.
        Do not add any new content or alter the existing text.
        The corrected output should be only the valid XML, starting with the <page> tag.

        Original Text:
        ---
        {original_text}
        ---

        Malformed XML:
        ---
        {malformed_xml}
        ---
        """

        response = gemini_api.generate_content(prompt)
        
        if response and response.text:
            # Use regex to find the <page> block, stripping everything else
            match = re.search(r'<page(?:\s[^>]*)?>.*?</page>', response.text, re.DOTALL)
            if not match:
                raise ValueError("Correction failed: Could not find valid <page> XML in response.")
            
            corrected_xml = match.group(0)
            ET.fromstring(corrected_xml)  # Validate the corrected XML
            
            corrected_xml_path = os.path.join(log_dir, "pages", f"page_{page_number}_corrected.xml")
            with open(corrected_xml_path, "w") as f:
                f.write(corrected_xml)
            logger.info(f"Successfully corrected XML for page {page_number}.")
            return corrected_xml
        else:
            raise ValueError("Correction failed: No response from model.")

    except Exception as e:
        logger.error(f"Failed to correct XML for page {page_number}: {e}")
        return f"<page><error>Failed to correct malformed XML. Error: {e}</error></page>"

def is_text_legible(text: str) -> bool:
    """
    Checks if a string is likely to be legible text.
    Falls back if more than 10 words are longer than 20 characters.
    """
    long_words = [word for word in text.split() if len(word) > 20]
    if len(long_words) > 10:
        logger.debug(f"Legibility check failed: Found {len(long_words)} words longer than 20 characters.")
        return False
    return True

def get_legible_text_from_page(page_bytes: bytes, page_number: int, log_dir: str) -> Tuple[str, str]:
    """
    Tries to extract legible text from a PDF page, first from embedded text,
    then falling back to local OCR if the embedded text seems corrupted.
    """
    embedded_text = ""
    # 1. Try embedded text
    try:
        with fitz.open("pdf", page_bytes) as pdf_page_doc:
            embedded_text = pdf_page_doc[0].get_text()
            embedded_output_path = os.path.join(log_dir, "pages", f"page_{page_number}_embedded.txt")
            with open(embedded_output_path, "w") as f:
                f.write(embedded_text)

            if is_text_legible(embedded_text):
                logger.debug(f"Page {page_number}: Using embedded text.")
                return embedded_text, "embedded"
            else:
                logger.warning(f"Page {page_number}: Embedded text seems corrupted. Falling back to OCR.")
    except Exception as e:
        logger.warning(f"Could not extract embedded text from page {page_number}: {e}. Falling back to OCR.")

    # 2. Fallback to local OCR
    try:
        with fitz.open("pdf", page_bytes) as pdf_page_doc:
            pix = pdf_page_doc[0].get_pixmap()
            img_bytes = pix.tobytes("png")
            image = Image.open(io.BytesIO(img_bytes))
            ocr_text = pytesseract.image_to_string(image)
            ocr_output_path = os.path.join(log_dir, "pages", f"page_{page_number}_ocr.txt")
            with open(ocr_output_path, "w") as f:
                f.write(ocr_text)
            
            logger.debug(f"Page {page_number}: Using OCR text.")
            return ocr_text, "ocr"
    except Exception as e:
        logger.error(f"Local OCR failed for page {page_number}: {e}")
        return embedded_text, "ocr_failed"

def get_xml_for_page(page_info: tuple) -> str:
    """
    Converts a single PDF page to XML, using a robust text extraction method.
    Now uses a pre-uploaded PDF file to avoid repeated uploads.
    """
    page_bytes, page_number, log_dir, uploaded_pdf_file = page_info
    display_name = f"page_{page_number}"
    max_retries = 3
    backoff_factor = 2

    page_pdf_path = os.path.join(log_dir, "pages", f"{display_name}.pdf")
    with open(page_pdf_path, "wb") as f:
        f.write(page_bytes)

    legible_text, text_source = get_legible_text_from_page(page_bytes, page_number, log_dir)
    pdf_word_count = count_words(legible_text)

    for attempt in range(max_retries):
        try:
            logger.debug(f"Processing {display_name} from uploaded PDF (Attempt {attempt + 1})...")

            # Get approved tags list for prompt
            approved_tags_text = get_approved_tags_text()

            prompt = f"""
            You are a highly skilled document analyst. Your task is to convert page {page_number} of the provided Dungeons & Dragons module PDF into a well-structured XML format.

            IMPORTANT: Process ONLY page {page_number} of the PDF. Ignore all other pages.

            ## APPROVED XML TAGS
            You MUST use ONLY the following XML tags. Using any other tags will result in an error:

            {approved_tags_text}

            ## FORMATTING RULES
            - The root element must be `<page>`
            - Do not wrap the XML in markdown fences like ```xml
            - **PRESERVE ALL FORMATTING from the source PDF:**
              - If text is italicized in the PDF, use Markdown: `*italic text*`
              - If text is bold in the PDF, use Markdown: `**bold text**`
              - If text is both bold AND italic, use: `***bold italic***`
            - All XML tags must be properly closed
            - Escape special characters (&, <, >)
            - Preserve the semantic structure of the document

            **CRITICAL**: Do NOT omit italics or bold formatting. Every word that appears italic or bold in the PDF must be preserved in the XML output using Markdown syntax.

            ## DISTINGUISHING HEADINGS FROM BOLD TEXT
            **CRITICAL**: `<section>`, `<subsection>`, and `<subsubsection>` tags should ONLY be used for actual headings.

            A heading is text that is:
            1. **Visually larger** than normal paragraph text (increased font size)
            2. Usually bold or emphasized
            3. Stands alone on its own line
            4. Introduces a new topic or section

            **DO NOT use heading tags for:**
            - Bold text that is the SAME SIZE as normal text (use `<p>**bold text**</p>` instead)
            - Labels or terms in definition lists (use `<term>` instead)
            - Emphasis within paragraphs (use **bold** markdown)

            **Example of correct usage:**
            - Large heading text ‚Üí `<section>Combat Encounters</section>`
            - Bold text same size as paragraph ‚Üí `<p>**Important:** Do not forget this rule.</p>`
            - NPC names in lists ‚Üí `<term>Toblen Stonehill</term>` not `<subsection>`

            ## HEADING HIERARCHY AND CONTEXT
            **CRITICAL**: Font size alone does NOT determine hierarchy level. A heading can be LARGER than normal text but still be a subsection or subsubsection based on CONTEXT.

            **Consider the semantic structure:**
            - If a heading introduces a topic WITHIN an existing section, it's a `<subsection>` (even if larger than body text)
            - If a heading introduces a detail WITHIN a subsection, it's a `<subsubsection>` (even if larger than body text)
            - Use `<section>` only for major topic changes or new top-level areas

            **Example:**
            If you're processing content about "Cragmaw Hideout" (a section), and you encounter:
            - "1. Cave Mouth" (larger text) ‚Üí This is a `<subsection>` (a location within the hideout), NOT a new `<section>`
            - "Treasure" (larger text under "1. Cave Mouth") ‚Üí This is a `<subsubsection>` (detail within the location)

            **Read the full page context** to understand where you are in the document structure. Don't promote headings to higher levels just because they're visually prominent.

            ## IDENTIFYING HEADERS AND FOOTERS
            - **Footers** are typically small text at the bottom of the page (page numbers, chapter names, copyright info)
            - **Headers** are typically text at the top of the page (chapter titles, section names)
            - If text appears in the same position on multiple pages with same/similar content, it's likely a header or footer
            - Tag repeating bottom text as `<footer>`, repeating top text as `<header>`
            - Page numbers should be tagged as `<page_number>` within the footer or header

            ## STAT BLOCK TAGGING
            **CRITICAL**: Tag all D&D 5e stat blocks with: `<stat_block name="Creature Name">raw text</stat_block>`

            - **Preserve COMPLETE original stat block text** inside the tag
            - Do NOT parse or structure the stat block - keep it as raw text
            - Stat blocks are typically boxed sections with creature stats:
              - Name and type/size (e.g., "GOBLIN / Small humanoid")
              - Armor Class, Hit Points, Speed
              - Ability scores (STR, DEX, CON, INT, WIS, CHA)
              - Challenge rating
              - Traits and actions

            **Example stat block tagging:**
            ```xml
            <stat_block name="Goblin">
            GOBLIN
            Small humanoid (goblinoid), neutral evil

            Armor Class 15 (leather armor, shield)
            Hit Points 7 (2d6)
            Speed 30 ft.

            STR     DEX     CON     INT     WIS     CHA
            8 (-1)  14 (+2) 10 (+0) 10 (+0) 8 (-1)  8 (-1)

            Challenge 1/4 (50 XP)

            Nimble Escape. The goblin can take the Disengage or Hide action...

            ACTIONS
            Scimitar. Melee Weapon Attack: +4 to hit...
            </stat_block>
            ```

            ## EXAMPLES
            - Headings (semantic): `<chapter_title>Chapter Title</chapter_title>`, `<section>Section Title</section>`, `<subsection>Subsection Title</subsection>`
            - Paragraphs: `<p>This is a paragraph with **bold** and *italic* text.</p>`
            - Lists: `<list><item>Item 1</item><item>Item 2</item></list>`
            - Boxed text: `<boxed_text><p>Special content in a box</p></boxed_text>`
            - Tables: `<table><table_row><table_cell>Cell 1</table_cell><table_cell>Cell 2</table_cell></table_row></table>`
            - Definition lists: `<definition_list><definition_item><term>Term</term><definition>Definition text</definition></definition_item></definition_list>`

            Do not include any text, comments, or data outside the root `<page>` element.
            """

            logger.debug(f"Generating XML for {display_name}")
            # Use the pre-uploaded file
            response = gemini_api.generate_content(prompt, uploaded_pdf_file)

            cleaned_xml = ""
            if response and response.text:
                raw_response_text = response.text
                raw_output_path = os.path.join(log_dir, "pages", f"{display_name}_attempt_{attempt + 1}_raw.xml")
                with open(raw_output_path, "w") as f:
                    f.write(raw_response_text)

                # Use regex to find the <page> block, stripping everything else
                match = re.search(r'<page(?:\s[^>]*)?>.*?</page>', raw_response_text, re.DOTALL)
                if not match:
                    raise ValueError("Could not find valid <page> XML in response.")

                cleaned_response_text = match.group(0)

                temp_xml = re.sub(r'<i>(.*?)</i>', r'*\1*', cleaned_response_text, flags=re.DOTALL)
                temp_xml = re.sub(r'<b>(.*?)</b>', r'**\1**', temp_xml, flags=re.DOTALL)
                temp_xml = re.sub(r'<italic>(.*?)</italic>', r'*\1*', temp_xml, flags=re.DOTALL)

                try:
                    ET.fromstring(temp_xml)  # Validate the cleaned XML
                    cleaned_xml = temp_xml
                except ET.ParseError as e:
                    logger.warning(f"Malformed XML detected on page {page_number}: {e}")
                    cleaned_xml = correct_xml_with_gemini(temp_xml, legible_text, page_number, log_dir)

                # Validate XML tags
                is_valid, unknown_tags = validate_xml_tags(cleaned_xml, page_number)
                if not is_valid:
                    raise ValueError(f"Unknown XML tags detected on page {page_number}: {', '.join(unknown_tags)}. Only approved tags are allowed.")

            else:
                raise ValueError("Failed to generate content.")

            if pdf_word_count >= 30:
                xml_word_count = count_words(cleaned_xml)
                difference = abs(pdf_word_count - xml_word_count)
                if pdf_word_count > 0:
                    percentage_diff = (difference / pdf_word_count) * 100
                    if percentage_diff > 15:
                        raise ValueError(f"Word count mismatch ({text_source}) for page {page_number} is over 15% ({percentage_diff:.2f}%).")
                elif xml_word_count > 0:
                    raise ValueError(f"Word count mismatch ({text_source}): PDF has 0 words, XML has {xml_word_count} words.")

            page_xml_path = os.path.join(log_dir, "pages", f"{display_name}.xml")
            with open(page_xml_path, "w") as f:
                f.write(cleaned_xml)

            return cleaned_xml

        except Exception as e:
            logger.warning(f"An error occurred on attempt {attempt + 1} for page {page_number}: {e}")
            if "Word count mismatch" in str(e):
                # Generate and save word frequency analysis only for word count errors
                pdf_word_freq = get_word_frequencies(legible_text)
                xml_word_freq = get_word_frequencies(cleaned_xml)

                pdf_freq_log_path = os.path.join(log_dir, "pages", f"{display_name}_pdf_word_frequencies.json")
                with open(pdf_freq_log_path, "w") as f:
                    json.dump(pdf_word_freq, f, indent=4)
                logger.debug(f"PDF word frequency analysis for page {page_number} saved to {pdf_freq_log_path}")

                xml_freq_log_path = os.path.join(log_dir, "pages", f"{display_name}_xml_word_frequencies.json")
                with open(xml_freq_log_path, "w") as f:
                    json.dump(xml_word_freq, f, indent=4)
                logger.debug(f"XML word frequency analysis for page {page_number} saved to {xml_freq_log_path}")

            if attempt < max_retries - 1:
                sleep_time = backoff_factor ** attempt
                logger.warning(f"Retrying in {sleep_time} seconds...")
                time.sleep(sleep_time)
            else:
                logger.error(f"Failed to process page {page_number} after {max_retries} attempts.")
                error_xml_root = ET.Element("page")
                error_tag = ET.SubElement(error_xml_root, "error")
                error_tag.text = f"Failed to process after multiple retries. Last error: {e}"
                return ET.tostring(error_xml_root, encoding='unicode')

    return "<page><error>An unexpected error occurred in the processing loop.</error></page>"

def process_chapter(pdf_path: str, output_xml_path: str, base_log_dir: str) -> List[str]:
    """
    Orchestrates the page-by-page conversion and merges them into a single XML file.
    If any page fails, the entire chapter is marked as failed.
    Now optimized to upload the PDF once and reuse it for all pages.
    """
    chapter_name = os.path.splitext(os.path.basename(pdf_path))[0]
    log_dir = os.path.join(base_log_dir, chapter_name)
    os.makedirs(os.path.join(log_dir, "pages"), exist_ok=True)
    page_errors = []

    logger.info(f"Starting processing for chapter: {chapter_name} ---")
    pdf_document = fitz.open(pdf_path)
    page_infos = []
    pdf_text = ""
    for page_num in range(len(pdf_document)):
        doc = fitz.open()
        doc.insert_pdf(pdf_document, from_page=page_num, to_page=page_num)
        page_bytes = doc.write()
        page_text, _ = get_legible_text_from_page(page_bytes, page_num + 1, log_dir)
        pdf_text += page_text
        doc.close()
        # Don't add uploaded_file yet - will add it after upload
        page_infos.append((page_bytes, page_num + 1, log_dir, None))
    pdf_document.close()

    # Upload the full PDF once and reuse for all pages
    logger.info(f"Uploading full PDF for chapter: {chapter_name}")
    with GeminiFileContext(gemini_api, pdf_path, f"chapter_{chapter_name}") as uploaded_pdf:
        # Update all page_infos with the uploaded file
        page_infos = [(pb, pn, ld, uploaded_pdf) for pb, pn, ld, _ in page_infos]

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            page_xmls = list(executor.map(get_xml_for_page, page_infos))

    # Sanitize chapter name for use as XML element name
    xml_element_name = sanitize_xml_element_name(chapter_name)
    chapter_root = ET.Element(xml_element_name)
    logger.info(f"Merging {len(page_xmls)} XML pages into a single document...")
    has_page_errors = False
    for i, page_xml_str in enumerate(page_xmls):
        try:
            page_root = ET.fromstring(page_xml_str)
            page_root.set('number', str(i + 1))
            chapter_root.append(page_root)
            if page_root.find('error') is not None:
                has_page_errors = True
                error_message = page_root.find('error').text
                page_errors.append(f"Page {i + 1}: {error_message}")
        except ET.ParseError as e:
            has_page_errors = True
            page_num = i + 1
            error_msg = f"Page {page_num}: Failed to parse XML. Error: {e}"
            logger.warning(f" {error_msg}")
            page_errors.append(error_msg)
            error_page = ET.SubElement(chapter_root, "page", {'number': str(page_num), 'parse_error': 'true'})
            error_message_tag = ET.SubElement(error_page, "error")
            error_message_tag.text = "Failed to parse XML content from Gemini."
            page_xml_path = os.path.join(log_dir, "pages", f"page_{page_num}_parse_error.xml")
            with open(page_xml_path, "w") as f:
                f.write(page_xml_str)

    if has_page_errors:
        raise Exception(f"Chapter failed due to page errors: {', '.join(page_errors)}")

    try:
        ET.indent(chapter_root)
    except AttributeError:
        pass
    final_xml_content = ET.tostring(chapter_root, encoding='unicode')

    with open(os.path.join(log_dir, "final_unverified.xml"), "w") as f:
        f.write(final_xml_content)

    try:
        verify_final_word_count(pdf_path, final_xml_content, log_dir)
    except ValueError as e:
        if "Final word count mismatch" in str(e):
            final_xml_content = verify_and_correct_xml(final_xml_content, pdf_text, xml_element_name, log_dir)
            verify_final_word_count(pdf_path, final_xml_content, log_dir)

    # Validate XML with XMLDocument model before saving
    logger.info(f"Validating final XML with XMLDocument model for chapter: {xml_element_name}")
    is_valid, error_msg = validate_xml_with_model(final_xml_content)
    if not is_valid:
        # Log to dedicated validation errors file
        error_file = os.path.join(log_dir, "validation_errors.txt")
        with open(error_file, 'a') as f:
            f.write(f"\n\n=== {xml_element_name} ===\n")
            f.write(f"Validation Error: {error_msg}\n")
            f.write(f"XML Content Preview:\n{final_xml_content[:1000]}\n")
        raise Exception(f"XMLDocument validation failed for chapter {xml_element_name}: {error_msg}")

    with open(output_xml_path, "w") as f:
        f.write(final_xml_content)
    logger.info(f"Successfully created final merged XML file: {output_xml_path}")
    return page_errors

def verify_final_word_count(pdf_path: str, final_xml_content: str, log_dir: str):
    """
    Compares the word count of the source PDF and the generated XML content.
    """
    logger.info("Verifying final word count...")
    # This function now uses the more robust get_legible_text_from_page for its source
    pdf_document = fitz.open(pdf_path)
    pdf_text = ""
    page_log_dir = os.path.join(log_dir, "final_verification_pages")
    os.makedirs(page_log_dir, exist_ok=True)

    for page_num in range(len(pdf_document)):
        doc = fitz.open()
        doc.insert_pdf(pdf_document, from_page=page_num, to_page=page_num)
        page_text, _ = get_legible_text_from_page(doc.write(), page_num + 1, page_log_dir)
        pdf_text += page_text
        doc.close()
    pdf_document.close()
    pdf_word_count = count_words(pdf_text)

    xml_word_count = count_words(final_xml_content)

    if pdf_word_count == 0:
        if xml_word_count > 0:
             raise ValueError("PDF word count is 0, but XML has content.")
        logger.info("Both PDF and XML appear to have no text content. Skipping verification.")
        return

    difference = abs(pdf_word_count - xml_word_count)
    percentage_diff = (difference / pdf_word_count) * 100 if pdf_word_count > 0 else 0

    logger.info(f"PDF word count (from legible text): {pdf_word_count}")
    logger.info(f"Final XML word count: {xml_word_count}")
    logger.info(f"Difference: {difference} words ({percentage_diff:.2f}%)")

    if percentage_diff > 15: # Loosen final threshold slightly
        # Generate and save word frequency analysis in two separate files
        pdf_word_freq = get_word_frequencies(pdf_text)
        xml_word_freq = get_word_frequencies(final_xml_content)
        chapter_name = os.path.splitext(os.path.basename(pdf_path))[0]

        pdf_freq_log_path = os.path.join(log_dir, f"{chapter_name}_final_pdf_word_frequencies.json")
        with open(pdf_freq_log_path, "w") as f:
            json.dump(pdf_word_freq, f, indent=4)
        logger.debug(f"Final PDF word frequency analysis saved to {pdf_freq_log_path}")

        xml_freq_log_path = os.path.join(log_dir, f"{chapter_name}_final_xml_word_frequencies.json")
        with open(xml_freq_log_path, "w") as f:
            json.dump(xml_word_freq, f, indent=4)
        logger.debug(f"Final XML word frequency analysis saved to {xml_freq_log_path}")

        raise ValueError(
            f"Final word count mismatch for {os.path.basename(pdf_path)}. "
            f"Difference is {percentage_diff:.2f}%, which is over the 15% threshold."
        )
    else:
        logger.info("Final word count verification passed.")

def write_error_report(run_dir: str, all_errors: dict):
    """Writes a summary of all processing errors to a text file."""
    report_path = os.path.join(run_dir, "error_report.txt")
    with open(report_path, "w") as f:
        if not all_errors:
            f.write("All chapters processed successfully.\n")
            return

        f.write("--- D&D Module Generation Error Report ---")
        for chapter, data in all_errors.items():
            f.write(f"Chapter: {chapter}\n")
            f.write(f"  Status: {data['status']}\n")
            f.write(f"  Details: {data['details']}\n")
            if data.get('page_errors'):
                f.write("  Page-Specific Errors:\n")
                for err in data['page_errors']:
                    f.write(f"    - {err}\n")
            f.write("\n")
    logger.info(f"Error report saved to: {report_path}")

import argparse

def main(input_dir: str, base_output_dir: str, single_file: str = None):
    """
    Main function to convert all PDF files in a directory to XML in parallel.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(base_output_dir, timestamp)
    output_dir = os.path.join(run_dir, "documents")
    log_dir = os.path.join(run_dir, "intermediate_logs")
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    # Reconfigure logger to write to run directory
    global logger
    from pathlib import Path
    logger = get_run_logger("pdf_to_xml", Path(run_dir))

    all_errors = {}
    if single_file:
        pdf_files = [single_file]
    else:
        pdf_files = sorted([f for f in os.listdir(input_dir) if f.endswith(".pdf")])
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_pdf = {}
        for pdf_file in pdf_files:
            pdf_path = os.path.join(input_dir, pdf_file)
            pdf_basename = os.path.splitext(pdf_file)[0]
            xml_output_path = os.path.join(output_dir, f"{pdf_basename}.xml")
            
            future = executor.submit(process_chapter, pdf_path, xml_output_path, log_dir)
            future_to_pdf[future] = pdf_basename

        for future in concurrent.futures.as_completed(future_to_pdf):
            pdf_basename = future_to_pdf[future]
            try:
                page_errors = future.result()
                if page_errors:
                    all_errors[pdf_basename] = {
                        "status": "Completed with page-level errors",
                        "details": "One or more pages failed to process correctly.",
                        "page_errors": page_errors
                    }
            except Exception as e:
                logger.error(f"FATAL: Chapter {pdf_basename} failed to process: {e}")
                all_errors[pdf_basename] = {
                    "status": "Failed",
                    "details": str(e),
                    "page_errors": []
                }
    
    write_error_report(run_dir, all_errors)

if __name__ == "__main__":
    configure_gemini()

    parser = argparse.ArgumentParser(description="Convert D&D module PDFs to XML.")
    parser.add_argument("--file", type=str, help="The name of a single PDF file to process.")
    args = parser.parse_args()

    pdf_sections_dir = os.path.join(PROJECT_ROOT, "data", "pdf_sections", "Lost_Mine_of_Phandelver")
    runs_output_dir = os.path.join(PROJECT_ROOT, "output", "runs")

    main(pdf_sections_dir, runs_output_dir, single_file=args.file)
</file>

<file path="src/pdf_processing/valid_xml_tags.py">
"""
Valid XML tags for D&D module conversion.

This module defines all approved XML tags that can be used in the PDF to XML
conversion process. Any tags not in this list will trigger a validation error.
"""

# Approved XML tags for generated content
APPROVED_XML_TAGS = {
    # Structure
    'page', 'header', 'footer', 'page_number', 'chapter_title',

    # Headings (semantic hierarchy: Chapter > Section > Subsection)
    'title', 'section', 'subsection', 'subsubsection',

    # Content
    'p', 'boxed_text',

    # Formatting (used internally, converted to Markdown)
    'b', 'i', 'italic', 'bold', 'em', 'strong', 'br',

    # Lists
    'list', 'item',

    # Tables
    'table', 'table_row', 'table_cell',

    # Monsters/Stats
    'stat_block',  # D&D 5e stat blocks (raw text preserved)

    # Definition lists
    'definition_list', 'definition_item', 'term', 'definition',

    # Special cases
    'error',  # For error handling
}

# Tags organized by category for prompt generation
TAGS_BY_CATEGORY = {
    "Structure": ['page', 'header', 'footer', 'page_number', 'chapter_title'],
    "Headings": ['title', 'section', 'subsection', 'subsubsection'],
    "Content": ['p', 'boxed_text'],
    "Lists": ['list', 'item'],
    "Tables": ['table', 'table_row', 'table_cell'],
    "Monsters": [
        'stat_block',  # D&D 5e stat blocks (raw text)
    ],
    "Definition Lists": ['definition_list', 'definition_item', 'term', 'definition'],
}

def get_approved_tags_text() -> str:
    """
    Returns a formatted string of approved tags organized by category.
    Suitable for including in prompts to Gemini.
    """
    return "\n".join([
        f"- **{category}**: {', '.join(f'<{tag}>' for tag in tags)}"
        for category, tags in TAGS_BY_CATEGORY.items()
    ])
</file>

<file path="src/util/gemini.py">
"""
Gemini API utilities for D&D Module Converter.

Centralized module for all Google Generative AI (Gemini) interactions.
"""

import asyncio
import os
import concurrent.futures
from typing import Optional, Any
from google import genai
from dotenv import load_dotenv
from pathlib import Path

# Default model name
DEFAULT_MODEL_NAME = "gemini-2.5-pro"


class GeminiAPI:
    """Wrapper for Gemini API operations."""

    def __init__(self, model_name: str = DEFAULT_MODEL_NAME, api_key: Optional[str] = None):
        """
        Initialize Gemini API client.

        Args:
            model_name: Name of the Gemini model to use
            api_key: API key (if None, loads from environment)
        """
        self.model_name = model_name
        self._configured = False
        self.client = None

        if api_key:
            self._configure_with_key(api_key)
        else:
            self._configure_from_env()

    def _configure_from_env(self):
        """Load API key from .env file and configure Gemini."""
        # Try to find .env file (search up to project root)
        current_dir = Path(__file__).parent
        project_root = current_dir.parent.parent
        dotenv_path = project_root / ".env"

        load_dotenv(dotenv_path=dotenv_path)

        api_key = os.getenv("GeminiImageAPI")
        if not api_key:
            raise ValueError(
                "Gemini API key not found. Set GeminiImageAPI in .env file or pass api_key parameter."
            )

        self.client = genai.Client(api_key=api_key)
        self._configured = True

    def _configure_with_key(self, api_key: str):
        """Configure Gemini with provided API key."""
        self.client = genai.Client(api_key=api_key)
        self._configured = True

    def create_model(self) -> Any:
        """
        Create a Gemini generative model instance.

        Returns:
            Client instance (for compatibility)
        """
        if not self._configured:
            raise RuntimeError("Gemini API not configured. Call configure() first.")

        return self.client

    def upload_file(self, file_path: str, display_name: Optional[str] = None) -> Any:
        """
        Upload a file to Gemini.

        Args:
            file_path: Path to file to upload
            display_name: Optional display name for the file

        Returns:
            Uploaded file object
        """
        if not self._configured:
            raise RuntimeError("Gemini API not configured.")

        return self.client.files.upload(file=file_path)

    def delete_file(self, file_name: str):
        """
        Delete a file from Gemini.

        Args:
            file_name: Name of the file to delete (from uploaded_file.name)
        """
        if not self._configured:
            raise RuntimeError("Gemini API not configured.")

        self.client.files.delete(name=file_name)

    def generate_content(self, prompt: str, file_obj: Optional[Any] = None, timeout: float = 120.0) -> Any:
        """
        Generate content using Gemini.

        Args:
            prompt: Text prompt
            file_obj: Optional file object (from upload_file)
            timeout: Timeout in seconds (default: 120 seconds / 2 minutes)

        Returns:
            Response object with .text attribute

        Raises:
            TimeoutError: If the API call exceeds the timeout
        """
        if not self._configured:
            raise RuntimeError("Gemini API not configured.")

        if file_obj:
            contents = [file_obj, "\n\n", prompt]
        else:
            contents = [prompt]

        # Wrap the synchronous API call with a timeout
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(
                self.client.models.generate_content,
                model=self.model_name,
                contents=contents
            )
            try:
                return future.result(timeout=timeout)
            except concurrent.futures.TimeoutError:
                future.cancel()
                raise TimeoutError(
                    f"Gemini API call exceeded timeout of {timeout} seconds"
                )


class GeminiFileContext:
    """Context manager for uploading and auto-deleting Gemini files."""

    def __init__(self, api: GeminiAPI, file_path: str, display_name: Optional[str] = None):
        """
        Initialize file upload context.

        Args:
            api: GeminiAPI instance
            file_path: Path to file to upload
            display_name: Optional display name
        """
        self.api = api
        self.file_path = file_path
        self.display_name = display_name
        self.uploaded_file = None

    def __enter__(self):
        """Upload file on context entry."""
        self.uploaded_file = self.api.upload_file(self.file_path, self.display_name)
        return self.uploaded_file

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Delete file on context exit."""
        if self.uploaded_file:
            try:
                self.api.delete_file(self.uploaded_file.name)
            except Exception:
                # Silently fail deletion - file will expire anyway
                pass


# Convenience function for legacy code
def configure_gemini(api_key: Optional[str] = None) -> GeminiAPI:
    """
    Configure and return a Gemini API instance.

    Args:
        api_key: Optional API key (loads from .env if not provided)

    Returns:
        Configured GeminiAPI instance
    """
    return GeminiAPI(api_key=api_key)


async def generate_content_async(
    client: genai.Client,
    model: str,
    contents: Any,
    config: Optional[dict] = None
) -> Any:
    """
    Async wrapper for generate_content using asyncio.to_thread.

    The google.genai library only provides synchronous generate_content().
    This wrapper allows async code to call it without blocking the event loop.

    Args:
        client: genai.Client instance
        model: Model name (e.g., "gemini-2.5-pro")
        contents: Content to send to the model
        config: Optional generation config dict

    Returns:
        Response object with .text attribute

    Example:
        client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
        response = await generate_content_async(
            client=client,
            model="gemini-2.5-pro",
            contents="Hello",
            config={'temperature': 0.7}
        )
    """
    return await asyncio.to_thread(
        client.models.generate_content,
        model=model,
        contents=contents,
        config=config
    )
</file>

<file path="tests/actors/test_extract_stat_blocks.py">
"""Tests for extracting stat blocks from XML."""

import pytest
from pathlib import Path
from actors.parse_stat_blocks import extract_stat_blocks_from_xml_file


@pytest.mark.unit
class TestExtractStatBlocksFromXML:
    """Test extracting raw stat block text from XML."""

    def test_extract_finds_stat_blocks(self):
        """Test extraction finds all stat block elements."""
        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_stat_blocks.xml"

        stat_blocks = extract_stat_blocks_from_xml_file(str(fixture_path))

        assert len(stat_blocks) == 2
        assert stat_blocks[0]["name"] == "Goblin"
        assert stat_blocks[1]["name"] == "Goblin Boss"
        assert "Small humanoid" in stat_blocks[0]["raw_text"]
        assert "Challenge 1/4" in stat_blocks[0]["raw_text"]

    def test_extract_empty_xml(self):
        """Test extraction with XML containing no stat blocks."""
        # Create temporary XML without stat blocks
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False) as f:
            f.write("<Chapter><page>No stat blocks here</page></Chapter>")
            temp_path = f.name

        stat_blocks = extract_stat_blocks_from_xml_file(temp_path)

        assert len(stat_blocks) == 0

    def test_extract_invalid_xml(self):
        """Test extraction handles malformed XML."""
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False) as f:
            f.write("<Chapter><unclosed>")
            temp_path = f.name

        with pytest.raises(Exception):  # xml.etree.ElementTree.ParseError
            extract_stat_blocks_from_xml_file(temp_path)


@pytest.mark.integration
@pytest.mark.requires_api
class TestExtractAndParseStatBlocks:
    """Integration test: extract from XML and parse with Gemini."""

    def test_full_extraction_pipeline(self, check_api_key):
        """Test complete workflow: XML ‚Üí raw text ‚Üí parsed StatBlock."""
        from actors.extract_stat_blocks import extract_and_parse_stat_blocks

        fixture_path = Path(__file__).parent / "fixtures" / "sample_chapter_with_stat_blocks.xml"

        parsed_stat_blocks = extract_and_parse_stat_blocks(str(fixture_path))

        assert len(parsed_stat_blocks) == 2

        # Check first stat block
        goblin = parsed_stat_blocks[0]
        # AI may return "Goblin" or "Goblin Boss" for first entry depending on extraction order
        assert "GOBLIN" in goblin.name.upper()
        # Accept values from either regular goblin or boss variant
        assert goblin.armor_class in [15, 17]  # Regular: 15, Boss: 17
        assert goblin.hit_points in [7, 21]     # Regular: 7, Boss: 21
        assert goblin.challenge_rating in [0.25, 1.0]  # Regular: 0.25, Boss: 1.0

        # Check second stat block
        boss = parsed_stat_blocks[1]
        assert "GOBLIN" in boss.name.upper()  # AI may return in different order
        assert boss.armor_class in [15, 17]
        assert boss.hit_points in [7, 21]
        assert boss.challenge_rating in [0.25, 1.0]
</file>

<file path="tests/api/test_api_integration.py">
"""Integration tests for public API (require real API keys)."""
import pytest
from pathlib import Path
from api import create_actor, extract_maps, process_pdf_to_journal, APIError


@pytest.mark.smoke
@pytest.mark.integration
@pytest.mark.slow
def test_create_actor_integration(check_api_key, check_foundry_credentials):
    """Smoke test: End-to-end actor creation via public API

    Test create_actor with real Gemini API."""
    result = create_actor(
        "A simple goblin scout with a shortbow",
        challenge_rating=0.25
    )

    # Verify result structure
    assert result.foundry_uuid.startswith("Actor.")
    assert isinstance(result.name, str) and len(result.name) > 0
    assert result.challenge_rating == 0.25
    assert result.output_dir.exists()

    # Verify output files exist
    assert (result.output_dir / "01_raw_stat_block.txt").exists()
    assert (result.output_dir / "04_foundry_actor.json").exists()


@pytest.mark.smoke
@pytest.mark.integration
@pytest.mark.slow
def test_extract_maps_integration(test_pdf_path, check_api_key):
    """Smoke test: Map extraction from PDF via public API

    Test extract_maps with real PDF."""
    result = extract_maps(str(test_pdf_path))

    # May not have maps, but should complete without error
    assert isinstance(result.total_maps, int)
    assert result.total_maps >= 0
    assert len(result.maps) == result.total_maps


@pytest.mark.integration
@pytest.mark.slow
def test_process_pdf_to_journal_integration(test_pdf_path, check_api_key):
    """Test process_pdf_to_journal with real PDF (skip upload)."""
    # NOTE: This test is expected to fail with NotImplementedError
    # because run_pdf_to_xml() is not yet implemented
    with pytest.raises(APIError) as exc_info:
        result = process_pdf_to_journal(
            str(test_pdf_path),
            "Integration Test Journal",
            skip_upload=True  # Don't actually upload in tests
        )

    # Verify the underlying error is NotImplementedError
    assert exc_info.value.__cause__ is not None
    assert isinstance(exc_info.value.__cause__, NotImplementedError)
</file>

<file path="tests/foundry/actors/test_pit_fiend_integration.py">
"""Integration test for Pit Fiend with all features."""

import pytest
from dotenv import load_dotenv
from pathlib import Path

from foundry.actors.models import (
    ParsedActorData, Attack, Trait, DamageFormula,
    Multiattack, InnateSpellcasting, InnateSpell, AttackSave
)
from foundry.actors.converter import convert_to_foundry
from foundry.client import FoundryClient
from foundry.actors.spell_cache import SpellCache

load_dotenv()


@pytest.mark.integration
@pytest.mark.requires_foundry
class TestPitFiendIntegration:
    """Full integration test for Pit Fiend."""

    @pytest.fixture
    def spell_cache(self):
        """Load spell cache."""
        spell_cache = SpellCache()
        spell_cache.load()
        return spell_cache

    @pytest.fixture
    def pit_fiend_data(self):
        """Complete Pit Fiend data."""
        return ParsedActorData(
            source_statblock_name="Pit Fiend",
            name="Pit Fiend",
            size="large",
            creature_type="fiend",
            creature_subtype="devil",
            alignment="lawful evil",
            armor_class=19,
            hit_points=300,
            hit_dice="24d10+168",
            speed_walk=30,
            speed_fly=60,
            challenge_rating=20,
            abilities={
                "STR": 26,
                "DEX": 14,
                "CON": 24,
                "INT": 22,
                "WIS": 18,
                "CHA": 24
            },
            saving_throw_proficiencies=["dex", "con", "wis"],
            condition_immunities=["poisoned"],
            truesight=120,
            languages=["Infernal", "Telepathy 120 ft."],
            multiattack=Multiattack(
                name="Multiattack",
                description="The pit fiend makes four attacks: one with its bite, one with its claw, one with its mace, and one with its tail.",
                num_attacks=4
            ),
            traits=[
                Trait(
                    name="Fear Aura",
                    description="Any creature hostile to the pit fiend that starts its turn within 20 feet must make a DC 21 Wisdom saving throw.",
                    activation="passive"
                ),
                Trait(
                    name="Magic Resistance",
                    description="The pit fiend has advantage on saving throws against spells and other magical effects.",
                    activation="passive"
                ),
                Trait(
                    name="Magic Weapons",
                    description="The pit fiend's weapon attacks are magical.",
                    activation="passive"
                ),
            ],
            innate_spellcasting=InnateSpellcasting(
                ability="charisma",
                save_dc=21,
                spells=[
                    InnateSpell(name="Detect Magic", frequency="at will"),
                    InnateSpell(name="Fireball", frequency="at will"),
                    InnateSpell(name="Hold Monster", frequency="3/day", uses=3),
                    InnateSpell(name="Wall of Fire", frequency="3/day", uses=3),
                ]
            ),
            attacks=[
                Attack(
                    name="Bite",
                    attack_type="melee",
                    attack_bonus=14,
                    reach=5,
                    damage=[
                        DamageFormula(number=4, denomination=6, bonus="+8", type="piercing")
                    ],
                    # NEW: Add saving throw for poison
                    attack_save=AttackSave(
                        ability="con",
                        dc=21,
                        ongoing_damage=[DamageFormula(number=6, denomination=6, bonus="", type="poison")],
                        duration_rounds=10,
                        effect_description="Poisoned - can't regain HP"
                    )
                ),
                Attack(
                    name="Claw",
                    attack_type="melee",
                    attack_bonus=14,
                    reach=10,
                    damage=[
                        DamageFormula(number=2, denomination=8, bonus="+8", type="slashing")
                    ]
                ),
                Attack(
                    name="Mace",
                    attack_type="melee",
                    attack_bonus=14,
                    reach=10,
                    damage=[
                        DamageFormula(number=2, denomination=6, bonus="+8", type="bludgeoning"),
                        DamageFormula(number=6, denomination=6, bonus="", type="fire")
                    ]
                ),
                Attack(
                    name="Tail",
                    attack_type="melee",
                    attack_bonus=14,
                    reach=10,
                    damage=[
                        DamageFormula(number=3, denomination=10, bonus="+8", type="bludgeoning")
                    ]
                ),
            ]
        )

    @pytest.mark.asyncio
    async def test_pit_fiend_has_all_items(self, pit_fiend_data, spell_cache):
        """Pit Fiend conversion should have weapons/feats in payload, spells in UUIDs."""
        result, spell_uuids = await convert_to_foundry(pit_fiend_data, spell_cache=spell_cache)

        items = result["items"]

        # Count by type
        weapons = [i for i in items if i["type"] == "weapon"]
        feats = [i for i in items if i["type"] == "feat"]
        spells = [i for i in items if i["type"] == "spell"]

        # Should have 4 weapons
        assert len(weapons) == 4
        assert any(w["name"] == "Bite" for w in weapons)
        assert any(w["name"] == "Claw" for w in weapons)
        assert any(w["name"] == "Mace" for w in weapons)
        assert any(w["name"] == "Tail" for w in weapons)

        # NEW: Verify Bite has 3 activities (attack + save + ongoing damage)
        bite = [w for w in weapons if w["name"] == "Bite"][0]
        assert len(bite["system"]["activities"]) == 3
        bite_activities = bite["system"]["activities"].values()
        assert any(a["type"] == "attack" for a in bite_activities)
        assert any(a["type"] == "save" for a in bite_activities)
        assert any(a["type"] == "damage" for a in bite_activities)

        # Other weapons should have 1 activity (just attack)
        claw = [w for w in weapons if w["name"] == "Claw"][0]
        assert len(claw["system"]["activities"]) == 1

        # Should have 5 feats (3 traits + multiattack + innate spellcasting)
        assert len(feats) >= 5
        assert any(f["name"] == "Multiattack" for f in feats)
        assert any(f["name"] == "Fear Aura" for f in feats)
        assert any(f["name"] == "Magic Resistance" for f in feats)
        assert any(f["name"] == "Magic Weapons" for f in feats)
        assert any("Innate Spellcasting" in f["name"] for f in feats)

        # NEW behavior: Spells NOT in payload, returned as UUIDs
        assert len(spells) == 0, "Spells should NOT be in CREATE payload"

        # Should have 4 spell UUIDs
        assert len(spell_uuids) == 4
        # All should be compendium references
        assert all(uuid.startswith("Compendium.") for uuid in spell_uuids)

        # Payload should have 9 items (4 weapons + 5 feats, NO spells)
        assert len(items) == 9

    @pytest.mark.asyncio
    async def test_pit_fiend_round_trip(self, pit_fiend_data, spell_cache):
        """Full upload/download round-trip with spells added via /give."""
        client = FoundryClient()

        # Convert and upload (with spell UUIDs for /give)
        foundry_json, spell_uuids = await convert_to_foundry(pit_fiend_data, spell_cache=spell_cache)
        actor_uuid = client.actors.create_actor(foundry_json, spell_uuids=spell_uuids)

        # Download and verify
        downloaded = client.actors.get_actor(actor_uuid)

        assert downloaded["name"] == "Pit Fiend"
        # Should have 13 items total (9 from payload + 4 spells added via /give)
        assert len(downloaded["items"]) == 13

        # Verify all weapons are present (not just count)
        uploaded_weapons = {i["name"] for i in foundry_json["items"] if i["type"] == "weapon"}
        downloaded_weapons = {i["name"] for i in downloaded["items"] if i["type"] == "weapon"}
        assert uploaded_weapons == downloaded_weapons, f"Weapons lost: {uploaded_weapons - downloaded_weapons}"
</file>

<file path="tests/foundry/test_spell_via_give.py">
"""Tests for adding compendium spells via /give endpoint."""

import pytest
from foundry.actors.models import ParsedActorData, Attack, DamageFormula, InnateSpellcasting, InnateSpell
from foundry.actors.converter import convert_to_foundry
from foundry.client import FoundryClient
from foundry.actors.spell_cache import SpellCache


@pytest.mark.integration
@pytest.mark.requires_api
@pytest.mark.asyncio
async def test_spell_via_give_workflow(check_foundry_credentials):
    """Test that spells added via /give have full compendium data."""
    # Create actor with weapons and spells
    actor = ParsedActorData(
        source_statblock_name="Test",
        name="Spell Via Give Test",
        size="medium",
        creature_type="humanoid",
        armor_class=15,
        hit_points=50,
        challenge_rating=2,
        abilities={"STR": 16, "DEX": 14, "CON": 14, "INT": 10, "WIS": 10, "CHA": 10},
        attacks=[
            Attack(name="Sword", attack_type="melee", attack_bonus=5, reach=5,
                   damage=[DamageFormula(number=1, denomination=8, bonus="+3", type="slashing")])
        ],
        innate_spellcasting=InnateSpellcasting(
            ability="charisma",
            save_dc=15,
            spells=[
                InnateSpell(name="Fireball", frequency="at will"),
                InnateSpell(name="Hold Monster", frequency="3/day", uses=3),
            ]
        )
    )

    # Load spell cache
    spell_cache = SpellCache()
    spell_cache.load()

    # Convert with new API
    actor_json, spell_uuids = await convert_to_foundry(actor, spell_cache=spell_cache)

    # Verify conversion
    assert len(spell_uuids) == 2, "Should collect 2 spell UUIDs"
    assert all("Compendium." in uuid for uuid in spell_uuids), "All UUIDs should be compendium refs"

    # Verify weapons in payload
    weapons = [i for i in actor_json["items"] if i["type"] == "weapon"]
    assert len(weapons) == 1, "Should have 1 weapon in payload"
    assert weapons[0]["name"] == "Sword"

    # Verify spells NOT in payload by default
    spells_in_payload = [i for i in actor_json["items"] if i["type"] == "spell"]
    assert len(spells_in_payload) == 0, "Should have 0 spells in payload (added via /give)"

    # Create actor with spell UUIDs
    client = FoundryClient()
    actor_uuid = client.actors.create_actor(actor_json, spell_uuids=spell_uuids)

    # Download and verify
    downloaded = client.actors.get_actor(actor_uuid)

    downloaded_weapons = [i["name"] for i in downloaded["items"] if i["type"] == "weapon"]
    downloaded_spells = [i["name"] for i in downloaded["items"] if i["type"] == "spell"]

    assert downloaded_weapons == ["Sword"], "Weapon should be preserved"
    assert set(downloaded_spells) == {"Fireball", "Hold Monster"}, "All spells should be added"

    # Verify spells have full compendium data
    for spell_item in downloaded["items"]:
        if spell_item["type"] == "spell":
            system_data = spell_item.get("system", {})
            assert system_data.get("description", {}).get("value"), \
                f"Spell {spell_item['name']} should have description"
            assert system_data.get("activities"), \
                f"Spell {spell_item['name']} should have activities"


@pytest.mark.integration
@pytest.mark.requires_api
@pytest.mark.asyncio
async def test_backward_compatibility_with_include_spells_flag(check_foundry_credentials):
    """Test that include_spells_in_payload=True still works (for backward compatibility)."""
    actor = ParsedActorData(
        source_statblock_name="Test",
        name="Backward Compat Test",
        size="medium",
        creature_type="humanoid",
        armor_class=15,
        hit_points=50,
        challenge_rating=2,
        abilities={"STR": 16, "DEX": 14, "CON": 14, "INT": 10, "WIS": 10, "CHA": 10},
        innate_spellcasting=InnateSpellcasting(
            ability="charisma",
            save_dc=15,
            spells=[InnateSpell(name="Fireball", frequency="at will")]
        )
    )

    spell_cache = SpellCache()
    spell_cache.load()

    # Convert with include_spells_in_payload=True
    actor_json, spell_uuids = await convert_to_foundry(
        actor,
        spell_cache=spell_cache,
        include_spells_in_payload=True
    )

    # Verify spells ARE in payload when flag is True
    spells_in_payload = [i for i in actor_json["items"] if i["type"] == "spell"]
    assert len(spells_in_payload) == 1, "Should have 1 spell in payload when flag is True"

    # Verify UUIDs still collected
    assert len(spell_uuids) == 1, "Should still collect spell UUIDs"


@pytest.mark.integration
@pytest.mark.requires_api
@pytest.mark.asyncio
async def test_multiple_actors_with_spells(check_foundry_credentials):
    """Test creating multiple actors with spells doesn't cause race conditions."""
    spell_cache = SpellCache()
    spell_cache.load()
    client = FoundryClient()

    results = []

    for i in range(1, 4):
        actor = ParsedActorData(
            source_statblock_name="Test",
            name=f"Test Actor {i}",
            size="medium",
            creature_type="humanoid",
            armor_class=15,
            hit_points=50,
            challenge_rating=2,
            abilities={"STR": 16, "DEX": 14, "CON": 14, "INT": 10, "WIS": 10, "CHA": 10},
            attacks=[
                Attack(name=f"Weapon_{j}", attack_type="melee", attack_bonus=5, reach=5,
                       damage=[DamageFormula(number=1, denomination=6, bonus="+3", type="slashing")])
                for j in range(1, i + 1)  # 1 weapon for actor 1, 2 for actor 2, 3 for actor 3
            ],
            innate_spellcasting=InnateSpellcasting(
                ability="charisma",
                save_dc=15,
                spells=[
                    InnateSpell(name="Fireball", frequency="at will"),
                    InnateSpell(name="Hold Monster", frequency="3/day", uses=3),
                ]
            )
        )

        actor_json, spell_uuids = await convert_to_foundry(actor, spell_cache=spell_cache)
        actor_uuid = client.actors.create_actor(actor_json, spell_uuids=spell_uuids)

        downloaded = client.actors.get_actor(actor_uuid)
        weapons = [item["name"] for item in downloaded["items"] if item["type"] == "weapon"]
        spells = [item["name"] for item in downloaded["items"] if item["type"] == "spell"]

        results.append({
            "actor": i,
            "weapons": weapons,
            "spells": spells,
            "expected_weapons": i,
            "expected_spells": 2
        })

    # Verify all actors have correct items
    for result in results:
        assert len(result["weapons"]) == result["expected_weapons"], \
            f"Actor {result['actor']} should have {result['expected_weapons']} weapons, got {len(result['weapons'])}"
        assert len(result["spells"]) == result["expected_spells"], \
            f"Actor {result['actor']} should have {result['expected_spells']} spells, got {len(result['spells'])}"


@pytest.mark.unit
async def test_converter_return_format():
    """Test that converter returns tuple of (actor_json, spell_uuids)."""
    actor = ParsedActorData(
        source_statblock_name="Test",
        name="Test Actor",
        size="medium",
        creature_type="humanoid",
        armor_class=15,
        hit_points=50,
        challenge_rating=2,
        abilities={"STR": 16, "DEX": 14, "CON": 14, "INT": 10, "WIS": 10, "CHA": 10},
        attacks=[]
    )

    result = await convert_to_foundry(actor, spell_cache=None)

    # Verify return type
    assert isinstance(result, tuple), "Should return a tuple"
    assert len(result) == 2, "Should return tuple of length 2"

    actor_json, spell_uuids = result

    assert isinstance(actor_json, dict), "First element should be dict"
    assert isinstance(spell_uuids, list), "Second element should be list"
    assert actor_json["name"] == "Test Actor"
</file>

<file path="tests/foundry/test_upload_journal.py">
"""Tests for Journal-based upload workflow in upload_journal_to_foundry.py"""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

from models import XMLDocument, Journal, parse_xml_file
from foundry.upload_journal_to_foundry import (
    upload_run_to_foundry,
    find_latest_run,
    find_xml_directory,
    build_image_mapping,
)


# Sample XML for testing
SAMPLE_XML = """
<Chapter_1>
  <page number="1">
    <chapter_title>Introduction</chapter_title>
    <p>Welcome to the adventure.</p>
    <section>Background</section>
    <p>Long ago in a distant land...</p>
    <image_ref key="page_1_intro_illustration" />
  </page>
  <page number="2">
    <subsection>Getting Started</subsection>
    <p>To begin your quest...</p>
  </page>
</Chapter_1>
"""


@pytest.fixture
def temp_run_dir(tmp_path):
    """Create a temporary run directory with sample XML and assets."""
    run_dir = tmp_path / "runs" / "20240101_120000"
    run_dir.mkdir(parents=True)

    # Create documents directory with XML file
    docs_dir = run_dir / "documents"
    docs_dir.mkdir()
    xml_file = docs_dir / "chapter_1.xml"
    xml_file.write_text(SAMPLE_XML)

    # Create map_assets directory with sample map
    map_assets_dir = run_dir / "map_assets" / "images"
    map_assets_dir.mkdir(parents=True)
    map_image = map_assets_dir / "battle_map_1.png"
    map_image.write_text("fake png data")

    # Create scene_artwork directory with sample scene
    scene_dir = run_dir / "scene_artwork" / "images"
    scene_dir.mkdir(parents=True)
    scene_image = scene_dir / "page_1_intro_illustration.png"
    scene_image.write_text("fake scene data")

    return run_dir


@pytest.fixture
def temp_runs_dir(tmp_path):
    """Create a temporary runs directory with multiple run subdirectories."""
    runs_dir = tmp_path / "runs"
    runs_dir.mkdir()

    # Create three run directories with different timestamps
    (runs_dir / "20240101_100000").mkdir()
    (runs_dir / "20240101_110000").mkdir()
    (runs_dir / "20240101_120000").mkdir()  # Latest

    return runs_dir


class TestFindLatestRun:
    """Tests for find_latest_run() function."""

    def test_finds_latest_run(self, temp_runs_dir):
        """Should find the most recent run directory by timestamp."""
        latest = find_latest_run(str(temp_runs_dir))
        assert latest.endswith("20240101_120000")

    def test_raises_on_missing_dir(self, tmp_path):
        """Should raise ValueError if runs directory doesn't exist."""
        with pytest.raises(ValueError, match="does not exist"):
            find_latest_run(str(tmp_path / "nonexistent"))

    def test_raises_on_empty_dir(self, tmp_path):
        """Should raise ValueError if no run directories found."""
        empty_dir = tmp_path / "empty"
        empty_dir.mkdir()
        with pytest.raises(ValueError, match="No run directories found"):
            find_latest_run(str(empty_dir))


class TestFindXMLDirectory:
    """Tests for find_xml_directory() function."""

    def test_finds_documents_directory(self, temp_run_dir):
        """Should find XML files in documents/ subdirectory."""
        xml_dir = find_xml_directory(str(temp_run_dir))
        assert xml_dir.endswith("documents")
        assert Path(xml_dir).exists()

    def test_finds_xml_in_root(self, tmp_path):
        """Should find XML files in run directory root if no documents/ dir."""
        run_dir = tmp_path / "run_1"
        run_dir.mkdir()
        xml_file = run_dir / "chapter.xml"
        xml_file.write_text(SAMPLE_XML)

        xml_dir = find_xml_directory(str(run_dir))
        assert xml_dir == str(run_dir)

    def test_raises_on_no_xml_files(self, tmp_path):
        """Should raise ValueError if no XML files found."""
        run_dir = tmp_path / "empty_run"
        run_dir.mkdir()

        with pytest.raises(ValueError, match="No XML files found"):
            find_xml_directory(str(run_dir))


class TestBuildImageMapping:
    """Tests for build_image_mapping() function."""

    def test_builds_mapping_from_map_assets(self, temp_run_dir):
        """Should build image mapping from map_assets directory."""
        mapping = build_image_mapping(temp_run_dir)

        # Should include map asset
        assert "battle_map_1" in mapping
        assert mapping["battle_map_1"].endswith("battle_map_1.png")

    def test_builds_mapping_from_scene_artwork(self, temp_run_dir):
        """Should build image mapping from scene_artwork directory."""
        mapping = build_image_mapping(temp_run_dir)

        # Should include scene artwork
        assert "page_1_intro_illustration" in mapping
        assert mapping["page_1_intro_illustration"].endswith("page_1_intro_illustration.png")

    def test_returns_empty_dict_when_no_images(self, tmp_path):
        """Should return empty dict when no image directories exist."""
        run_dir = tmp_path / "run_no_images"
        run_dir.mkdir()

        mapping = build_image_mapping(run_dir)
        assert mapping == {}

    def test_handles_mixed_image_types(self, temp_run_dir):
        """Should handle both PNG and JPG images."""
        # Add a JPG file
        map_dir = temp_run_dir / "map_assets" / "images"
        jpg_file = map_dir / "map_2.jpg"
        jpg_file.write_text("fake jpg")

        mapping = build_image_mapping(temp_run_dir)

        assert "battle_map_1" in mapping  # PNG
        assert "map_2" in mapping  # JPG


class TestJournalBasedUpload:
    """Tests for Journal-based upload workflow."""

    def test_parses_xml_to_journal(self, temp_run_dir):
        """Should parse XML files using XMLDocument and Journal models."""
        xml_dir = find_xml_directory(str(temp_run_dir))
        xml_files = list(Path(xml_dir).glob("*.xml"))

        assert len(xml_files) == 1

        # Parse using models
        xml_doc = parse_xml_file(xml_files[0])
        journal = Journal.from_xml_document(xml_doc)

        # Verify structure
        assert journal.title == "Chapter_1"
        assert len(journal.chapters) == 1
        assert journal.chapters[0].title == "Introduction"

    def test_journal_to_html_conversion(self, temp_run_dir):
        """Should convert Journal to HTML using to_foundry_html()."""
        xml_dir = find_xml_directory(str(temp_run_dir))
        xml_file = list(Path(xml_dir).glob("*.xml"))[0]

        # Parse to Journal
        xml_doc = parse_xml_file(xml_file)
        journal = Journal.from_xml_document(xml_doc)

        # Build image mapping
        image_mapping = build_image_mapping(temp_run_dir)

        # Convert to HTML
        html = journal.to_foundry_html(image_mapping)

        # Verify HTML structure
        assert "<h1>Introduction</h1>" in html
        assert "<h2>Background</h2>" in html
        assert "<h3>Getting Started</h3>" in html
        assert "<p>Welcome to the adventure.</p>" in html

    def test_image_mapping_applied_in_html(self, temp_run_dir):
        """Should apply image_mapping to ImageRef elements in HTML output."""
        xml_dir = find_xml_directory(str(temp_run_dir))
        xml_file = list(Path(xml_dir).glob("*.xml"))[0]

        xml_doc = parse_xml_file(xml_file)
        journal = Journal.from_xml_document(xml_doc)

        # Build image mapping
        image_mapping = build_image_mapping(temp_run_dir)

        # Convert to HTML
        html = journal.to_foundry_html(image_mapping)

        # Verify image reference is rendered with correct path
        assert "page_1_intro_illustration" in image_mapping
        assert f'<img src="{image_mapping["page_1_intro_illustration"]}"' in html

    @patch('foundry.upload_journal_to_foundry.FoundryClient')
    def test_upload_run_uses_journal_workflow(self, mock_client_class, temp_run_dir):
        """Should use XMLDocument -> Journal -> HTML workflow for upload."""
        # Setup mock client
        mock_client = Mock()
        mock_client.client_id = "test-client"
        mock_client.create_or_replace_journal.return_value = {
            'uuid': 'JournalEntry.test123',
            'entity': {'_id': 'test123'}
        }
        mock_client.upload_file = Mock()
        mock_client_class.return_value = mock_client

        # Call upload function
        result = upload_run_to_foundry(
            run_dir=str(temp_run_dir),
            target="local",
            journal_name="Test Journal"
        )

        # Verify FoundryClient was called
        mock_client_class.assert_called_once_with(target="local")

        # Verify create_or_replace_journal was called with pages
        mock_client.create_or_replace_journal.assert_called_once()
        call_args = mock_client.create_or_replace_journal.call_args

        assert call_args.kwargs['name'] == "Test Journal"
        pages = call_args.kwargs['pages']

        # Should have one page for the chapter
        assert len(pages) == 1
        assert pages[0]['name'] == "Chapter_1"
        assert "<h1>Introduction</h1>" in pages[0]['content']

        # Verify result
        assert result['uploaded'] == 1
        assert result['failed'] == 0
        assert result['journal_uuid'] == 'JournalEntry.test123'

    @patch('foundry.upload_journal_to_foundry.FoundryClient')
    def test_upload_preserves_semantic_hierarchy(self, mock_client_class, temp_run_dir):
        """Should preserve semantic hierarchy (not page-based) in upload."""
        mock_client = Mock()
        mock_client.client_id = "test-client"
        mock_client.create_or_replace_journal.return_value = {
            'uuid': 'JournalEntry.test123',
            'entity': {'_id': 'test123'}
        }
        mock_client.upload_file = Mock()
        mock_client_class.return_value = mock_client

        result = upload_run_to_foundry(
            run_dir=str(temp_run_dir),
            target="local",
            journal_name="Test"
        )

        # Get the HTML content that was uploaded
        call_args = mock_client.create_or_replace_journal.call_args
        pages = call_args.kwargs['pages']
        html = pages[0]['content']

        # Verify semantic structure (sections spanning pages)
        # Should have chapter title, section, and subsection
        assert "<h1>Introduction</h1>" in html
        assert "<h2>Background</h2>" in html
        assert "<h3>Getting Started</h3>" in html

        # Content from both pages should be merged under semantic hierarchy
        assert "Welcome to the adventure" in html
        assert "To begin your quest" in html


@pytest.mark.integration
def test_upload_journal_includes_positioned_images(tmp_path):
    """Test that upload includes automatically positioned images."""
    # Create mock run directory structure
    run_dir = tmp_path / "runs" / "test_run"
    docs_dir = run_dir / "documents"
    maps_dir = run_dir / "map_assets" / "images"
    scenes_dir = run_dir / "scene_artwork" / "images"

    docs_dir.mkdir(parents=True)
    maps_dir.mkdir(parents=True)
    scenes_dir.mkdir(parents=True)

    # Copy sample XML
    import shutil
    shutil.copy("tests/fixtures/sample_chapter.xml", docs_dir / "chapter_01.xml")

    # Create mock metadata files
    import json

    maps_metadata = {
        "maps": [
            {
                "name": "Test Map",
                "page_num": 5,
                "type": "battle_map",
                "source": "extracted"
            }
        ]
    }

    with open(run_dir / "map_assets" / "maps_metadata.json", "w") as f:
        json.dump(maps_metadata, f)

    # Create mock image files
    from PIL import Image
    img = Image.new('RGB', (100, 100), color='red')
    img.save(maps_dir / "page_005_test_map.png")
    img.save(scenes_dir / "scene_001_test_scene.png")

    # Load and process journal
    from foundry.upload_journal_to_foundry import load_and_position_images

    journal = load_and_position_images(run_dir)

    # Verify images are in registry with positions
    assert "page_005_test_map" in journal.image_registry
    assert journal.image_registry["page_005_test_map"].insert_before_content_id is not None


def test_load_and_position_uses_scene_metadata(tmp_path):
    """Test that scene positioning uses scenes_metadata.json."""
    run_dir = tmp_path / "run"
    docs_dir = run_dir / "documents"
    scenes_dir = run_dir / "scene_artwork" / "images"

    docs_dir.mkdir(parents=True)
    scenes_dir.mkdir(parents=True)

    # Copy XML
    import shutil
    shutil.copy("tests/fixtures/sample_chapter.xml", docs_dir / "chapter_01.xml")

    # Create scene metadata
    import json
    scenes_metadata = {
        "scenes": [
            {
                "section_path": "Chapter 1: Goblin Arrows ‚Üí Goblin Ambush",
                "name": "Forest Ambush",
                "description": "Dense forest path",
                "location_type": "outdoor",
                "image_file": "images/scene_001_forest_ambush.png"
            }
        ]
    }

    with open(run_dir / "scene_artwork" / "scenes_metadata.json", "w") as f:
        json.dump(scenes_metadata, f)

    # Create mock image
    from PIL import Image
    img = Image.new('RGB', (100, 100), color='blue')
    img.save(scenes_dir / "scene_001_forest_ambush.png")

    # Load journal
    from foundry.upload_journal_to_foundry import load_and_position_images
    journal = load_and_position_images(run_dir)

    # Verify scene was positioned using metadata
    assert "scene_forest_ambush" in journal.image_registry
</file>

<file path="tests/models/test_xml_to_html_workflow.py">
"""Integration tests for XML ‚Üí XMLDocument ‚Üí Journal ‚Üí HTML transformation workflow.

This test suite validates the complete document model pipeline:
1. XML parsing to XMLDocument models
2. Stat block extraction from XMLDocument
3. Image reference extraction and registry management
4. Journal creation from XMLDocument with semantic hierarchy
5. HTML generation for FoundryVTT from Journal
"""

import pytest
from pathlib import Path
from models import XMLDocument, Journal, ImageRef, StatBlockRaw


@pytest.mark.smoke
@pytest.mark.integration
def test_xml_document_to_journal_to_html_complete_workflow():
    """Smoke test: Validates complete XML‚ÜíXMLDocument‚ÜíJournal‚ÜíHTML transformation

    This end-to-end test validates:
    - XMLDocument parsing from real XML files
    - Journal creation from XMLDocument with semantic hierarchy
    - HTML export with image mapping
    - Round-trip XML serialization preserves data
    """
    # Use test fixture XML file
    xml_path = Path("tests/fixtures/xml/02_Part_1_Goblin_Arrows.xml")

    assert xml_path.exists(), f"Test XML file not found: {xml_path}"

    xml_string = xml_path.read_text()

    # Step 1: Parse to XMLDocument
    doc = XMLDocument.from_xml(xml_string)
    assert doc.title, "XMLDocument should have a title"
    assert len(doc.pages) > 0, "XMLDocument should have pages"

    # Step 2: Validate XMLDocument structure
    for page in doc.pages:
        assert page.number > 0, "Page numbers should be positive"

    # At least some pages should have content (not all pages may have content - some might be blank)
    pages_with_content = [p for p in doc.pages if len(p.content) > 0]
    assert len(pages_with_content) > 0, "At least some pages should have content"

    # Step 3: Create Journal from XMLDocument
    journal = Journal.from_xml_document(doc)
    assert len(journal.chapters) > 0, "Journal should have chapters"

    # Step 4: Validate Journal structure
    for chapter in journal.chapters:
        assert chapter.title, "Chapters should have titles"
        assert len(chapter.sections) > 0, "Chapters should have sections"

    # Step 5: Export to HTML
    html = journal.to_foundry_html(image_mapping={})
    assert len(html) > 0, "HTML output should not be empty"
    assert "<h1>" in html or "<h2>" in html, "HTML should contain headers"

    # Step 6: Validate round-trip XML serialization
    xml_out = doc.to_xml()
    doc2 = XMLDocument.from_xml(xml_out)
    assert doc2.title == doc.title, "Round-trip should preserve title"
    assert len(doc2.pages) == len(doc.pages), "Round-trip should preserve page count"


@pytest.mark.integration
def test_xmldocument_extracts_stat_blocks_from_xml():
    """Test XMLDocument correctly extracts and preserves stat blocks from XML.

    Validates:
    - StatBlockRaw extraction from XML <stat_block> tags
    - Stat block name and xml_element preservation
    - Integration with XMLDocument parser
    """
    # Find a real XML file with stat blocks
    xml_files = list(Path("output/runs").glob("*/documents/*.xml"))
    if not xml_files:
        pytest.skip("No XML files found in output/runs")

    stat_blocks_found = False
    for xml_path in xml_files:
        xml_string = xml_path.read_text()

        # Check if this file has stat_block tags
        if "<stat_block>" not in xml_string:
            continue

        # Parse with XMLDocument
        doc = XMLDocument.from_xml(xml_string)

        # Extract stat blocks from all pages
        all_stat_blocks = []
        for page in doc.pages:
            for content_item in page.content:
                if content_item.type == "stat_block" and isinstance(content_item.data, StatBlockRaw):
                    all_stat_blocks.append(content_item.data)

        if all_stat_blocks:
            stat_blocks_found = True

            # Validate stat blocks
            for stat_block in all_stat_blocks:
                assert stat_block.name, "Stat block should have a name"
                assert stat_block.xml_element, "Stat block should have xml_element"
                assert len(stat_block.xml_element) > 20, "Stat block xml_element should be substantial"

            break

    if not stat_blocks_found:
        pytest.skip("No stat blocks found in XML files")


@pytest.mark.integration
def test_journal_image_registry_and_html_replacement():
    """Test Journal image registry and HTML placeholder replacement.

    Validates:
    - ImageRef parsing from XML <image> tags
    - Image registry population in Journal
    - Placeholder format ({{image:path}})
    - Image URL replacement in HTML export
    """
    # Use a known good XML file - try multiple locations for one with images
    test_paths = [
        Path("output/runs/20251022_180524/documents/05_Part_4_Wave_Echo_Cave.xml"),
        Path("output/runs/20251017_111632/documents/05_Part_4_Wave_Echo_Cave.xml"),
        Path("output/runs/20251022_180524/documents/02_Part_1_Goblin_Arrows.xml"),
    ]

    images_found = False
    for xml_path in test_paths:
        if not xml_path.exists():
            continue

        xml_string = xml_path.read_text()

        # Check if this file has image tags
        if "<image" not in xml_string:
            continue

        # Parse with XMLDocument
        doc = XMLDocument.from_xml(xml_string)

        # Create Journal and check image registry
        journal = Journal.from_xml_document(doc)

        if journal.images:
            images_found = True

            # Validate image metadata
            for img_path, img_meta in journal.images.items():
                assert img_meta.original_path, "Image should have original_path"
                assert img_meta.placeholder, "Image should have placeholder"
                assert img_meta.placeholder.startswith("{{image:"), "Placeholder should use {{image:}} format"
                assert img_meta.alt_text or img_meta.caption, "Image should have alt_text or caption"

            # Test HTML export with image mapping
            image_mapping = {
                img_path: f"https://example.com/{img_path}"
                for img_path in journal.images.keys()
            }
            html = journal.to_foundry_html(image_mapping=image_mapping)

            # Validate images were replaced in HTML
            for img_path, url in image_mapping.items():
                # Check that placeholder was replaced
                placeholder = journal.images[img_path].placeholder
                assert placeholder not in html, f"Placeholder {placeholder} should be replaced"
                # Note: We don't check for the URL directly as HTML structure may vary

            break

    if not images_found:
        pytest.skip("No images found in XML files")


@pytest.mark.integration
def test_minimal_xml_document_converts_to_html():
    """Test that minimal XML documents convert correctly to HTML without errors."""
    minimal_xml = """
    <Chapter_01>
      <page number="1">
        <chapter_title>Test Chapter</chapter_title>
        <p>Single paragraph.</p>
      </page>
    </Chapter_01>
    """

    # Should parse without errors
    doc = XMLDocument.from_xml(minimal_xml)
    assert doc.title == "Chapter_01"
    assert len(doc.pages) == 1

    # Should create journal without errors
    journal = Journal.from_xml_document(doc)
    assert len(journal.chapters) == 1

    # Should export HTML without errors
    html = journal.to_foundry_html(image_mapping={})
    assert len(html) > 0
    assert "Test Chapter" in html
    assert "Single paragraph" in html


@pytest.mark.integration
def test_tables_lists_and_definition_lists_convert_to_html():
    """Test that complex content types (tables, lists, definition lists) convert correctly to HTML."""
    complex_xml = """
    <Chapter_01>
      <page number="1">
        <chapter_title>Complex Content</chapter_title>

        <section>Lists and Tables</section>

        <list>
          <li>First item</li>
          <li>Second item</li>
        </list>

        <table>
          <tr>
            <td>Cell 1</td>
            <td>Cell 2</td>
          </tr>
          <tr>
            <td>Cell 3</td>
            <td>Cell 4</td>
          </tr>
        </table>

        <definition_list>
          <dt>Term 1</dt>
          <dd>Definition 1</dd>
          <dt>Term 2</dt>
          <dd>Definition 2</dd>
        </definition_list>
      </page>
    </Chapter_01>
    """

    # Parse and validate
    doc = XMLDocument.from_xml(complex_xml)
    assert len(doc.pages) == 1

    # Check content types were parsed
    content_types = [item.type for item in doc.pages[0].content]
    assert "list" in content_types
    assert "table" in content_types
    assert "definition_list" in content_types

    # Create journal and export HTML
    journal = Journal.from_xml_document(doc)
    html = journal.to_foundry_html(image_mapping={})

    # Validate HTML contains expected elements
    assert "<ul>" in html or "<ol>" in html, "HTML should contain list"
    assert "table" in html.lower(), "HTML should contain table"
    assert "<dl>" in html, "HTML should contain definition list"


@pytest.mark.integration
def test_multi_page_chapter_flattens_to_journal_hierarchy():
    """Test that multi-page chapters correctly flatten into Journal's semantic hierarchy."""
    multi_page_xml = """
    <Chapter_02>
      <page number="1">
        <chapter_title>Multi-Page Chapter</chapter_title>
        <section>Section 1</section>
        <p>Content on page 1.</p>
      </page>
      <page number="2">
        <section>Section 2</section>
        <p>Content on page 2.</p>
      </page>
      <page number="3">
        <subsection>Subsection 2.1</subsection>
        <p>Content on page 3.</p>
      </page>
    </Chapter_02>
    """

    # Parse and validate
    doc = XMLDocument.from_xml(multi_page_xml)
    assert len(doc.pages) == 3

    # Create journal and validate flattening
    journal = Journal.from_xml_document(doc)
    assert len(journal.chapters) == 1

    chapter = journal.chapters[0]
    assert len(chapter.sections) >= 2, "Should have at least 2 sections"

    # Export HTML and validate structure
    html = journal.to_foundry_html(image_mapping={})
    assert "Section 1" in html
    assert "Section 2" in html
    assert "Subsection 2.1" in html
</file>

<file path="tests/util/test_gemini.py">
"""
Tests for src/util/gemini.py

Basic tests for Gemini API wrapper functionality.
"""

import pytest
from pathlib import Path
import sys
import os
import tempfile

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from util.gemini import GeminiAPI, GeminiFileContext, configure_gemini


class TestGeminiAPIBasics:
    """Test basic Gemini API functionality."""

    def test_configure_from_env(self, check_api_key):
        """Test configuration from environment variables."""
        api = GeminiAPI()
        assert api._configured is True
        assert api.model_name == "gemini-2.5-pro"

    def test_configure_with_custom_model(self, check_api_key):
        """Test configuration with custom model name."""
        api = GeminiAPI(model_name="gemini-1.5-pro")
        assert api.model_name == "gemini-1.5-pro"
        assert api._configured is True

    def test_configure_without_api_key_raises_error(self, monkeypatch, tmp_path):
        """Test that missing API key raises ValueError."""
        monkeypatch.delenv("GeminiImageAPI", raising=False)
        # Patch load_dotenv to prevent loading from actual .env file
        monkeypatch.setattr("util.gemini.load_dotenv", lambda **kwargs: None)
        with pytest.raises(ValueError, match="Gemini API key not found"):
            GeminiAPI()

    def test_convenience_function(self, check_api_key):
        """Test the configure_gemini convenience function."""
        api = configure_gemini()
        assert isinstance(api, GeminiAPI)
        assert api._configured is True

    def test_create_model(self, check_api_key):
        """Test creating a Gemini client instance."""
        api = GeminiAPI()
        client = api.create_model()
        assert client is not None
        assert hasattr(client, "models")

    def test_create_model_without_configuration(self):
        """Test that creating model without configuration raises error."""
        api = GeminiAPI.__new__(GeminiAPI)
        api._configured = False
        with pytest.raises(RuntimeError, match="Gemini API not configured"):
            api.create_model()


class TestGeminiAPIIntegration:
    """Integration tests that make real API calls."""

    @pytest.mark.integration
    @pytest.mark.slow
    @pytest.mark.requires_api
    def test_generate_text_content(self, check_api_key):
        """Test basic text generation."""
        api = GeminiAPI()
        response = api.generate_content("Say 'Hello World' and nothing else.")
        assert response is not None
        assert hasattr(response, "text")
        assert "Hello" in response.text or "hello" in response.text

    @pytest.mark.integration
    @pytest.mark.slow
    @pytest.mark.requires_api
    def test_file_upload_and_deletion(self, check_api_key):
        """Test file upload and deletion."""
        api = GeminiAPI()

        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write("Test content")
            temp_file_path = f.name

        try:
            uploaded_file = api.upload_file(temp_file_path, display_name="test")
            assert uploaded_file is not None
            assert hasattr(uploaded_file, "name")
            api.delete_file(uploaded_file.name)
        finally:
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)

    @pytest.mark.integration
    @pytest.mark.slow
    @pytest.mark.requires_api
    def test_context_manager(self, check_api_key):
        """Test context manager auto-cleanup."""
        api = GeminiAPI()

        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write("The answer is: 42")
            temp_file_path = f.name

        try:
            with GeminiFileContext(api, temp_file_path, "test") as uploaded_file:
                response = api.generate_content(
                    "What is the answer in the file?",
                    file_obj=uploaded_file
                )
                assert response is not None
                assert "42" in response.text
        finally:
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_generate_content_async_wrapper(self, check_api_key):
        """Test async wrapper for generate_content."""
        from util.gemini import generate_content_async
        from google import genai

        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GeminiImageAPI")
        if not api_key:
            pytest.skip("No API key available")

        client = genai.Client(api_key=api_key)

        response = await generate_content_async(
            client=client,
            model="gemini-2.0-flash",
            contents="Say 'test' in one word",
            config={'temperature': 0.0}
        )

        assert response.text
        assert isinstance(response.text, str)
</file>

<file path="src/actors/generate_actor_file.py">
"""Generate D&D 5e actor text files from descriptions using Gemini.

This module uses Gemini to create complete stat blocks and bios from natural
language descriptions. The generated text files can then be processed through
the existing pipeline:
  1. Text file ‚Üí parse_raw_text_to_statblock() ‚Üí StatBlock
  2. StatBlock ‚Üí parse_stat_block_parallel() ‚Üí ParsedActorData
  3. ParsedActorData ‚Üí convert_to_foundry() ‚Üí FoundryVTT actor JSON
"""

import asyncio
import logging
import os
from pathlib import Path
from typing import Optional

from google import genai
from dotenv import load_dotenv

from util.gemini import generate_content_async

# Load environment
PROJECT_ROOT = Path(__file__).parent.parent.parent
load_dotenv(PROJECT_ROOT / ".env")

logger = logging.getLogger(__name__)

# Model configuration
DEFAULT_MODEL = "gemini-2.5-pro"
GENERATION_TEMPERATURE = 0.7  # Creative but consistent


async def generate_actor_description(
    description: str,
    challenge_rating: Optional[float] = None,
    model_name: str = DEFAULT_MODEL
) -> str:
    """
    Generate a complete D&D 5e stat block from natural language description.

    This is a simpler version that returns the raw text directly (used by orchestrate.py).
    For a version that saves to file with more options, use generate_actor_from_description.

    Args:
        description: Natural language description of the actor
        challenge_rating: Optional CR (0.125-30). If None, Gemini determines it.
        model_name: Gemini model to use (default: gemini-2.5-pro)

    Returns:
        Generated stat block text in D&D 5e format

    Example:
        raw_text = await generate_actor_description(
            "A cunning goblin assassin with poisoned daggers",
            challenge_rating=2.0
        )
    """
    # Use the full version but don't save to file
    temp_output = Path("/tmp") / "temp_actor.txt"
    result_path = await generate_actor_from_description(
        description=description,
        challenge_rating=challenge_rating,
        output_path=temp_output,
        model_name=model_name
    )

    # Read the generated text and return it
    text = result_path.read_text(encoding="utf-8")

    # Clean up temp file
    try:
        result_path.unlink()
    except Exception:
        pass

    return text


async def generate_actor_from_description(
    description: str,
    challenge_rating: Optional[float] = None,
    name: Optional[str] = None,
    bio_context: Optional[str] = None,
    output_path: Optional[Path] = None,
    model_name: str = DEFAULT_MODEL
) -> Path:
    """
    Generate a complete D&D 5e actor file from a description using Gemini.

    Args:
        description: Natural language description of the creature/NPC
        challenge_rating: Optional CR (0.125, 0.25, 0.5, 1-30). If not provided, Gemini will determine appropriate CR.
        name: Optional custom name (Gemini will generate if not provided)
        bio_context: Optional additional context for biography
        output_path: Optional custom output path (defaults to data/actors/<name>.txt)
        model_name: Gemini model to use (default: gemini-2.5-pro)

    Returns:
        Path to generated file

    Example:
        >>> # With explicit CR
        >>> path = await generate_actor_from_description(
        ...     description="A mutated sea creature with acidic tentacles and telepathic abilities",
        ...     challenge_rating=5,
        ...     bio_context="Found in the ruins of an underwater temple"
        ... )
        >>> # Let Gemini determine CR
        >>> path = await generate_actor_from_description(
        ...     description="An ancient lich with reality-warping powers",
        ...     bio_context="Destroyed kingdoms millennia ago"
        ... )
    """
    # Build comprehensive prompt
    cr_examples = {
        0.125: "CR 1/8 (25 XP) - weak creatures like kobolds",
        0.25: "CR 1/4 (50 XP) - goblins, skeletons",
        0.5: "CR 1/2 (100 XP) - orcs, giant wasps",
        1: "CR 1 (200 XP) - dire wolves, animated armor",
        2: "CR 2 (450 XP) - ogres, griffons",
        5: "CR 5 (1,800 XP) - hill giants, troll",
        10: "CR 10 (5,900 XP) - stone golems, young dragons",
        20: "CR 20 (25,000 XP) - pit fiends, ancient dragons"
    }

    if challenge_rating is not None:
        cr_guidance = cr_examples.get(challenge_rating, f"CR {challenge_rating}")
        cr_instruction = f"""CHALLENGE RATING: {challenge_rating} ({cr_guidance})

Use this exact CR for the stat block."""
    else:
        cr_instruction = """CHALLENGE RATING: Not specified - you must determine the appropriate CR.

REASONING PROCESS:
1. Analyze the description for power indicators:
   - Magical abilities (spells, innate magic)
   - Physical capabilities (size, strength, special attacks)
   - Defensive abilities (resistances, immunities, regeneration)
   - Tactical complexity (legendary actions, lair actions)
   - Narrative role (minion, boss, world-ending threat)

2. Consider CR benchmarks:
   - CR 0-1/4: Weak creatures (kobolds, skeletons, commoners)
   - CR 1/2-2: Common threats (orcs, goblins, zombies)
   - CR 3-5: Dangerous foes (ogres, griffons, basic spellcasters)
   - CR 6-10: Serious threats (giants, young dragons, mages)
   - CR 11-15: Deadly encounters (adult dragons, powerful demons)
   - CR 16-20: Legendary threats (ancient dragons, pit fiends)
   - CR 21-30: World-ending entities (demon lords, gods)

3. Select the most appropriate CR based on your analysis.
4. State your reasoning briefly after the stat block (before the Bio section).

Format:
[Stat block]

CR Reasoning: [1-2 sentences explaining why you chose this CR]

Bio
[Biography]"""

    prompt = f"""Generate a complete D&D 5e stat block and biography for a creature.

DESCRIPTION:
{description}

{cr_instruction}
{f"NAME: {name}" if name else ""}
{f"BIO CONTEXT: {bio_context}" if bio_context else ""}

REQUIREMENTS:
1. Create a balanced D&D 5e stat block appropriate for the chosen CR
2. Use official D&D 5e stat block format (see example below)
3. Include appropriate abilities, traits, and actions for the CR
4. Make the creature interesting and mechanically sound
5. Add a 2-3 paragraph biography that fits the description

STAT BLOCK FORMAT (follow exactly):
```
Creature Name
Size Type, Alignment
Armor Class X (description)
Hit Points X (XdY + Z)
Speed X ft., [additional movement]
STR
XX (+X)
DEX
XX (+X)
CON
XX (+X)
INT
XX (+X)
WIS
XX (+X)
CHA
XX (+X)
[Saving Throws ...]
[Skills ...]
[Damage Resistances ...]
[Damage Immunities ...]
[Damage Vulnerabilities ...]
[Condition Immunities ...]
Senses ..., Passive Perception XX
Languages ...
Challenge {challenge_rating} (XP)
Proficiency Bonus +X
Traits

Trait Name. Trait description.

[More traits...]
Actions

Action Name. Action description with mechanics.

[More actions...]

[Reactions]

Reaction Name. Reaction description.

[Legendary Actions]

The [creature] can take 3 legendary actions...
```

STAT BLOCK GUIDELINES:
- HP should be appropriate for the CR (roughly 15-30 per CR level)
- Attack bonuses should be roughly CR/2 + 5 (minimum +3)
- Damage should scale with CR (roughly 7-15 damage per CR level)
- Save DCs should be roughly 8 + proficiency bonus + ability modifier
- Include 1-3 interesting traits
- Include 2-5 actions (including Multiattack if CR >= 3)
- Consider legendary actions if CR >= 10
- Ensure all numbers are mechanically balanced for the chosen CR

After the stat block, add:

Bio

[2-3 paragraph biography describing the creature's nature, habitat, behavior, and role in the world]

OUTPUT ONLY THE STAT BLOCK AND BIO. No additional commentary or explanations.
"""

    # Call Gemini
    if challenge_rating is not None:
        logger.info(f"Generating CR {challenge_rating} creature: {description[:100]}...")
    else:
        logger.info(f"Generating creature (auto CR): {description[:100]}...")

    # Initialize client (uses GEMINI_API_KEY or GeminiImageAPI env var)
    client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))

    response = await generate_content_async(
        client=client,
        model=model_name,
        contents=prompt,
        config={'temperature': GENERATION_TEMPERATURE}
    )

    generated_text = response.text.strip()

    # Extract creature name from first line if not provided
    if name is None:
        first_line = generated_text.split('\n')[0].strip()
        name = first_line
        logger.info(f"Generated creature: {name}")

    # Determine output path
    if output_path is None:
        output_dir = PROJECT_ROOT / "data" / "actors"
        output_dir.mkdir(parents=True, exist_ok=True)

        # Sanitize filename
        safe_name = name.lower().replace(" ", "_").replace("'", "")
        output_path = output_dir / f"{safe_name}.txt"

    # Write to file
    output_path.write_text(generated_text, encoding="utf-8")

    logger.info(f"Generated actor file: {output_path}")
    return output_path


async def generate_multiple_actors(
    descriptions: list[tuple[str, Optional[float]]],
    output_dir: Optional[Path] = None
) -> list[Path]:
    """
    Generate multiple actor files in parallel.

    Args:
        descriptions: List of (description, challenge_rating) tuples. CR can be None for auto-determination.
        output_dir: Optional output directory (defaults to data/actors/)

    Returns:
        List of paths to generated files

    Example:
        >>> paths = await generate_multiple_actors([
        ...     ("A fire-breathing dragon", 15),
        ...     ("A cunning thief", None),  # Auto CR
        ...     ("An ancient lich", 21)
        ... ])
    """
    tasks = [
        generate_actor_from_description(desc, cr, output_path=output_dir)
        for desc, cr in descriptions
    ]

    return await asyncio.gather(*tasks)


# Synchronous wrapper for convenience
def generate_actor_sync(
    description: str,
    challenge_rating: Optional[float] = None,
    name: Optional[str] = None,
    bio_context: Optional[str] = None,
    output_path: Optional[Path] = None,
    model_name: str = DEFAULT_MODEL
) -> Path:
    """
    Synchronous wrapper for generate_actor_from_description.

    See generate_actor_from_description for full documentation.
    """
    return asyncio.run(generate_actor_from_description(
        description, challenge_rating, name, bio_context, output_path, model_name
    ))


# Example usage
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    async def main():
        # Example 1: Explicit CR - crystalline spider
        path1 = await generate_actor_from_description(
            description="A crystalline spider that feeds on magical energy",
            challenge_rating=3,
            bio_context="Lives in the Underdark near wizard towers"
        )
        print(f"Generated: {path1}")

        # Example 2: Auto CR - let Gemini decide based on description
        path2 = await generate_actor_from_description(
            description="A half-orc warrior who was raised by elves and fights with grace",
            name="Grok Silverblade",
            bio_context="Leader of the Moonwood Rangers"
        )
        print(f"Generated (auto CR): {path2}")

        # Example 3: Auto CR - ancient lich (should be high CR)
        path3 = await generate_actor_from_description(
            description="An ancient lich with reality-warping powers who destroyed entire kingdoms",
            bio_context="Awakened after millennia of slumber beneath a cursed mountain"
        )
        print(f"Generated (auto CR): {path3}")

        # Example 4: Explicit high CR - elemental titan
        path4 = await generate_actor_from_description(
            description="An elemental titan made of living storm clouds",
            challenge_rating=18,
            bio_context="Created by an ancient god as punishment for hubris"
        )
        print(f"Generated: {path4}")

    asyncio.run(main())
</file>

<file path="src/foundry/actors/parser.py">
"""Parallel parser for converting StatBlock to ParsedActorData using Gemini."""

import asyncio
import json
import logging
from typing import Optional, Union
from pathlib import Path
import os

from google import genai
from dotenv import load_dotenv

from actors.models import StatBlock
from foundry.actors.models import (
    ParsedActorData, Attack, Trait, Multiattack,
    InnateSpellcasting, InnateSpell, DamageFormula, AttackSave,
    SkillProficiency, DamageModification
)
from foundry.actors.spell_cache import SpellCache
from util.gemini import generate_content_async

# Load environment
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
load_dotenv(PROJECT_ROOT / ".env")

logger = logging.getLogger(__name__)

# Model configuration
DEFAULT_MODEL = "gemini-2.0-flash"
PARSE_TEMPERATURE = 0.0  # Deterministic parsing


def parse_senses(senses_str: Optional[str]) -> dict:
    """
    Parse senses string into structured fields.

    Example: "Darkvision 60 ft., Passive Perception 14"
    Returns: {"darkvision": 60, "passive_perception": 14}
    """
    import re

    if not senses_str:
        return {}

    result = {}

    # Parse darkvision (e.g., "Darkvision 60 ft.")
    darkvision_match = re.search(r'darkvision\s+(\d+)\s*ft', senses_str, re.IGNORECASE)
    if darkvision_match:
        result["darkvision"] = int(darkvision_match.group(1))

    # Parse blindsight
    blindsight_match = re.search(r'blindsight\s+(\d+)\s*ft', senses_str, re.IGNORECASE)
    if blindsight_match:
        result["blindsight"] = int(blindsight_match.group(1))

    # Parse tremorsense
    tremorsense_match = re.search(r'tremorsense\s+(\d+)\s*ft', senses_str, re.IGNORECASE)
    if tremorsense_match:
        result["tremorsense"] = int(tremorsense_match.group(1))

    # Parse truesight
    truesight_match = re.search(r'truesight\s+(\d+)\s*ft', senses_str, re.IGNORECASE)
    if truesight_match:
        result["truesight"] = int(truesight_match.group(1))

    # Parse passive perception
    passive_match = re.search(r'passive\s+perception\s+(\d+)', senses_str, re.IGNORECASE)
    if passive_match:
        result["passive_perception"] = int(passive_match.group(1))

    return result


async def parse_single_action_async(
    action_text: str,
    model_name: str = DEFAULT_MODEL
) -> Union[Attack, Multiattack, Trait]:
    """
    Parse a single action entry into Attack, Multiattack, or Trait.

    Actions can be:
    - Attacks (e.g., "Tentacles. Melee Weapon Attack: +5...")
    - Multiattack (e.g., "Multiattack. The creature makes...")
    - Special actions (e.g., "Ink Cloud. A 20-foot-radius cloud...")

    Args:
        action_text: Raw action text
        model_name: Gemini model to use

    Returns:
        Attack, Multiattack, or Trait object
    """
    # Detect if this is a multiattack
    is_multiattack = "multiattack" in action_text.lower()

    # Detect if this is an attack (supports both 2014 and 2024 D&D formats)
    # 2014: "Melee Weapon Attack:" or "Ranged Weapon Attack:"
    # 2024: "Melee Attack Roll:" or "Ranged Attack Roll:"
    is_attack = (
        "weapon attack:" in action_text.lower() or
        "spell attack:" in action_text.lower() or
        "attack roll:" in action_text.lower()
    )

    if is_multiattack:
        schema = {
            "name": "Multiattack",
            "description": "The pit fiend makes four attacks...",
            "num_attacks": 4
        }
        prompt = f"""
Parse this D&D 5e multiattack action into JSON.

ACTION TEXT:
{action_text}

OUTPUT JSON SCHEMA:
{{
  "name": "string (action name)",
  "description": "string (full description)",
  "num_attacks": integer (number of attacks, e.g., 'three attacks' ‚Üí 3)
}}

Extract the number of attacks from phrases like "makes X attacks", "three attacks", etc.

OUTPUT ONLY VALID JSON. No explanations.
"""
    elif is_attack:
        schema = {
            "name": "Scimitar",
            "attack_type": "melee",
            "attack_bonus": 4,
            "reach": 5,
            "damage": [
                {"number": 1, "denomination": 6, "bonus": "+2", "type": "slashing"}
            ],
            "attack_save": {
                "ability": "con",
                "dc": 13,
                "damage": [{"number": 2, "denomination": 6, "bonus": "", "type": "poison"}],
                "on_save": "half"
            }
        }
        prompt = f"""
Parse this D&D 5e attack action into JSON.

ACTION TEXT:
{action_text}

OUTPUT JSON SCHEMA:
{{
  "name": "string (weapon name)",
  "attack_type": "string ('melee' or 'ranged')",
  "attack_bonus": integer (e.g., '+4 to hit' ‚Üí 4),
  "reach": integer (for melee, in feet),
  "range": integer (for ranged, short range in feet),
  "range_long": integer (for ranged, long range in feet),
  "damage": [
    {{
      "number": integer (dice count, e.g., '2d6' ‚Üí 2),
      "denomination": integer (die size, e.g., '2d6' ‚Üí 6),
      "bonus": "string (modifier with sign, e.g., '+2' or '')",
      "type": "string (damage type: slashing, piercing, bludgeoning, fire, etc.)"
    }}
  ],
  "attack_save": {{  // OPTIONAL - only if attack requires a saving throw
    "ability": "string (con, dex, wis, etc.)",
    "dc": integer,
    "damage": [{{...}}],  // Damage on failed save
    "ongoing_damage": [{{...}}],  // OPTIONAL - damage each turn
    "duration_rounds": integer,  // OPTIONAL - duration of ongoing effect
    "on_save": "string ('half', 'none', 'negates')",
    "effect_description": "string"  // OPTIONAL
  }}
}}

PARSING RULES:
1. attack_type: "melee" for melee attacks, "ranged" for ranged attacks, "melee_ranged" for "Melee or Ranged"
2. attack_bonus: Extract from "+X to hit" or "Attack Roll: +X" (just the number)
3. reach: For melee, extract from "reach X ft"
4. range/range_long: For ranged, extract from "range X/Y ft" or "range X ft"
5. damage: Parse "XdY+Z damage_type" or "X (XdY + Z) damage_type" format. Can have multiple damage entries.
6. attack_save: Include ONLY if the attack description mentions a saving throw (DC X ability save)
7. ongoing_damage: Include ONLY if damage continues each turn/round

SUPPORTS BOTH 2014 AND 2024 D&D FORMATS:
- 2014: "Melee Weapon Attack: +5 to hit"
- 2024: "Melee Attack Roll: +12, reach 5 ft. Hit: 31 (4d12 + 5) Force damage"

OUTPUT ONLY VALID JSON. No explanations.
"""
    else:
        # Special action (not an attack) - parse as Trait with activation
        prompt = f"""
Parse this D&D 5e special action into JSON.

ACTION TEXT:
{action_text}

OUTPUT JSON SCHEMA:
{{
  "name": "string (action name)",
  "description": "string (full description)",
  "activation": "string ('action', 'bonus', 'reaction', 'legendary')"
}}

ACTIVATION TYPE RULES:
- "action": Requires an action to use (most common for special actions like Breath Weapon, Ink Cloud)
- "bonus": Requires a bonus action
- "reaction": Triggered reaction
- "legendary": Legendary action

Most special actions use "action" unless the description mentions bonus action, reaction, or legendary.

OUTPUT ONLY VALID JSON. No explanations.
"""

    # Call Gemini
    client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))
    response = await generate_content_async(
        client=client,
        model=model_name,
        contents=prompt,
        config={
            'temperature': PARSE_TEMPERATURE,
            'response_mime_type': 'application/json'
        }
    )

    # Parse response
    parsed_json = json.loads(response.text)
    logger.debug(f"Gemini response type: {type(parsed_json)}, value: {parsed_json}")

    # Handle case where Gemini returns a list instead of dict
    if isinstance(parsed_json, list):
        if len(parsed_json) == 0:
            raise ValueError(f"Gemini returned empty list for action: {action_text[:100]}")
        parsed_json = parsed_json[0]  # Take first element
        logger.warning(f"Gemini returned list instead of dict, using first element")

    # Create appropriate object
    if is_multiattack:
        return Multiattack(**parsed_json)
    elif is_attack:
        # Rename "range" to "range_short" for backwards compatibility
        if "range" in parsed_json:
            parsed_json["range_short"] = parsed_json.pop("range")

        # Convert damage formulas
        if "damage" in parsed_json:
            parsed_json["damage"] = [
                DamageFormula(**dmg) for dmg in parsed_json["damage"]
            ]

        # Convert attack save if present
        if "attack_save" in parsed_json and parsed_json["attack_save"]:
            save_data = parsed_json["attack_save"]
            if "damage" in save_data:
                save_data["damage"] = [DamageFormula(**dmg) for dmg in save_data["damage"]]
            if "ongoing_damage" in save_data:
                save_data["ongoing_damage"] = [DamageFormula(**dmg) for dmg in save_data["ongoing_damage"]]
            parsed_json["attack_save"] = AttackSave(**save_data)

        return Attack(**parsed_json)
    else:
        # Special action - return as Trait
        return Trait(**parsed_json)


async def parse_single_trait_async(
    trait_text: str,
    spell_cache: Optional[SpellCache] = None,
    model_name: str = DEFAULT_MODEL
) -> Union[Trait, InnateSpellcasting]:
    """
    Parse a single trait entry into Trait or InnateSpellcasting.

    Args:
        trait_text: Raw trait text
        spell_cache: Optional spell cache for UUID resolution
        model_name: Gemini model to use

    Returns:
        Trait or InnateSpellcasting object
    """
    # Detect if this is innate spellcasting
    is_spellcasting = "spellcasting" in trait_text.lower() and "innate" in trait_text.lower()

    if is_spellcasting:
        return await parse_innate_spellcasting_async(trait_text, spell_cache, model_name)
    else:
        prompt = f"""
Parse this D&D 5e trait/feature into JSON.

TRAIT TEXT:
{trait_text}

OUTPUT JSON SCHEMA:
{{
  "name": "string (trait name, e.g., 'Nimble Escape')",
  "description": "string (full description)",
  "activation": "string ('passive', 'action', 'bonus', 'reaction', 'legendary')"
}}

ACTIVATION TYPE RULES:
- "passive": Always active, no action required (e.g., Magic Resistance, Keen Senses)
- "action": Requires an action to use (e.g., Breath Weapon, Frightful Presence)
- "bonus": Requires a bonus action (e.g., Nimble Escape allows Disengage/Hide as bonus action)
- "reaction": Triggered reaction (e.g., Parry, Shield)
- "legendary": Legendary action

Most traits are "passive" unless the description mentions an action type.

OUTPUT ONLY VALID JSON. No explanations.
"""

        client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))
        response = await generate_content_async(
            client=client,
            model=model_name,
            contents=prompt,
            config={
                'temperature': PARSE_TEMPERATURE,
                'response_mime_type': 'application/json'
            }
        )

        parsed_json = json.loads(response.text)
        logger.debug(f"Gemini trait response type: {type(parsed_json)}, value: {parsed_json}")

        # Handle case where Gemini returns a list instead of dict
        if isinstance(parsed_json, list):
            if len(parsed_json) == 0:
                raise ValueError(f"Gemini returned empty list for trait: {trait_text[:100]}")
            parsed_json = parsed_json[0]
            logger.warning(f"Gemini returned list instead of dict for trait, using first element")

        return Trait(**parsed_json)


async def parse_innate_spellcasting_async(
    trait_text: str,
    spell_cache: Optional[SpellCache] = None,
    model_name: str = DEFAULT_MODEL
) -> InnateSpellcasting:
    """
    Parse innate spellcasting trait with spell UUID resolution.

    Args:
        trait_text: Raw innate spellcasting trait text
        spell_cache: Optional spell cache for UUID resolution
        model_name: Gemini model to use

    Returns:
        InnateSpellcasting object with resolved spell UUIDs
    """
    prompt = f"""
Parse this D&D 5e innate spellcasting trait into JSON.

TRAIT TEXT:
{trait_text}

OUTPUT JSON SCHEMA:
{{
  "ability": "string (charisma, intelligence, wisdom)",
  "save_dc": integer,
  "spells": [
    {{
      "name": "string (spell name, lowercase)",
      "frequency": "string ('at will', '3/day', '1/day', etc.)",
      "uses": integer (OPTIONAL - only for limited use spells, e.g., '3/day' ‚Üí 3)
    }}
  ]
}}

PARSING RULES:
1. ability: Extract from "(Charisma)" or similar in the trait name
2. save_dc: Extract from "spell save DC X"
3. spells: Parse spell list organized by frequency
   - "At will: detect magic, fireball" ‚Üí frequency: "at will"
   - "3/day each: hold monster" ‚Üí frequency: "3/day", uses: 3
   - "1/day: wish" ‚Üí frequency: "1/day", uses: 1
4. Spell names should be lowercase and match official D&D 5e spell names

OUTPUT ONLY VALID JSON. No explanations.
"""

    client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))
    response = await generate_content_async(
        client=client,
        model=model_name,
        contents=prompt,
        config={
            'temperature': PARSE_TEMPERATURE,
            'response_mime_type': 'application/json'
        }
    )

    parsed_json = json.loads(response.text)
    logger.debug(f"Gemini spellcasting response type: {type(parsed_json)}, value: {parsed_json}")

    # Handle case where Gemini returns a list instead of dict
    if isinstance(parsed_json, list):
        if len(parsed_json) == 0:
            raise ValueError(f"Gemini returned empty list for spellcasting: {trait_text[:100]}")
        parsed_json = parsed_json[0]
        logger.warning(f"Gemini returned list instead of dict for spellcasting, using first element")

    # Resolve spell UUIDs if spell_cache provided
    if spell_cache:
        for spell in parsed_json["spells"]:
            uuid = spell_cache.get_spell_uuid(spell["name"])
            if uuid:
                spell["uuid"] = uuid
            else:
                logger.warning(f"Spell '{spell['name']}' not found in cache")

    # Create InnateSpell objects
    parsed_json["spells"] = [InnateSpell(**spell) for spell in parsed_json["spells"]]

    return InnateSpellcasting(**parsed_json)


async def parse_stat_block_parallel(
    stat_block: StatBlock,
    spell_cache: Optional[SpellCache] = None,
    model_name: str = DEFAULT_MODEL
) -> ParsedActorData:
    """
    Parse StatBlock to ParsedActorData with maximum parallelization.

    Each action, trait, and reaction is parsed in parallel using async Gemini calls.

    Args:
        stat_block: StatBlock with pre-split lists
        spell_cache: Optional spell cache for spell UUID resolution
        model_name: Gemini model to use

    Returns:
        ParsedActorData with fully structured attacks, traits, spells
    """
    logger.info(f"Parsing {stat_block.name} with {len(stat_block.actions)} actions, {len(stat_block.traits)} traits")

    # Create parse tasks for all items in parallel
    action_tasks = [
        parse_single_action_async(action_text, model_name)
        for action_text in stat_block.actions
    ]

    trait_tasks = [
        parse_single_trait_async(trait_text, spell_cache, model_name)
        for trait_text in stat_block.traits
    ]

    reaction_tasks = [
        parse_single_trait_async(reaction_text, spell_cache, model_name)
        for reaction_text in stat_block.reactions
    ]

    legendary_action_tasks = [
        parse_single_trait_async(legendary_text, spell_cache, model_name)
        for legendary_text in stat_block.legendary_actions
    ]

    # Run all tasks in parallel
    logger.debug(f"Starting {len(action_tasks) + len(trait_tasks) + len(reaction_tasks) + len(legendary_action_tasks)} parallel Gemini calls")

    action_results, trait_results, reaction_results, legendary_results = await asyncio.gather(
        asyncio.gather(*action_tasks) if action_tasks else asyncio.sleep(0, result=[]),
        asyncio.gather(*trait_tasks) if trait_tasks else asyncio.sleep(0, result=[]),
        asyncio.gather(*reaction_tasks) if reaction_tasks else asyncio.sleep(0, result=[]),
        asyncio.gather(*legendary_action_tasks) if legendary_action_tasks else asyncio.sleep(0, result=[])
    )

    # Separate multiattack from regular attacks and special actions
    multiattack = None
    attacks = []
    traits = []
    for result in action_results:
        if isinstance(result, Multiattack):
            multiattack = result
        elif isinstance(result, Attack):
            attacks.append(result)
        elif isinstance(result, Trait):
            # Special actions (like Ink Cloud) parsed as Traits
            traits.append(result)

    # Separate innate spellcasting from regular traits
    innate_spellcasting = None
    for result in trait_results:
        if isinstance(result, InnateSpellcasting):
            innate_spellcasting = result
        elif isinstance(result, Trait):
            traits.append(result)

    # Reactions are just traits with reaction activation
    traits.extend([t for t in reaction_results if isinstance(t, Trait)])

    # Legendary actions are traits with legendary activation
    traits.extend([t for t in legendary_results if isinstance(t, Trait)])

    logger.info(f"Parsed {stat_block.name}: {len(attacks)} attacks, {len(traits)} traits, {multiattack is not None} multiattack, {innate_spellcasting is not None} spellcasting")

    # Convert saving throws to proficiency list
    saving_throw_proficiencies = []
    if stat_block.saving_throws:
        saving_throw_proficiencies = list(stat_block.saving_throws.keys())

    # Convert skills to SkillProficiency objects
    skill_proficiencies = []
    if stat_block.skills:
        for skill_name, bonus in stat_block.skills.items():
            skill_proficiencies.append(SkillProficiency(
                skill=skill_name.capitalize(),
                bonus=bonus,
                proficiency_level=1  # Assume proficient (not expertise)
            ))

    # Parse senses
    senses = parse_senses(stat_block.senses)

    # Parse damage modifiers into DamageModification objects
    damage_resistances = None
    if stat_block.damage_resistances:
        types = [r.strip().lower() for r in stat_block.damage_resistances.split(',')]
        damage_resistances = DamageModification(types=types)

    damage_immunities = None
    if stat_block.damage_immunities:
        types = [i.strip().lower() for i in stat_block.damage_immunities.split(',')]
        damage_immunities = DamageModification(types=types)

    damage_vulnerabilities = None
    if stat_block.damage_vulnerabilities:
        types = [v.strip().lower() for v in stat_block.damage_vulnerabilities.split(',')]
        damage_vulnerabilities = DamageModification(types=types)

    condition_immunities_list = []
    if stat_block.condition_immunities:
        condition_immunities_list = [c.strip().lower() for c in stat_block.condition_immunities.split(',')]

    # Build ParsedActorData
    return ParsedActorData(
        source_statblock_name=stat_block.name,
        name=stat_block.name,
        armor_class=stat_block.armor_class,
        hit_points=stat_block.hit_points,
        challenge_rating=stat_block.challenge_rating,
        abilities=stat_block.abilities or {},
        saving_throw_proficiencies=saving_throw_proficiencies,
        skill_proficiencies=skill_proficiencies,
        attacks=attacks,
        traits=traits,
        multiattack=multiattack,
        innate_spellcasting=innate_spellcasting,
        size=stat_block.size,
        creature_type=stat_block.type,
        alignment=stat_block.alignment,
        darkvision=senses.get("darkvision"),
        blindsight=senses.get("blindsight"),
        tremorsense=senses.get("tremorsense"),
        truesight=senses.get("truesight"),
        passive_perception=senses.get("passive_perception"),
        damage_resistances=damage_resistances,
        damage_immunities=damage_immunities,
        damage_vulnerabilities=damage_vulnerabilities,
        condition_immunities=condition_immunities_list
    )


# Convenience function for parsing multiple stat blocks
async def parse_multiple_stat_blocks(
    stat_blocks: list[StatBlock],
    spell_cache: Optional[SpellCache] = None,
    model_name: str = DEFAULT_MODEL
) -> list[ParsedActorData]:
    """
    Parse multiple stat blocks in parallel.

    Args:
        stat_blocks: List of StatBlock objects
        spell_cache: Optional spell cache
        model_name: Gemini model to use

    Returns:
        List of ParsedActorData objects
    """
    tasks = [
        parse_stat_block_parallel(sb, spell_cache, model_name)
        for sb in stat_blocks
    ]

    return await asyncio.gather(*tasks)
</file>

<file path="src/foundry/upload_journal_to_foundry.py">
#!/usr/bin/env python3
"""Upload generated XML documents to FoundryVTT as journal entries."""

import os
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from foundry.client import FoundryClient
from models import XMLDocument, Journal, parse_xml_file
from logging_config import setup_logging

logger = setup_logging(__name__)


def find_latest_run(runs_dir: str) -> str:
    """
    Find the most recent run directory.

    Args:
        runs_dir: Path to output/runs directory

    Returns:
        Path to latest run directory

    Raises:
        ValueError: If no run directories found
    """
    runs_path = Path(runs_dir)

    if not runs_path.exists():
        raise ValueError(f"Runs directory does not exist: {runs_dir}")

    run_dirs = [d for d in runs_path.iterdir() if d.is_dir()]

    if not run_dirs:
        raise ValueError(f"No run directories found in: {runs_dir}")

    # Sort by directory name (timestamp format YYYYMMDD_HHMMSS)
    latest = sorted(run_dirs, key=lambda d: d.name)[-1]

    logger.info(f"Latest run: {latest.name}")
    return str(latest)


def find_xml_directory(run_dir: str) -> str:
    """
    Find the XML documents directory in a run.

    Args:
        run_dir: Path to run directory

    Returns:
        Path to XML documents directory

    Raises:
        ValueError: If XML directory not found
    """
    run_path = Path(run_dir)

    # Try documents/ directory (standard location)
    xml_dir = run_path / "documents"
    if xml_dir.exists() and list(xml_dir.glob("*.xml")):
        return str(xml_dir)

    # Try root of run directory
    if list(run_path.glob("*.xml")):
        return str(run_path)

    raise ValueError(f"No XML files found in run directory: {run_dir}")


def build_image_mapping(run_dir: Path) -> Dict[str, str]:
    """
    Build image mapping from map_assets and scene_artwork directories.

    Scans for images in:
    - run_dir/map_assets/images/
    - run_dir/scene_artwork/images/

    Args:
        run_dir: Path to run directory

    Returns:
        Dictionary mapping image keys (filename without extension) to file paths
    """
    image_mapping = {}

    # Check map_assets directory
    map_assets_dir = run_dir / "map_assets" / "images"
    if map_assets_dir.exists():
        for image_file in map_assets_dir.iterdir():
            if image_file.suffix.lower() in ['.png', '.jpg', '.jpeg']:
                # Use filename without extension as key
                key = image_file.stem
                image_mapping[key] = str(image_file)
                logger.debug(f"  Added map asset: {key} -> {image_file.name}")

    # Check scene_artwork directory
    scene_artwork_dir = run_dir / "scene_artwork" / "images"
    if scene_artwork_dir.exists():
        for image_file in scene_artwork_dir.iterdir():
            if image_file.suffix.lower() in ['.png', '.jpg', '.jpeg']:
                # Use filename without extension as key
                key = image_file.stem
                image_mapping[key] = str(image_file)
                logger.debug(f"  Added scene artwork: {key} -> {image_file.name}")

    logger.info(f"Built image mapping with {len(image_mapping)} images")
    return image_mapping


def load_and_position_images(run_dir: Path, map_positioning_mode: str = "page") -> Journal:
    """Load Journal from XML and automatically position all extracted images.

    Processes:
    1. Load XMLDocument from documents/ directory
    2. Convert to Journal
    3. Add map assets with automatic positioning
    4. Add scene artwork with automatic positioning

    Args:
        run_dir: Run directory containing documents/, map_assets/, scene_artwork/
        map_positioning_mode: "page" (use page number + section logic) or "semantic" (use Gemini to match map name to sections)

    Returns:
        Journal with all images positioned
    """
    import json

    # Load all XML documents
    xml_dir = run_dir / "documents"
    xml_files = sorted(xml_dir.glob("*.xml"))

    if not xml_files:
        raise ValueError(f"No XML files found in {xml_dir}")

    # For now, merge all chapters into one Journal
    # TODO: Support multi-chapter journals properly
    journals = []
    for xml_file in xml_files:
        xml_doc = parse_xml_file(xml_file)
        journal = Journal.from_xml_document(xml_doc)
        journals.append(journal)

    # Use first journal (single-chapter workflow)
    journal = journals[0]
    logger.info(f"Loaded journal: {journal.title}")

    # Add map assets if present
    maps_metadata_file = run_dir / "map_assets" / "maps_metadata.json"
    if maps_metadata_file.exists():
        with open(maps_metadata_file) as f:
            maps_data = json.load(f)
            maps = maps_data.get("maps", [])

        if maps:
            maps_dir = run_dir / "map_assets"
            journal.add_map_assets(maps, maps_dir, positioning_mode=map_positioning_mode)
            logger.info(f"Added {len(maps)} map assets to journal (mode: {map_positioning_mode})")

    # Add scene artwork if present
    scenes_metadata_file = run_dir / "scene_artwork" / "scenes_metadata.json"
    if scenes_metadata_file.exists():
        with open(scenes_metadata_file) as f:
            scenes_data = json.load(f)
            scenes = scenes_data.get("scenes", [])

        if scenes:
            scenes_dir = run_dir / "scene_artwork" / "images"
            journal.add_scene_artwork(scenes, scenes_dir)
            logger.info(f"Added {len(scenes)} scene artworks to journal")
    elif (run_dir / "scene_artwork" / "images").exists():
        # Fallback to filename parsing if no metadata
        logger.warning("No scenes_metadata.json found, using filename heuristic")
        scenes_dir = run_dir / "scene_artwork" / "images"
        scenes = []
        for i, img_file in enumerate(sorted(scenes_dir.glob("scene_*.png")), start=1):
            # Extract name from filename: scene_001_forest_ambush.png -> Forest Ambush
            name_part = img_file.stem.split("_", 2)[-1] if len(img_file.stem.split("_")) > 2 else img_file.stem
            name = name_part.replace("_", " ").title()

            scenes.append({
                "section_path": f"{journal.title} ‚Üí Scene {i}",
                "name": name,
                "description": ""
            })

        if scenes:
            journal.add_scene_artwork(scenes, scenes_dir)
            logger.info(f"Added {len(scenes)} scene artworks to journal (using filename heuristic)")

    return journal


def upload_scene_gallery(client: FoundryClient, run_dir: Path) -> Optional[Dict[str, Any]]:
    """
    Upload scene artwork and create gallery journal page.

    Args:
        client: FoundryClient instance
        run_dir: Run directory (Path object)

    Returns:
        Gallery page dict or None if no scene gallery found
    """
    scene_artwork_dir = run_dir / "scene_artwork"
    images_dir = scene_artwork_dir / "images"
    gallery_file = scene_artwork_dir / "scene_gallery.html"

    if not gallery_file.exists():
        logger.info("No scene gallery found, skipping")
        return None

    logger.info("Uploading scene artwork...")

    # Upload images to FoundryVTT
    image_path_mapping = {}
    if images_dir.exists():
        image_files = list(images_dir.glob("*.png")) + list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.jpeg"))

        for image_file in image_files:
            # Upload to worlds/<client-id>/images/
            target_path = f"worlds/{client.client_id}/images/{image_file.name}"

            try:
                client.upload_file(str(image_file), target_path)
                # Map local path format to FoundryVTT path
                image_path_mapping[f"images/{image_file.name}"] = target_path
                logger.debug(f"  Uploaded {image_file.name}")
            except Exception as e:
                logger.error(f"Failed to upload {image_file.name}: {e}")

    # Update gallery HTML with FoundryVTT paths
    gallery_html = gallery_file.read_text()
    for old_path, new_path in image_path_mapping.items():
        gallery_html = gallery_html.replace(old_path, new_path)

    # Create gallery page dict
    gallery_page = {
        "name": "Scene Gallery",
        "type": "text",
        "text": {
            "content": gallery_html,
            "format": 1
        }
    }

    logger.info(f"‚úì Scene gallery page created ({len(image_path_mapping)} images)")
    return gallery_page


def upload_run_to_foundry(
    run_dir: str,
    target: str = "local",
    journal_name: str = None
) -> Dict[str, Any]:
    """
    Upload XML documents from a run to FoundryVTT as a single journal with multiple pages.

    This is a pipeline function that:
    1. Finds XML files in the run directory
    2. Converts them to Journal using XMLDocument and Journal models
    3. Builds image mapping from map_assets and scene_artwork
    4. Renders Journal to HTML using to_foundry_html()
    5. Uploads to FoundryVTT using client module
    6. Optionally uploads scene gallery if present

    Args:
        run_dir: Path to run directory (contains documents/ with XML files)
        target: Target environment ('local' or 'forge')
        journal_name: Name for the journal entry (default: "D&D Module")

    Returns:
        Dict with upload statistics
    """
    logger.info(f"Uploading to FoundryVTT ({target})")

    # Find XML directory
    try:
        xml_dir = find_xml_directory(run_dir)
        logger.info(f"Found XML directory: {xml_dir}")
    except ValueError as e:
        logger.error(str(e))
        return {"uploaded": 0, "failed": 0, "errors": [str(e)]}

    # Get all XML files
    xml_files = list(Path(xml_dir).glob("*.xml"))
    if not xml_files:
        logger.warning("No XML files found")
        return {"uploaded": 0, "failed": 0}

    logger.info(f"Found {len(xml_files)} XML file(s)")

    # Build image mapping from map_assets and scene_artwork
    run_path = Path(run_dir)
    image_mapping = build_image_mapping(run_path)

    # Convert XML files to Journal pages using XMLDocument and Journal models
    pages = []
    for xml_file in xml_files:
        try:
            # Parse XML file to XMLDocument
            xml_doc = parse_xml_file(xml_file)
            logger.debug(f"  Parsed {xml_file.name} -> {xml_doc.title}")

            # Convert XMLDocument to Journal with positioned images
            journal = Journal.from_xml_document(xml_doc)

            # Add positioned images if this is the first/only chapter
            # (For multi-chapter support, this logic will need refinement)
            if len(xml_files) == 1 or xml_file == xml_files[0]:
                # Add map assets if present
                maps_metadata_file = run_path / "map_assets" / "maps_metadata.json"
                if maps_metadata_file.exists():
                    import json
                    with open(maps_metadata_file) as f:
                        maps_data = json.load(f)
                        maps = maps_data.get("maps", [])

                    if maps:
                        maps_dir = run_path / "map_assets" / "images"
                        journal.add_map_assets(maps, maps_dir)
                        logger.info(f"Added {len(maps)} map assets to journal")

                # Add scene artwork if present
                scenes_metadata_file = run_path / "scene_artwork" / "scenes_metadata.json"
                if scenes_metadata_file.exists():
                    import json
                    with open(scenes_metadata_file) as f:
                        scenes_data = json.load(f)
                        scenes = scenes_data.get("scenes", [])

                    if scenes:
                        scenes_dir = run_path / "scene_artwork" / "images"
                        journal.add_scene_artwork(scenes, scenes_dir)
                        logger.info(f"Added {len(scenes)} scene artworks to journal")

            # Render Journal to HTML using to_foundry_html()
            html = journal.to_foundry_html(image_mapping)

            # Create page dict for FoundryVTT
            pages.append({
                "name": xml_doc.title,
                "content": html
            })

            logger.debug(f"  Converted {xml_file.name} to HTML ({len(html)} chars)")

        except Exception as e:
            error_msg = f"Failed to process {xml_file.name}: {e}"
            logger.error(error_msg)
            return {"uploaded": 0, "failed": 0, "errors": [error_msg]}

    if not pages:
        logger.warning("No journal pages to upload")
        return {"uploaded": 0, "failed": 0}

    logger.info(f"Converted {len(pages)} XML file(s) to journal pages")

    # Determine journal name
    if not journal_name:
        journal_name = "D&D Module"
        logger.info(f"Using default journal name: {journal_name}")

    # Initialize client
    client = FoundryClient(target=target)

    # Add scene gallery page if present
    gallery_page = upload_scene_gallery(client, run_path)
    if gallery_page:
        pages.append(gallery_page)

    logger.info(f"Uploading journal '{journal_name}' with {len(pages)} page(s)")

    # Upload as single journal with multiple pages
    try:
        result = client.create_or_replace_journal(
            name=journal_name,
            pages=pages
        )

        # Extract UUID from response
        journal_uuid = result.get('uuid')
        if not journal_uuid:
            # Construct from entity ID if uuid not directly available
            entity = result.get('entity', {})
            if isinstance(entity, list):
                entity_id = entity[0].get('_id') if entity else None
            else:
                entity_id = entity.get('_id')

            if entity_id:
                journal_uuid = f"JournalEntry.{entity_id}"
            else:
                journal_uuid = 'unknown'

        logger.info(f"‚úì Uploaded journal: {journal_name} with {len(pages)} page(s) (UUID: {journal_uuid})")

        return {
            "uploaded": len(pages),
            "failed": 0,
            "errors": [],
            "journal_uuid": journal_uuid,
            "journal_name": journal_name
        }

    except Exception as e:
        error_msg = f"‚úó Failed to upload journal: {journal_name} - {e}"
        logger.error(error_msg)
        return {
            "uploaded": 0,
            "failed": len(pages),
            "errors": [error_msg],
            "journal_uuid": None,
            "journal_name": journal_name
        }


def upload_file_to_foundry(
    local_path: str,
    target_path: str,
    target: str = "local",
    overwrite: bool = True
) -> Dict[str, Any]:
    """
    Upload a file to FoundryVTT.

    Convenience wrapper around FoundryClient.upload_file() for use in pipelines.

    Args:
        local_path: Path to local file
        target_path: Target path in FoundryVTT (e.g., "worlds/my-world/assets/image.png")
        target: Target environment ('local' or 'forge')
        overwrite: Whether to overwrite existing files (default: True)

    Returns:
        Upload response dict

    Raises:
        RuntimeError: If upload fails
    """
    logger.info(f"Uploading file to FoundryVTT ({target})")
    logger.debug(f"  Local:  {local_path}")
    logger.debug(f"  Target: {target_path}")

    client = FoundryClient(target=target)

    try:
        result = client.upload_file(local_path, target_path, overwrite=overwrite)
        logger.info(f"‚úì File uploaded successfully")
        return result
    except Exception as e:
        logger.error(f"File upload failed: {e}")
        raise


def main():
    """Main entry point for upload script."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Upload XML documents to FoundryVTT as a single journal with multiple pages"
    )
    parser.add_argument(
        "--run-dir",
        help="Specific run directory (default: latest)"
    )
    parser.add_argument(
        "--target",
        choices=["local", "forge"],
        default="local",
        help="Target environment (default: local)"
    )
    parser.add_argument(
        "--journal-name",
        help="Name for the journal entry (default: 'D&D Module')"
    )

    args = parser.parse_args()

    # Load environment variables
    load_dotenv()

    # Determine run directory
    if args.run_dir:
        run_dir = args.run_dir
    else:
        project_root = Path(__file__).parent.parent.parent
        runs_dir = project_root / "output" / "runs"
        run_dir = find_latest_run(str(runs_dir))

    # Upload (pipeline: find XML -> convert to journal HTML -> upload)
    try:
        result = upload_run_to_foundry(
            run_dir=run_dir,
            target=args.target,
            journal_name=args.journal_name
        )

        if result["failed"] > 0 or result.get("errors"):
            logger.error("Upload completed with errors")
            sys.exit(1)
        else:
            logger.info("Upload complete!")
            sys.exit(0)

    except Exception as e:
        logger.error(f"Upload failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="tests/foundry/test_client.py">
"""Tests for FoundryVTT API client."""

import pytest
import os
from unittest.mock import Mock, patch
from dotenv import load_dotenv
from src.foundry.client import FoundryClient


class TestFoundryClientInit:
    """Tests for FoundryClient initialization."""

    def test_client_initialization_with_env_vars(self, monkeypatch):
        """Test client initializes with environment variables."""
        monkeypatch.setenv("FOUNDRY_LOCAL_URL", "http://localhost:30000")
        monkeypatch.setenv("FOUNDRY_LOCAL_API_KEY", "test-api-key")
        monkeypatch.setenv("FOUNDRY_LOCAL_CLIENT_ID", "test-client-id")
        monkeypatch.setenv("FOUNDRY_RELAY_URL", "https://relay.example.com")

        client = FoundryClient(target="local")

        assert client.foundry_url == "http://localhost:30000"
        assert client.api_key == "test-api-key"
        assert client.client_id == "test-client-id"
        assert client.relay_url == "https://relay.example.com"

    def test_client_initialization_forge(self, monkeypatch):
        """Test client initializes with forge environment."""
        monkeypatch.setenv("FOUNDRY_FORGE_URL", "https://game.forge-vtt.com")
        monkeypatch.setenv("FOUNDRY_FORGE_API_KEY", "forge-api-key")
        monkeypatch.setenv("FOUNDRY_FORGE_CLIENT_ID", "forge-client-id")
        monkeypatch.setenv("FOUNDRY_RELAY_URL", "https://relay.example.com")

        client = FoundryClient(target="forge")

        assert client.foundry_url == "https://game.forge-vtt.com"
        assert client.api_key == "forge-api-key"
        assert client.client_id == "forge-client-id"

    def test_client_raises_on_missing_env_vars(self, monkeypatch):
        """Test client raises ValueError when required env vars missing."""
        # Clear all relevant env vars
        monkeypatch.delenv("FOUNDRY_LOCAL_URL", raising=False)
        monkeypatch.delenv("FOUNDRY_LOCAL_API_KEY", raising=False)
        monkeypatch.delenv("FOUNDRY_RELAY_URL", raising=False)

        with pytest.raises(ValueError, match="FOUNDRY_RELAY_URL not set"):
            FoundryClient(target="local")


class TestJournalOperations:
    """Tests for journal entry operations delegation."""

    @pytest.fixture
    def mock_client(self, monkeypatch):
        """Create a FoundryClient with mocked environment."""
        monkeypatch.setenv("FOUNDRY_LOCAL_URL", "http://localhost:30000")
        monkeypatch.setenv("FOUNDRY_LOCAL_API_KEY", "test-key")
        monkeypatch.setenv("FOUNDRY_LOCAL_CLIENT_ID", "test-client-id")
        monkeypatch.setenv("FOUNDRY_RELAY_URL", "https://relay.example.com")
        return FoundryClient(target="local")

    def test_client_has_journals_manager(self, mock_client):
        """Test that FoundryClient has journals manager."""
        from src.foundry.journals import JournalManager
        assert hasattr(mock_client, 'journals')
        assert isinstance(mock_client.journals, JournalManager)

    @patch('requests.post')
    def test_create_journal_delegates_to_manager(self, mock_post, mock_client):
        """Test that create_journal_entry delegates to JournalManager."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "journal123"},
            "uuid": "JournalEntry.journal123"
        }
        mock_post.return_value = mock_response

        result = mock_client.create_journal_entry(
            name="Test Journal",
            content="<p>Test content</p>"
        )

        assert result["entity"]["_id"] == "journal123"
        mock_post.assert_called_once()

    @patch('requests.get')
    def test_get_journal_by_name_delegates_to_manager(self, mock_get, mock_client):
        """Test that get_journal_by_name delegates to JournalManager."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = [
            {"_id": "journal123", "name": "Test Journal"}
        ]
        mock_get.return_value = mock_response

        result = mock_client.get_journal_by_name("Test Journal")

        assert result is not None
        assert result["_id"] == "journal123"


class TestFoundryIntegration:
    """Integration tests for FoundryVTT API (requires running server)."""

    @pytest.fixture
    def real_client(self):
        """Create a real FoundryClient with environment variables."""
        load_dotenv()

        # Check if required environment variables are set
        required_vars = [
            "FOUNDRY_RELAY_URL",
            "FOUNDRY_LOCAL_URL",
            "FOUNDRY_LOCAL_API_KEY",
            "FOUNDRY_LOCAL_CLIENT_ID"
        ]

        missing_vars = [var for var in required_vars if not os.getenv(var)]
        if missing_vars:
            pytest.skip(f"Missing required environment variables: {', '.join(missing_vars)}")

        return FoundryClient(target="local")

    @pytest.mark.smoke
    @pytest.mark.integration
    @pytest.mark.slow
    def test_create_and_delete(self, real_client):
        """Smoke test: Basic FoundryVTT journal CRUD operations

        Test basic create and delete workflow (minimal API calls)."""
        journal_name = "Integration Test Create Delete"

        # 1. CREATE (1 API call)
        create_result = real_client.create_journal_entry(
            name=journal_name,
            content="<h1>Test Content</h1><p>This journal will be deleted immediately.</p>"
        )

        # Extract UUID from create response
        journal_uuid = create_result.get('uuid')
        if not journal_uuid:
            entity_id = create_result.get('entity', {}).get('_id')
            journal_uuid = f"JournalEntry.{entity_id}"

        assert journal_uuid.startswith('JournalEntry.'), f"Invalid UUID format: {journal_uuid}"

        # 2. DELETE (1 API call)
        delete_result = real_client.delete_journal_entry(journal_uuid=journal_uuid)
        assert delete_result.get('success') is True, "Delete operation did not return success"

    @pytest.mark.integration
    @pytest.mark.slow
    def test_full_crud_workflow(self, real_client):
        """Test complete create ‚Üí search ‚Üí update ‚Üí delete workflow with real server."""
        journal_name = "Integration Test CRUD Workflow"

        # 1. CREATE
        create_result = real_client.create_journal_entry(
            name=journal_name,
            content="<h1>Initial Content</h1><p>This is a test.</p>"
        )
        entity_id = create_result.get('entity', {}).get('_id') or create_result.get('uuid', 'unknown')
        assert entity_id != 'unknown', "Failed to extract entity ID from create response"

        # 2. SEARCH
        found = real_client.get_journal_by_name(journal_name)
        assert found is not None, "Failed to find newly created journal"
        assert found['name'] == journal_name

        # Extract UUID for update/delete
        journal_uuid = found.get('uuid') or f"JournalEntry.{found.get('_id') or found.get('id')}"
        assert journal_uuid.startswith('JournalEntry.'), f"Invalid UUID format: {journal_uuid}"

        # 3. UPDATE
        update_result = real_client.update_journal_entry(
            journal_uuid=journal_uuid,
            content="<h1>Updated Content</h1><p>Content has been updated!</p>",
            name=f"{journal_name} (Updated)"
        )
        assert update_result is not None, "Update operation failed"

        # 4. VERIFY UPDATE
        found_updated = real_client.get_journal_by_name(f"{journal_name} (Updated)")
        if not found_updated:
            # Name update might be delayed, try old name
            found_updated = real_client.get_journal_by_name(journal_name)
        assert found_updated is not None, "Failed to find journal after update"

        # 5. DELETE
        delete_result = real_client.delete_journal_entry(journal_uuid=journal_uuid)
        assert delete_result.get('success') is True, "Delete operation did not return success"

        # 6. VERIFY DELETION
        found_deleted = real_client.get_journal_by_name(journal_name)
        assert found_deleted is None, "Journal still exists after deletion"

    @pytest.mark.integration
    @pytest.mark.slow
    def test_create_or_replace_workflow(self, real_client):
        """Test create_or_replace creates on first call, deletes and creates on second call."""
        journal_name = "Integration Test Create or Replace"

        try:
            # First call should CREATE
            result1 = real_client.create_or_replace_journal(
                name=journal_name,
                content="<h1>First Version</h1><p>Initial content</p>"
            )

            # Extract ID from create response
            entity1 = result1.get('entity', {})
            if isinstance(entity1, list):
                id1 = entity1[0].get('_id') if entity1 else None
            else:
                id1 = entity1.get('_id')

            assert id1 is not None, "Failed to extract ID from first call"

            # Second call should DELETE old journal and CREATE new one (different ID)
            result2 = real_client.create_or_replace_journal(
                name=journal_name,
                content="<h1>Second Version</h1><p>Updated content!</p>"
            )

            # Extract ID from create response
            entity2 = result2.get('entity', {})
            if isinstance(entity2, list):
                id2 = entity2[0].get('_id') if entity2 else None
            else:
                id2 = entity2.get('_id')

            # Should be different journal (replaced, not updated)
            assert id2 != id1, f"Second call should create new journal with different ID: {id1} vs {id2}"

            # Verify only one journal exists with this name
            found = real_client.get_journal_by_name(journal_name)
            assert found is not None, "Journal not found after create_or_replace"

        finally:
            # Clean up - delete the test journal
            found = real_client.get_journal_by_name(journal_name)
            if found:
                journal_uuid = found.get('uuid') or f"JournalEntry.{found.get('_id') or found.get('id')}"
                real_client.delete_journal_entry(journal_uuid=journal_uuid)

    @pytest.mark.integration
    @pytest.mark.slow
    def test_delete_nonexistent_journal(self, real_client):
        """Test deleting a non-existent journal succeeds (idempotent delete)."""
        fake_uuid = "JournalEntry.nonexistentfake123"

        # Delete endpoint is idempotent - returns success even if journal doesn't exist
        result = real_client.delete_journal_entry(journal_uuid=fake_uuid)
        assert result.get('success') is True

    @pytest.mark.integration
    @pytest.mark.slow
    def test_update_nonexistent_journal(self, real_client):
        """Test updating a non-existent journal raises appropriate error."""
        fake_uuid = "JournalEntry.nonexistentfake456"

        with pytest.raises(RuntimeError, match="Failed to update journal"):
            real_client.update_journal_entry(
                journal_uuid=fake_uuid,
                content="<h1>Test</h1>"
            )

    @pytest.mark.integration
    @pytest.mark.slow
    @pytest.mark.flaky(reruns=2, reruns_delay=1)
    @pytest.mark.order(1)  # Run first - if infrastructure is down, fail fast
    def test_upload_and_download_file(self, real_client, tmp_path):
        """Test uploading a file to FoundryVTT and downloading it back.

        Note: Retries up to 2 times to handle intermittent network issues.
        If this test fails after retries, the relay server at localhost:3010 needs attention.
        Check: docker-compose ps or start with: cd relay-server && docker-compose up -d

        File will remain on server as there's no delete endpoint in the API.
        Files are uploaded to worlds/{world}/test_uploads/ for manual cleanup.
        """
        import os
        import time
        from dotenv import load_dotenv
        load_dotenv()

        # Check if a world is active (works for both browser and headless sessions)
        if not real_client.is_world_active():
            pytest.skip("No active FoundryVTT world - file uploads require a running world. "
                       "Launch a world in FoundryVTT to run this test.")

        # Create a test file with timestamp
        timestamp = time.time()
        test_content = f"Integration test file content\nTimestamp: {timestamp}\nTest run marker"

        upload_file = tmp_path / "test_upload.txt"
        upload_file.write_text(test_content)

        # Get world name from env
        world_name = os.getenv("FOUNDRY_WORLD_NAME", "testing-world")

        # Upload file to test directory
        target_path = f"worlds/{world_name}/test_uploads/integration_test.txt"

        # Upload the file
        upload_result = real_client.upload_file(
            local_path=str(upload_file),
            target_path=target_path,
            overwrite=True
        )

        # Verify upload succeeded
        assert upload_result is not None, "Upload did not return a result"

        # Download the file back
        download_file = tmp_path / "test_download.txt"
        real_client.download_file(
            target_path=target_path,
            local_path=str(download_file)
        )

        # Verify downloaded file exists and content matches
        assert download_file.exists(), "Downloaded file does not exist"
        downloaded_content = download_file.read_text()
        assert downloaded_content == test_content, "Downloaded content does not match uploaded content"
        assert str(timestamp) in downloaded_content, "Timestamp not found in downloaded content"

        # Note: No delete endpoint available in FoundryVTT relay API
        # File at worlds/{world}/test_uploads/integration_test.txt should be manually cleaned up
        # or will be overwritten on next test run


@pytest.mark.unit
class TestActorOperations:
    """Tests for actor operations via FoundryClient."""

    @pytest.fixture
    def mock_client(self, monkeypatch):
        """Create a FoundryClient with mocked environment."""
        monkeypatch.setenv("FOUNDRY_LOCAL_URL", "http://localhost:30000")
        monkeypatch.setenv("FOUNDRY_LOCAL_API_KEY", "test-key")
        monkeypatch.setenv("FOUNDRY_LOCAL_CLIENT_ID", "test-client-id")
        monkeypatch.setenv("FOUNDRY_RELAY_URL", "https://relay.example.com")
        return FoundryClient(target="local")

    def test_client_initializes_actor_manager(self, mock_client):
        """Test client creates ActorManager instance."""
        assert hasattr(mock_client, 'actors')
        assert mock_client.actors is not None

    @patch('requests.get')
    def test_client_search_actor_delegates(self, mock_get, mock_client):
        """Test client.search_actor delegates to ActorManager."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = [
            {"uuid": "Actor.abc123", "name": "Goblin"}
        ]
        mock_get.return_value = mock_response

        uuid = mock_client.search_actor("Goblin")

        assert uuid == "Actor.abc123"
        mock_get.assert_called_once()

    @patch('requests.post')
    def test_client_create_creature_actor_delegates(self, mock_post, mock_client):
        """Test client.create_creature_actor delegates to ActorManager."""
        from src.actors.models import StatBlock

        stat_block = StatBlock(
            name="Goblin",
            raw_text="Goblin text",
            armor_class=15,
            hit_points=7,
            challenge_rating=0.25
        )

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "abc123"},
            "uuid": "Actor.abc123"
        }
        mock_post.return_value = mock_response

        uuid = mock_client.create_creature_actor(stat_block)

        assert uuid == "Actor.abc123"
        mock_post.assert_called_once()

    @patch('requests.post')
    def test_client_create_npc_actor_delegates(self, mock_post, mock_client):
        """Test client.create_npc_actor delegates to ActorManager."""
        from src.actors.models import NPC

        npc = NPC(
            name="Klarg",
            creature_stat_block_name="Goblin Boss",
            description="Leader",
            plot_relevance="Guards supplies"
        )

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "entity": {"_id": "xyz789"},
            "uuid": "Actor.xyz789"
        }
        mock_post.return_value = mock_response

        uuid = mock_client.create_npc_actor(npc, stat_block_uuid="Actor.boss123")

        assert uuid == "Actor.xyz789"
        mock_post.assert_called_once()

    def test_foundry_client_has_icon_cache(self, mock_client):
        """Test FoundryClient exposes icon cache."""
        from src.foundry.icon_cache import IconCache

        assert hasattr(mock_client, 'icons')
        assert isinstance(mock_client.icons, IconCache)
</file>

<file path="tests/models/test_journal_image_positioning.py">
"""Tests for automatic image positioning in Journal model."""

import pytest
from pathlib import Path
from models.xml_document import parse_xml_file
from models.journal import Journal, ImageMetadata


def test_add_map_assets_positions_images_near_source_page():
    """Test that map assets are positioned near their source page in the Journal."""
    # Load a real XML document
    xml_path = Path("tests/fixtures/sample_chapter.xml")
    xml_doc = parse_xml_file(xml_path)
    journal = Journal.from_xml_document(xml_doc)

    # Simulate map metadata from extract_map_assets.py
    map_metadata = [
        {
            "name": "Goblin Ambush",
            "page_num": 5,
            "type": "battle_map",
            "source": "extracted"
        },
        {
            "name": "Cragmaw Hideout",
            "page_num": 8,
            "type": "navigation_map",
            "source": "segmented"
        }
    ]

    # Add map assets to journal
    journal.add_map_assets(map_metadata, image_dir=Path("output/runs/test/map_assets/images"))

    # Verify maps were added to registry
    assert "page_005_goblin_ambush" in journal.image_registry
    assert "page_008_cragmaw_hideout" in journal.image_registry

    # Verify positioning: different pages should get different positions
    img_meta_5 = journal.image_registry["page_005_goblin_ambush"]
    img_meta_8 = journal.image_registry["page_008_cragmaw_hideout"]

    assert img_meta_5.insert_before_content_id is not None
    assert img_meta_8.insert_before_content_id is not None
    assert "chapter_0_section" in img_meta_5.insert_before_content_id
    assert "chapter_0_section" in img_meta_8.insert_before_content_id

    # Critical assertion: different source pages MUST get different positions
    assert img_meta_5.insert_before_content_id != img_meta_8.insert_before_content_id, \
        "Images from different pages should not have the same position"


def test_add_scene_artwork_positions_at_sections():
    """Test that scene artwork is positioned at section/subsection boundaries."""
    xml_path = Path("tests/fixtures/sample_chapter.xml")
    xml_doc = parse_xml_file(xml_path)
    journal = Journal.from_xml_document(xml_doc)

    # Simulate scene metadata from generate_scene_art.py
    scenes = [
        {
            "section_path": "Chapter 1: Goblin Arrows ‚Üí Goblin Ambush",
            "name": "Forest Road Ambush",
            "description": "A dense forest path with overturned wagon"
        },
        {
            "section_path": "Chapter 1: Goblin Arrows ‚Üí The Cragmaw Hideout ‚Üí Area 1: Cave Entrance",
            "name": "Cave Entrance",
            "description": "Rocky cave entrance with twin pools"
        }
    ]

    # Add scene artwork
    journal.add_scene_artwork(scenes, image_dir=Path("output/runs/test/scene_artwork/images"))

    # Verify scenes were added to registry
    assert "scene_forest_road_ambush" in journal.image_registry
    assert "scene_cave_entrance" in journal.image_registry

    # Verify positioning: should be at subsection boundaries
    img_meta = journal.image_registry["scene_forest_road_ambush"]
    assert img_meta.insert_before_content_id is not None
</file>

<file path="src/models/__init__.py">
"""Models for representing XML documents and their conversion to FoundryVTT journals."""

from models.xml_document import (
    Content,
    DefinitionItem,
    DefinitionList,
    ImageRef,
    ListContent,
    ListItem,
    Page,
    StatBlockRaw,
    Table,
    TableRow,
    XMLDocument,
    parse_xml_file,
    parse_xml_string,
)

from models.journal import (
    Chapter,
    ImageMetadata,
    Journal,
    Section,
    Subsection,
    Subsubsection,
)

__all__ = [
    # XMLDocument models
    "Content",
    "DefinitionItem",
    "DefinitionList",
    "ImageRef",
    "ListContent",
    "ListItem",
    "Page",
    "StatBlockRaw",
    "Table",
    "TableRow",
    "XMLDocument",
    "parse_xml_file",
    "parse_xml_string",
    # Journal models
    "Chapter",
    "ImageMetadata",
    "Journal",
    "Section",
    "Subsection",
    "Subsubsection",
]
</file>

<file path="tests/models/test_journal.py">
"""Tests for Journal models."""

import pytest
from models.xml_document import XMLDocument
from models.journal import Journal, Chapter, Section, Subsection, Subsubsection, ImageMetadata


class TestJournal:
    """Test Journal class."""

    def test_journal_flattens_pages_to_chapters(self):
        """Test that Journal.from_xml_document() flattens page structure to semantic hierarchy."""
        # Create an XMLDocument with page-based structure
        xml_string = """
<Chapter_1>
  <page number="1">
    <chapter_title>The Adventure Begins</chapter_title>
    <section>Introduction</section>
    <paragraph>This is the introduction paragraph.</paragraph>
    <subsection>Background</subsection>
    <paragraph>This is background information.</paragraph>
  </page>
  <page number="2">
    <paragraph>This continues the background section.</paragraph>
    <section>The Quest</section>
    <paragraph>A new section about the quest.</paragraph>
    <subsection>Quest Details</subsection>
    <paragraph>Details about the quest.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)

        # Convert to Journal
        journal = Journal.from_xml_document(xml_doc)

        # Verify basic structure
        assert journal.title == "Chapter_1"
        assert journal.source == xml_doc
        assert len(journal.chapters) == 1

        # Verify chapter
        chapter = journal.chapters[0]
        assert chapter.title == "The Adventure Begins"
        assert len(chapter.sections) == 2

        # Verify first section (spans pages 1-2)
        section1 = chapter.sections[0]
        assert section1.title == "Introduction"
        assert len(section1.subsections) == 1

        # Verify subsection that spans pages
        subsection = section1.subsections[0]
        assert subsection.title == "Background"
        # Should have 2 paragraphs (one from page 1, one from page 2)
        assert len(subsection.content) == 2

        # Verify content IDs have been reassigned
        content_id = subsection.content[0].id
        assert content_id.startswith("chapter_0_section_0_")

        # Verify second section
        section2 = chapter.sections[1]
        assert section2.title == "The Quest"
        assert len(section2.subsections) == 1

        # Verify quest subsection
        quest_subsection = section2.subsections[0]
        assert quest_subsection.title == "Quest Details"


class TestImageMetadata:
    """Test ImageMetadata class."""

    def test_image_metadata_creation(self):
        """Test creating ImageMetadata."""
        img_meta = ImageMetadata(
            key="map_001",
            source_page=5,
            type="map"
        )
        assert img_meta.key == "map_001"
        assert img_meta.source_page == 5
        assert img_meta.type == "map"


class TestHierarchyModels:
    """Test Chapter, Section, Subsection, Subsubsection models."""

    def test_chapter_creation(self):
        """Test creating a Chapter."""
        chapter = Chapter(
            title="Test Chapter",
            sections=[]
        )
        assert chapter.title == "Test Chapter"
        assert chapter.sections == []

    def test_section_creation(self):
        """Test creating a Section."""
        section = Section(
            title="Test Section",
            subsections=[],
            content=[]
        )
        assert section.title == "Test Section"
        assert section.subsections == []
        assert section.content == []

    def test_subsection_creation(self):
        """Test creating a Subsection."""
        subsection = Subsection(
            title="Test Subsection",
            subsubsections=[],
            content=[]
        )
        assert subsection.title == "Test Subsection"
        assert subsection.subsubsections == []
        assert subsection.content == []

    def test_subsubsection_creation(self):
        """Test creating a Subsubsection."""
        subsubsection = Subsubsection(
            title="Test Subsubsection",
            content=[]
        )
        assert subsubsection.title == "Test Subsubsection"
        assert subsubsection.content == []


class TestImageRefExtraction:
    """Test ImageRef extraction to image registry."""

    def test_journal_extracts_image_refs_to_registry(self):
        """Test that Journal._extract_image_refs() populates image_registry from XMLDocument."""
        # Create an XMLDocument with ImageRef elements
        xml_string = """
<Chapter_1>
  <page number="5">
    <paragraph>This is a paragraph.</paragraph>
    <image_ref key="page_5_top_battle_map" />
    <paragraph>Another paragraph.</paragraph>
  </page>
  <page number="7">
    <paragraph>Some text.</paragraph>
    <image_ref key="page_7_illustration" />
  </page>
  <page number="10">
    <image_ref key="encounter_map_goblin_cave" />
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)

        # Convert to Journal
        journal = Journal.from_xml_document(xml_doc)

        # Verify image_registry has been populated
        assert len(journal.image_registry) == 3

        # Verify first image ref
        assert "page_5_top_battle_map" in journal.image_registry
        img1 = journal.image_registry["page_5_top_battle_map"]
        assert img1.key == "page_5_top_battle_map"
        assert img1.source_page == 5  # Parsed from key
        assert img1.type == "map"  # Inferred from "battle_map"

        # Verify second image ref
        assert "page_7_illustration" in journal.image_registry
        img2 = journal.image_registry["page_7_illustration"]
        assert img2.key == "page_7_illustration"
        assert img2.source_page == 7  # Parsed from key
        assert img2.type == "illustration"  # Inferred from key

        # Verify third image ref (no page number in key)
        assert "encounter_map_goblin_cave" in journal.image_registry
        img3 = journal.image_registry["encounter_map_goblin_cave"]
        assert img3.key == "encounter_map_goblin_cave"
        assert img3.source_page == 10  # Falls back to actual page number
        assert img3.type == "map"  # Inferred from "map" in key

    def test_image_refs_remain_in_content_stream(self):
        """Test that ImageRef elements stay in content (not removed during extraction)."""
        xml_string = """
<Chapter_1>
  <page number="5">
    <chapter_title>Test Chapter</chapter_title>
    <paragraph>Before image.</paragraph>
    <image_ref key="page_5_map" />
    <paragraph>After image.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Verify ImageRef is in the registry
        assert "page_5_map" in journal.image_registry

        # Verify ImageRef is still in the content
        chapter = journal.chapters[0]
        assert len(chapter.content) == 3
        assert chapter.content[1].type == "image_ref"
        assert chapter.content[1].data.key == "page_5_map"


class TestImageManipulation:
    """Test Journal image manipulation methods."""

    def test_journal_add_image(self):
        """Test that add_image() adds a new image to the registry."""
        # Create a simple journal
        xml_string = """
<Chapter_1>
  <page number="1">
    <chapter_title>Test Chapter</chapter_title>
    <paragraph>Some content.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Verify registry is empty initially
        assert len(journal.image_registry) == 0

        # Add a new image (e.g., scene artwork)
        scene_img = ImageMetadata(
            key="scene_artwork_tavern",
            source_page=1,
            type="illustration",
            description="A cozy tavern interior",
            file_path="/path/to/tavern.png"
        )
        journal.add_image("scene_artwork_tavern", scene_img)

        # Verify image was added
        assert len(journal.image_registry) == 1
        assert "scene_artwork_tavern" in journal.image_registry
        assert journal.image_registry["scene_artwork_tavern"].key == "scene_artwork_tavern"
        assert journal.image_registry["scene_artwork_tavern"].type == "illustration"
        assert journal.image_registry["scene_artwork_tavern"].description == "A cozy tavern interior"

    def test_journal_reposition_image(self):
        """Test that reposition_image() changes image placement."""
        # Create a journal with an existing image
        xml_string = """
<Chapter_1>
  <page number="5">
    <chapter_title>Test Chapter</chapter_title>
    <paragraph>Before image.</paragraph>
    <image_ref key="page_5_map" />
    <paragraph>After image.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Verify image exists
        assert "page_5_map" in journal.image_registry
        original_metadata = journal.image_registry["page_5_map"]

        # Initially, insert_before_content_id should be None
        assert original_metadata.insert_before_content_id is None

        # Reposition the image to appear before a specific content ID
        journal.reposition_image("page_5_map", "chapter_0_section_-1_content_5")

        # Verify the metadata was updated
        updated_metadata = journal.image_registry["page_5_map"]
        assert updated_metadata.insert_before_content_id == "chapter_0_section_-1_content_5"

    def test_journal_remove_image(self):
        """Test that remove_image() deletes image from registry."""
        # Create a journal with an existing image
        xml_string = """
<Chapter_1>
  <page number="5">
    <chapter_title>Test Chapter</chapter_title>
    <image_ref key="page_5_map" />
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Verify image exists
        assert "page_5_map" in journal.image_registry
        assert len(journal.image_registry) == 1

        # Remove the image
        journal.remove_image("page_5_map")

        # Verify image was removed
        assert "page_5_map" not in journal.image_registry
        assert len(journal.image_registry) == 0

    def test_remove_nonexistent_image_does_nothing(self):
        """Test that removing a non-existent image doesn't raise an error."""
        xml_string = """
<Chapter_1>
  <page number="1">
    <chapter_title>Test Chapter</chapter_title>
    <paragraph>Content.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Try to remove non-existent image
        journal.remove_image("non_existent_key")

        # Should not raise error
        assert len(journal.image_registry) == 0


class TestJournalExport:
    """Test Journal export methods (to_foundry_html, to_html, to_markdown)."""

    def test_journal_to_foundry_html(self):
        """Test that to_foundry_html() exports journal with proper semantic HTML hierarchy."""
        # Create a journal with nested structure
        xml_string = """
<Chapter_1>
  <page number="1">
    <chapter_title>The Adventure Begins</chapter_title>
    <paragraph>This is the introduction.</paragraph>
    <section>Introduction</section>
    <paragraph>First paragraph of intro.</paragraph>
    <subsection>Background</subsection>
    <paragraph>Background details here.</paragraph>
    <boxed_text>Important note in a box!</boxed_text>
    <subsubsection>History</subsubsection>
    <paragraph>Historical context.</paragraph>
  </page>
  <page number="2">
    <image_ref key="page_2_battle_map" />
    <paragraph>Text after image.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Add an image with file path and reposition it
        img_metadata = ImageMetadata(
            key="scene_artwork_tavern",
            source_page=1,
            type="illustration",
            file_path="/path/to/tavern.png",
            insert_before_content_id="chapter_0_section_0_content_0"
        )
        journal.add_image("scene_artwork_tavern", img_metadata)

        # Create image mapping (simulating actual image files)
        image_mapping = {
            "page_2_battle_map": "https://example.com/battle_map.png",
            "scene_artwork_tavern": "https://example.com/tavern.png"
        }

        # Export to HTML
        html = journal.to_foundry_html(image_mapping)

        # Verify semantic structure
        assert "<h1>The Adventure Begins</h1>" in html
        assert "<h2>Introduction</h2>" in html
        assert "<h3>Background</h3>" in html
        assert "<h4>History</h4>" in html

        # Verify content rendering
        assert "<p>This is the introduction.</p>" in html
        assert "<p>First paragraph of intro.</p>" in html
        assert "<p>Background details here.</p>" in html
        assert "<p>Historical context.</p>" in html

        # Verify boxed_text rendering (uses <aside> tag)
        assert "<aside" in html
        assert "Important note in a box!" in html

        # Verify image insertion (inserted before specific content)
        assert '<img src="https://example.com/tavern.png"' in html
        # Should appear before "First paragraph of intro"
        tavern_pos = html.index("tavern.png")
        intro_pos = html.index("First paragraph of intro")
        assert tavern_pos < intro_pos

        # Verify image_ref rendering (at original location)
        assert '<img src="https://example.com/battle_map.png"' in html
        # Should appear before "Text after image"
        map_pos = html.index("battle_map.png")
        text_pos = html.index("Text after image")
        assert map_pos < text_pos

    def test_to_html_calls_to_foundry_html(self):
        """Test that to_html() is a stub that calls to_foundry_html()."""
        xml_string = """
<Chapter_1>
  <page number="1">
    <chapter_title>Test Chapter</chapter_title>
    <paragraph>Some content.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Call both methods with same image_mapping
        image_mapping = {}
        html1 = journal.to_foundry_html(image_mapping)
        html2 = journal.to_html(image_mapping)

        # Should produce same output
        assert html1 == html2

    def test_to_markdown_returns_placeholder(self):
        """Test that to_markdown() is a stub that returns placeholder."""
        xml_string = """
<Chapter_1>
  <page number="1">
    <chapter_title>Test Chapter</chapter_title>
    <paragraph>Some content.</paragraph>
  </page>
</Chapter_1>
"""
        xml_doc = XMLDocument.from_xml(xml_string)
        journal = Journal.from_xml_document(xml_doc)

        # Call to_markdown
        markdown = journal.to_markdown()

        # Should return a placeholder message
        assert "Markdown export not yet implemented" in markdown or "TODO" in markdown
</file>

<file path="ui/backend/app/tools/actor_creator.py">
"""Actor creation tool using the public API."""
import sys
from pathlib import Path
from dotenv import load_dotenv
from .base import BaseTool, ToolSchema, ToolResponse

# Add project paths for api module imports
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))  # For "from src.xxx" imports
sys.path.insert(0, str(project_root / "src"))  # For "from xxx" imports

# Load environment variables from project root before imports
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)

from api import create_actor, APIError  # noqa: E402


class ActorCreatorTool(BaseTool):
    """Tool for creating D&D actors from descriptions."""

    @property
    def name(self) -> str:
        return "create_actor"

    def get_schema(self) -> ToolSchema:
        """Return tool schema for Gemini function calling."""
        return ToolSchema(
            name="create_actor",
            description=(
                "Create a D&D actor/creature in FoundryVTT from a natural "
                "language description. Use when user asks to create, make, "
                "or generate an actor, monster, NPC, or creature."
            ),
            parameters={
                "type": "object",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "Detailed description of the creature"
                    },
                    "challenge_rating": {
                        "type": "number",
                        "description": "Optional CR (0.125 to 30). Omit to infer from description.",
                        "minimum": 0.125,
                        "maximum": 30
                    }
                },
                "required": ["description"]
            }
        )

    async def execute(self, description: str, challenge_rating: float = None) -> ToolResponse:
        """Execute actor creation."""
        try:
            # Call synchronous API in thread pool (non-blocking)
            import asyncio
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                create_actor,
                description,
                challenge_rating
            )

            # Format text response
            cr_text = f"CR {result.challenge_rating}"
            message = (
                f"Created **{result.name}** ({cr_text})!\n\n"
                f"- **FoundryVTT UUID**: `{result.foundry_uuid}`\n"
                f"- **Output Directory**: `{result.output_dir}`"
            )

            return ToolResponse(
                type="text",
                message=message,
                data=None
            )

        except APIError as e:
            return ToolResponse(
                type="error",
                message=f"Failed to create actor: {str(e)}",
                data=None
            )
</file>

<file path="tests/api/test_api.py">
"""Tests for public API facade."""
import pytest
from pathlib import Path
from unittest.mock import Mock, patch
from api import (
    APIError,
    ActorCreationResult,
    MapExtractionResult,
    JournalCreationResult,
    create_actor
)


def test_api_error_can_be_raised():
    """Test that APIError can be raised and caught."""
    with pytest.raises(APIError, match="test error"):
        raise APIError("test error")


def test_api_error_preserves_cause():
    """Test that APIError preserves original exception."""
    original = ValueError("original error")

    try:
        try:
            raise original
        except ValueError as e:
            raise APIError("wrapped error") from e
    except APIError as api_err:
        assert api_err.__cause__ is original


def test_actor_creation_result_instantiation():
    """Test ActorCreationResult can be created."""
    result = ActorCreationResult(
        foundry_uuid="Actor.abc123",
        name="Goblin Warrior",
        challenge_rating=1.0,
        output_dir=Path("output/runs/test"),
        timestamp="2025-11-05T12:00:00"
    )

    assert result.foundry_uuid == "Actor.abc123"
    assert result.name == "Goblin Warrior"
    assert result.challenge_rating == 1.0


def test_map_extraction_result_instantiation():
    """Test MapExtractionResult can be created."""
    result = MapExtractionResult(
        maps=[{"name": "Test Map", "type": "battle_map"}],
        output_dir=Path("output/runs/test"),
        total_maps=1,
        timestamp="2025-11-05T12:00:00"
    )

    assert result.total_maps == 1
    assert len(result.maps) == 1


def test_journal_creation_result_instantiation():
    """Test JournalCreationResult can be created."""
    result = JournalCreationResult(
        journal_uuid="JournalEntry.xyz789",
        journal_name="Test Journal",
        output_dir=Path("output/runs/test"),
        chapter_count=5,
        timestamp="2025-11-05T12:00:00"
    )

    assert result.journal_uuid == "JournalEntry.xyz789"
    assert result.chapter_count == 5


@patch('api.orchestrate_create_actor_from_description_sync')
def test_create_actor_happy_path(mock_create):
    """Test create_actor wraps orchestrate correctly."""
    # Mock the orchestrate function
    from actors.models import ActorCreationResult as OrchestrateResult

    mock_result = OrchestrateResult(
        description="A fierce goblin",
        challenge_rating=1.0,
        raw_stat_block_text="RAW TEXT",
        stat_block=Mock(),
        parsed_actor_data=Mock(name="Goblin Warrior"),
        foundry_uuid="Actor.abc123",
        output_dir=Path("output/runs/test"),
        raw_text_file=Path("output/runs/test/01.txt"),
        stat_block_file=Path("output/runs/test/02.json"),
        parsed_data_file=Path("output/runs/test/03.json"),
        foundry_json_file=Path("output/runs/test/04.json"),
        timestamp="2025-11-05T12:00:00",
        model_used="gemini-2.0-flash"
    )
    mock_create.return_value = mock_result

    # Call our API function
    result = create_actor("A fierce goblin", challenge_rating=1.0)

    # Verify it called orchestrate with correct args
    mock_create.assert_called_once_with(
        description="A fierce goblin",
        challenge_rating=1.0
    )

    # Verify result is our simplified dataclass
    assert isinstance(result, ActorCreationResult)
    assert result.foundry_uuid == "Actor.abc123"
    assert result.challenge_rating == 1.0
    assert result.output_dir == Path("output/runs/test")


@patch('api.orchestrate_create_actor_from_description_sync')
def test_create_actor_error_handling(mock_create):
    """Test create_actor wraps exceptions as APIError."""
    mock_create.side_effect = ValueError("Gemini API error")

    with pytest.raises(APIError, match="Failed to create actor"):
        create_actor("broken description")

    # Verify original exception is preserved
    try:
        create_actor("broken description")
    except APIError as e:
        assert isinstance(e.__cause__, ValueError)
        assert str(e.__cause__) == "Gemini API error"


@patch('api.extract_maps_from_pdf')
def test_extract_maps_happy_path(mock_extract):
    """Test extract_maps wraps map extraction correctly."""
    from pdf_processing.image_asset_processing.models import MapMetadata

    # Mock extraction results
    mock_maps = [
        MapMetadata(
            name="Cave Entrance",
            page_num=1,
            type="battle_map",
            source="extracted",
            chapter="Chapter 1"
        ),
        MapMetadata(
            name="Goblin Hideout",
            page_num=2,
            type="battle_map",
            source="segmented",
            chapter="Chapter 1"
        )
    ]
    mock_extract.return_value = mock_maps

    # Import after patching
    from api import extract_maps

    # Call API function
    result = extract_maps("test.pdf", chapter="Chapter 1")

    # Verify extraction was called
    mock_extract.assert_called_once()

    # Verify result
    assert isinstance(result, MapExtractionResult)
    assert result.total_maps == 2
    assert len(result.maps) == 2
    assert result.maps[0]["name"] == "Cave Entrance"


@patch('api.extract_maps_from_pdf')
def test_extract_maps_error_handling(mock_extract):
    """Test extract_maps wraps exceptions."""
    mock_extract.side_effect = FileNotFoundError("PDF not found")

    # Import after patching
    from api import extract_maps

    with pytest.raises(APIError, match="Failed to extract maps"):
        extract_maps("missing.pdf")


@patch('api.run_pdf_to_xml')
@patch('api.upload_xml_to_foundry')
def test_process_pdf_to_journal_happy_path(mock_upload, mock_pdf_to_xml):
    """Test process_pdf_to_journal wraps pipeline correctly."""
    from api import process_pdf_to_journal

    # Mock PDF to XML with a mock Path that has glob method
    mock_run_dir = Mock(spec=Path)
    mock_run_dir.glob.return_value = [
        Path("output/runs/20251105_120000/documents/chapter1.xml"),
        Path("output/runs/20251105_120000/documents/chapter2.xml"),
        Path("output/runs/20251105_120000/documents/chapter3.xml")
    ]
    mock_pdf_to_xml.return_value = mock_run_dir

    # Mock upload
    mock_upload.return_value = "JournalEntry.xyz789"

    # Call API function
    result = process_pdf_to_journal(
        "test.pdf",
        "Test Journal",
        skip_upload=False
    )

    # Verify calls
    mock_pdf_to_xml.assert_called_once()
    mock_upload.assert_called_once_with(mock_run_dir, "Test Journal")

    # Verify result
    assert isinstance(result, JournalCreationResult)
    assert result.journal_uuid == "JournalEntry.xyz789"
    assert result.journal_name == "Test Journal"
    assert result.chapter_count == 3


@patch('api.run_pdf_to_xml')
def test_process_pdf_to_journal_skip_upload(mock_pdf_to_xml):
    """Test process_pdf_to_journal with skip_upload=True."""
    from api import process_pdf_to_journal

    # Mock run_dir with glob method
    mock_run_dir = Mock(spec=Path)
    mock_run_dir.glob.return_value = [
        Path("output/runs/20251105_120000/documents/chapter1.xml")
    ]
    mock_pdf_to_xml.return_value = mock_run_dir

    result = process_pdf_to_journal(
        "test.pdf",
        "Test Journal",
        skip_upload=True
    )

    # Should have empty UUID when upload is skipped
    assert result.journal_uuid == ""
    assert result.output_dir == mock_run_dir
    assert result.chapter_count == 1


@patch('api.run_pdf_to_xml')
def test_process_pdf_to_journal_error_handling(mock_pdf_to_xml):
    """Test process_pdf_to_journal error handling."""
    from api import process_pdf_to_journal

    mock_pdf_to_xml.side_effect = RuntimeError("PDF processing failed")

    with pytest.raises(APIError, match="Failed to process PDF"):
        process_pdf_to_journal("broken.pdf", "Test")
</file>

<file path="tests/models/test_integration.py">
"""Integration tests for XMLDocument ‚Üí Journal ‚Üí HTML workflow."""
import pytest
from pathlib import Path
from models import XMLDocument, Journal


@pytest.mark.integration
def test_full_workflow_with_real_xml():
    """Test complete workflow: Load XML ‚Üí Parse ‚Üí Create Journal ‚Üí Export HTML"""
    # Use test fixture XML file
    xml_path = Path("tests/fixtures/xml/02_Part_1_Goblin_Arrows.xml")

    assert xml_path.exists(), f"Test XML file not found: {xml_path}"

    xml_string = xml_path.read_text()

    # Step 1: Parse to XMLDocument
    doc = XMLDocument.from_xml(xml_string)
    assert doc.title
    assert len(doc.pages) > 0

    # Step 2: Create Journal
    journal = Journal.from_xml_document(doc)
    assert len(journal.chapters) > 0

    # Step 3: Export to HTML
    html = journal.to_foundry_html(image_mapping={})
    assert len(html) > 0
    assert "<h1>" in html

    # Step 4: Validate round-trip
    xml_out = doc.to_xml()
    doc2 = XMLDocument.from_xml(xml_out)
    assert doc2.title == doc.title
    assert len(doc2.pages) == len(doc.pages)


def test_journal_preserves_content():
    """Test Journal doesn't lose content during hierarchy flattening"""
    xml_string = """
    <Chapter_01>
      <page number="1">
        <chapter_title>Title</chapter_title>
        <section>Section 1</section>
        <p>Para 1</p>
      </page>
      <page number="2">
        <p>Para 2</p>
        <section>Section 2</section>
        <p>Para 3</p>
      </page>
    </Chapter_01>
    """
    doc = XMLDocument.from_xml(xml_string)
    journal = Journal.from_xml_document(doc)

    # Count all content elements
    total_content = 0
    for chapter in journal.chapters:
        for section in chapter.sections:
            total_content += len(section.content)

    # Should have 3 paragraphs
    assert total_content == 3
</file>

<file path="tests/conftest.py">
"""
Shared pytest fixtures for D&D Module Converter tests.
"""

import os
import shutil
import sys
import pytest
from pathlib import Path


def pytest_addoption(parser):
    """Add --full flag to run entire test suite"""
    parser.addoption(
        "--full",
        action="store_true",
        default=False,
        help="Run full test suite (skip smoke-only mode)"
    )


def pytest_configure(config):
    """Configure test run based on flags"""
    if config.getoption("--full"):
        # Only clear default marker if no explicit -m flag was provided
        # Check if markexpr is the default from pytest.ini
        if config.option.markexpr == "(not integration and not slow) or smoke":
            config.option.markexpr = ""  # Run all tests


@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    """Store test outcomes on items for later inspection."""
    outcome = yield
    rep = outcome.get_result()
    setattr(item, f"rep_{rep.when}", rep)


def pytest_sessionfinish(session, exitstatus):
    """Auto-escalate to full suite if smoke tests fail"""
    # Check if auto-escalation is enabled
    auto_escalate = os.getenv("AUTO_ESCALATE", "true").lower() == "true"

    # Check if we're already running full suite or using default markers
    is_full_run = session.config.getoption("--full")
    is_default_markers = session.config.option.markexpr == "not integration and not slow"

    # Only escalate if a SMOKE test specifically failed
    if not is_full_run and is_default_markers and exitstatus != 0 and auto_escalate:
        # Check if any failed tests have the "smoke" marker
        smoke_test_failed = False
        for item in session.items:
            if item.get_closest_marker("smoke"):
                # Check if test failed (check setup, call, or teardown)
                if (hasattr(item, "rep_setup") and item.rep_setup.failed) or \
                   (hasattr(item, "rep_call") and item.rep_call.failed) or \
                   (hasattr(item, "rep_teardown") and item.rep_teardown.failed):
                    smoke_test_failed = True
                    break

        if smoke_test_failed:
            print("\n" + "="*70)
            print("‚ö†Ô∏è  Smoke test failed. Running full test suite (including slow/integration)...")
            print("="*70 + "\n")

            # Re-run pytest with full suite
            sys.exit(pytest.main(["--full"] + sys.argv[1:]))


# Project root
PROJECT_ROOT = Path(__file__).parent.parent

# Test paths
TEST_PDF_PATH = PROJECT_ROOT / "data" / "pdfs" / "Lost_Mine_of_Phandelver_test.pdf"
FULL_PDF_PATH = PROJECT_ROOT / "data" / "pdfs" / "Lost_Mine_of_Phandelver.pdf"
TEST_OUTPUT_DIR = PROJECT_ROOT / "tests" / "output"
TEST_RUNS_DIR = PROJECT_ROOT / "tests" / "test_runs"


@pytest.fixture(scope="session")
def project_root():
    """Return the project root directory."""
    return PROJECT_ROOT


@pytest.fixture(scope="session")
def test_pdf_path():
    """Return path to the test PDF file."""
    if not TEST_PDF_PATH.exists():
        pytest.skip(f"Test PDF not found: {TEST_PDF_PATH}")
    return TEST_PDF_PATH


@pytest.fixture(scope="session")
def full_pdf_path():
    """Return path to the full PDF file (for TOC tests)."""
    if not FULL_PDF_PATH.exists():
        pytest.skip(f"Full PDF not found: {FULL_PDF_PATH}")
    return FULL_PDF_PATH


@pytest.fixture(scope="function")
def test_output_dir(tmp_path):
    """
    Return a clean test output directory for each test.
    Uses pytest's tmp_path fixture which is automatically cleaned up.
    """
    output_dir = tmp_path / "output"
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


@pytest.fixture(scope="session")
def persistent_test_output_dir():
    """
    Return the persistent tests/output directory.
    This directory is NOT cleaned up automatically.
    """
    TEST_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    return TEST_OUTPUT_DIR


@pytest.fixture(scope="session")
def integration_test_output_dir():
    """
    Return a persistent output directory for integration tests.
    Creates a single timestamped directory under tests/test_runs/ for the
    entire test session to preserve artifacts for inspection.
    All tests in the session share this directory.
    """
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = TEST_RUNS_DIR / timestamp
    run_dir.mkdir(parents=True, exist_ok=True)
    return run_dir


@pytest.fixture(scope="function")
def clean_test_output():
    """
    Clean the persistent tests/output directory before each test.
    Use this when you want to inspect output after test runs.
    """
    if TEST_OUTPUT_DIR.exists():
        shutil.rmtree(TEST_OUTPUT_DIR)
    TEST_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    yield TEST_OUTPUT_DIR
    # Don't clean up after - keep for inspection


@pytest.fixture(scope="session")
def check_api_key():
    """Check if Gemini API key is available."""
    from dotenv import load_dotenv
    load_dotenv(PROJECT_ROOT / ".env")
    api_key = os.getenv("GeminiImageAPI")
    if not api_key:
        pytest.skip("Gemini API key not found. Set GeminiImageAPI in .env file.")
    return api_key


@pytest.fixture(scope="session")
def check_foundry_credentials():
    """Check if FoundryVTT API credentials are available."""
    from dotenv import load_dotenv
    load_dotenv(PROJECT_ROOT / ".env")

    relay_url = os.getenv("FOUNDRY_RELAY_URL")
    api_key = os.getenv("FOUNDRY_LOCAL_API_KEY")
    client_id = os.getenv("FOUNDRY_LOCAL_CLIENT_ID")

    if not all([relay_url, api_key, client_id]):
        pytest.skip("FoundryVTT credentials not found. Set FOUNDRY_RELAY_URL, FOUNDRY_LOCAL_API_KEY, and FOUNDRY_LOCAL_CLIENT_ID in .env file.")

    return {
        "relay_url": relay_url,
        "api_key": api_key,
        "client_id": client_id
    }


@pytest.fixture(scope="session")
def sample_xml_content():
    """Return sample XML content for testing."""
    return """<Chapter_01_Introduction>
    <page number="1">
        <section>Introduction</section>
        <p>This is a test paragraph with some **bold** text and *italic* text.</p>
        <list>
            <item>First item</item>
            <item>Second item</item>
        </list>
    </page>
</Chapter_01_Introduction>"""


@pytest.fixture(scope="session")
def sample_malformed_xml():
    """Return malformed XML for testing error handling."""
    return """<Chapter_01_Introduction>
    <page number="1">
        <heading>Introduction</heading>
        <paragraph>Unclosed paragraph
        <list>
            <item>First item
        </list>
    </page>"""
</file>

<file path="src/foundry/icon_cache.py">
"""Icon cache for resolving item types to FoundryVTT icon paths."""

import asyncio
import logging
import os
import requests
from difflib import SequenceMatcher
from typing import Dict, List, Optional, Tuple, Union
from google import genai

from util.gemini import generate_content_async

logger = logging.getLogger(__name__)


class IconCache:
    """
    Caches icon paths from FoundryVTT for intelligent icon selection.

    Usage:
        cache = IconCache()
        cache.load()  # Fetch all icons from FoundryVTT
        icon_path = cache.get_icon("Scimitar", category="weapon")
    """

    def __init__(self):
        """Initialize empty icon cache."""
        self._icons_by_category: Dict[str, List[str]] = {}  # Full paths: "weapons/swords" ‚Üí [icons...]
        self._all_icons: List[str] = []
        self._loaded = False

    @property
    def loaded(self) -> bool:
        """Check if cache has been loaded."""
        return self._loaded

    @property
    def icon_count(self) -> int:
        """Get total number of icons in cache."""
        return len(self._all_icons)

    def load(
        self,
        relay_url: Optional[str] = None,
        api_key: Optional[str] = None,
        client_id: Optional[str] = None,
        icon_extensions: Optional[List[str]] = None
    ) -> None:
        """
        Load all icon files from FoundryVTT file system.

        Args:
            relay_url: Relay server URL (defaults to env var)
            api_key: API key (defaults to env var)
            client_id: Client ID (defaults to env var)
            icon_extensions: List of file extensions to include (default: ['.webp', '.png', '.jpg', '.svg'])

        Raises:
            ValueError: If required credentials are missing
            RuntimeError: If API request fails
        """
        logger.info("Loading icon cache from FoundryVTT file system...")

        # Get credentials from env if not provided
        relay_url = relay_url or os.getenv("FOUNDRY_RELAY_URL")
        api_key = api_key or os.getenv("FOUNDRY_API_KEY")
        client_id = client_id or os.getenv("FOUNDRY_CLIENT_ID")

        if not all([relay_url, api_key, client_id]):
            raise ValueError("Missing required credentials: relay_url, api_key, client_id")

        icon_extensions = icon_extensions or ['.webp', '.png', '.jpg', '.svg']

        # Fetch file system recursively from icons/ directory
        endpoint = f"{relay_url}/file-system"
        headers = {"x-api-key": api_key}
        params = {
            "clientId": client_id,
            "path": "icons",
            "recursive": "true",
            "source": "public"  # Public source includes core icons + modules
        }

        try:
            response = requests.get(endpoint, headers=headers, params=params, timeout=60)
            response.raise_for_status()
            data = response.json()

            # Extract icon file paths (API returns 'results' not 'files')
            files = data.get('results', data.get('files', []))
            for file_info in files:
                path = file_info.get('path', '')
                # Filter by extension
                if any(path.endswith(ext) for ext in icon_extensions):
                    self._all_icons.append(path)

                    # Categorize by directory structure
                    self._categorize_icon(path)

            self._loaded = True
            logger.info(f"‚úì Loaded {len(self._all_icons)} icons into cache")
            logger.info(f"  Categories: {list(self._icons_by_category.keys())}")

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to load icon cache: {e}")
            raise RuntimeError(f"Failed to load icon cache: {e}") from e

    def _categorize_icon(self, path: str) -> None:
        """
        Categorize icon by full directory hierarchy.

        Mirrors FoundryVTT's existing structure by preserving all category levels.

        Example: "icons/weapons/swords/sword-steel.webp" creates:
            - "weapons" ‚Üí [path]
            - "weapons/swords" ‚Üí [path]

        This allows matching at different specificity levels.
        """
        parts = path.split('/')
        if len(parts) >= 2 and parts[0] == 'icons':
            # Add to top-level category (e.g., "weapons")
            top_level = parts[1]
            if top_level not in self._icons_by_category:
                self._icons_by_category[top_level] = []
            self._icons_by_category[top_level].append(path)

            # Add to all subcategory levels (e.g., "weapons/swords")
            for i in range(2, len(parts) - 1):  # -1 to exclude filename
                category_path = '/'.join(parts[1:i+1])
                if category_path not in self._icons_by_category:
                    self._icons_by_category[category_path] = []
                self._icons_by_category[category_path].append(path)

    def get_icon(
        self,
        search_term: str,
        category: Optional[str] = None,
        threshold: float = 0.6
    ) -> Optional[str]:
        """
        Get best matching icon path using fuzzy string matching.

        Args:
            search_term: Item/attack/trait name to match (e.g., "Scimitar", "Fireball")
            category: Optional category to narrow search (e.g., "weapons", "magic", "equipment")
            threshold: Similarity threshold for matching (0.0-1.0, default 0.6)

        Returns:
            Best matching icon path if found, None otherwise

        Example:
            >>> cache.get_icon("scimitar", category="weapons")
            'icons/weapons/swords/scimitar-curved-blue.webp'
        """
        if not self._loaded:
            logger.warning("IconCache.get_icon() called before load()")
            return None

        # Normalize search term
        search_term = search_term.lower().replace(" ", "-")

        # Determine search pool
        if category and category in self._icons_by_category:
            search_pool = self._icons_by_category[category]
        else:
            search_pool = self._all_icons

        if not search_pool:
            return None

        # Find best match using fuzzy string matching
        best_match = None
        best_score = 0.0

        for icon_path in search_pool:
            # Extract filename without extension for matching
            filename = icon_path.split('/')[-1].rsplit('.', 1)[0]

            # Calculate similarity against full filename
            similarity = SequenceMatcher(None, search_term, filename).ratio()

            # Also check similarity against individual words in filename (separated by hyphens)
            words = filename.split('-')
            for word in words:
                word_similarity = SequenceMatcher(None, search_term, word).ratio()
                similarity = max(similarity, word_similarity)

            if similarity > best_score and similarity >= threshold:
                best_score = similarity
                best_match = icon_path

        if best_match:
            logger.debug(f"Matched '{search_term}' ‚Üí '{best_match}' (score: {best_score:.2f})")

        return best_match

    def get_icon_by_keywords(
        self,
        keywords: List[str],
        category: Optional[str] = None
    ) -> Optional[str]:
        """
        Get icon matching any of the provided keywords.

        Args:
            keywords: List of keywords to match against (tries in order)
            category: Optional category to narrow search

        Returns:
            First matching icon path, or None if no match

        Example:
            >>> cache.get_icon_by_keywords(["sword", "blade", "weapon"], category="weapons")
            'icons/weapons/swords/sword-steel.webp'
        """
        for keyword in keywords:
            icon = self.get_icon(keyword, category=category)
            if icon:
                return icon

        return None

    async def _select_icon_with_gemini(
        self,
        item_name: str,
        icon_paths: List[str],
        model_name: str = "gemini-2.0-flash"
    ) -> Optional[str]:
        """
        Use Gemini to select the most appropriate icon from a list.

        Args:
            item_name: Name of the item/attack/trait (e.g., "Flaming Sword")
            icon_paths: List of candidate icon file paths
            model_name: Gemini model to use

        Returns:
            Best matching icon path, or None if Gemini fails
        """
        if not icon_paths:
            return None

        # Extract category path + filename (includes folder context like "lightning", "fire", etc.)
        display_paths = []
        for path in icon_paths:
            # Remove "icons/" prefix and file extension, keep category folders
            # e.g., "icons/magic/lightning/bolt-blue.webp" ‚Üí "magic/lightning/bolt-blue"
            clean_path = path.replace('icons/', '', 1).rsplit('.', 1)[0]
            display_paths.append(clean_path)

        prompt = f"""You are an icon selection assistant for a D&D 5e virtual tabletop.

Given an item/ability name, select the MOST APPROPRIATE icon from the list below.

ITEM NAME: {item_name}

AVAILABLE ICONS (numbered, showing category/subcategory/filename):
{chr(10).join(f"{i+1}. {name}" for i, name in enumerate(display_paths))}

INSTRUCTIONS:
1. Consider the item's theme, purpose, and visual style
2. Pay attention to folder names (e.g., "lightning", "fire", "acid") as they indicate icon theme
3. Match based on semantic meaning, not just literal words
4. Respond with ONLY the number of your choice (1-{len(display_paths)})
5. Choose the single best match

Your response (number only):"""

        try:
            # Initialize client and call Gemini
            client = genai.Client(api_key=os.getenv("GeminiImageAPI") or os.getenv("GEMINI_API_KEY"))
            response = await generate_content_async(
                client=client,
                model=model_name,
                contents=prompt,
                config={'temperature': 0.0}
            )

            # Parse response (should be just a number)
            choice_text = response.text.strip()
            choice_num = int(choice_text)

            if 1 <= choice_num <= len(icon_paths):
                selected = icon_paths[choice_num - 1]
                logger.info(f"Gemini selected icon for '{item_name}': {selected}")
                return selected
            else:
                logger.warning(f"Gemini returned invalid choice {choice_num} for '{item_name}'")
                return None

        except Exception as e:
            logger.error(f"Gemini icon selection failed for '{item_name}': {e}")
            return None

    async def get_icon_with_ai_fallback(
        self,
        search_term: str,
        category: Optional[Union[str, List[str]]] = None,
        model_name: str = "gemini-2.0-flash"
    ) -> Optional[str]:
        """
        Get icon using perfect word matching, falling back to Gemini if no perfect match.

        This method first attempts perfect word matching (search words must appear as
        complete words in icon filename). If no perfect match is found, it uses Gemini
        to intelligently select from the category's icons.

        Args:
            search_term: Item/attack/trait name to match
            category: Optional category or list of categories to narrow search
                     (e.g., "weapons", ["weapons", "creatures"])
            model_name: Gemini model to use for AI selection

        Returns:
            Best matching icon path, or None if all methods fail

        Example:
            >>> icon = await cache.get_icon_with_ai_fallback("Flame Sword", category="weapons")
            >>> # Perfect match: "sword" in "flame-sword-fire.webp"
            >>> # Or Gemini selects best from weapon icons

            >>> icon = await cache.get_icon_with_ai_fallback("Claw", category=["weapons", "creatures"])
            >>> # Searches both weapons and creatures folders
        """
        if not self._loaded:
            logger.warning("IconCache.get_icon_with_ai_fallback() called before load()")
            return None

        # Normalize search term and extract words
        search_words = set(search_term.lower().replace("-", " ").split())

        # Determine search pool (merge multiple categories if list provided)
        search_pool = []
        if category:
            categories = [category] if isinstance(category, str) else category
            seen_icons = set()  # Deduplicate icons that appear in multiple categories

            for cat in categories:
                if cat in self._icons_by_category:
                    for icon in self._icons_by_category[cat]:
                        if icon not in seen_icons:
                            search_pool.append(icon)
                            seen_icons.add(icon)
        else:
            search_pool = self._all_icons

        if not search_pool:
            return None

        # Try perfect word matching first
        for icon_path in search_pool:
            filename = icon_path.split('/')[-1].rsplit('.', 1)[0]
            icon_words = set(filename.lower().split('-'))

            # Check if ALL search words are in icon words (perfect match)
            if search_words <= icon_words:  # search_words is subset of icon_words
                logger.info(f"Perfect word match for '{search_term}' ‚Üí '{icon_path}'")
                return icon_path

        # No perfect match found, use Gemini
        logger.info(f"No perfect match for '{search_term}', using Gemini...")

        # Get top 200 icons from category for Gemini to choose from
        # (limit to 200 to keep prompt manageable while providing good coverage)
        candidate_icons = search_pool[:200] if len(search_pool) > 200 else search_pool

        gemini_choice = await self._select_icon_with_gemini(
            search_term,
            candidate_icons,
            model_name=model_name
        )

        if gemini_choice:
            return gemini_choice

        # If Gemini fails, return first icon as last resort
        if search_pool:
            logger.warning(f"Gemini failed for '{search_term}', using first icon from category")
            return search_pool[0]

        return None

    async def get_icons_batch(
        self,
        items: List[Tuple[str, Optional[Union[str, List[str]]]]],
        model_name: str = "gemini-2.0-flash"
    ) -> List[Optional[str]]:
        """
        Get icons for multiple items in parallel using perfect word match + AI fallback.

        Args:
            items: List of (search_term, category) tuples where category can be:
                  - Single string: "weapons"
                  - List of strings: ["weapons", "creatures"]
                  - None: search all icons
            model_name: Gemini model to use for AI selection

        Returns:
            List of icon paths (same order as input), None for failed matches

        Example:
            >>> items = [
            ...     ("Shortsword", "weapons"),
            ...     ("Claw", ["weapons", "creatures"]),
            ...     ("Nimble Escape", ["magic", "skills"])
            ... ]
            >>> icons = await cache.get_icons_batch(items)
        """
        tasks = [
            self.get_icon_with_ai_fallback(
                search_term=term,
                category=cat,
                model_name=model_name
            )
            for term, cat in items
        ]

        return await asyncio.gather(*tasks, return_exceptions=False)
</file>

<file path="src/api.py">
"""
Public API for D&D Module Processing.

This module provides the official interface for external applications
(chat UI, CLI tools, etc.) to interact with the module processing system.

All functions use environment variables for configuration (.env file).
Operations are synchronous and may take several minutes for large PDFs.

Quick Start:
-----------

    from api import create_actor, extract_maps, process_pdf_to_journal

    # Create actor from description
    result = create_actor("A fierce goblin warrior", challenge_rating=1.0)
    print(f"Created: {result.name} ({result.foundry_uuid})")

    # Extract maps from PDF
    maps = extract_maps("data/pdfs/module.pdf", chapter="Chapter 1")
    for map_meta in maps.maps:
        print(f"Found map: {map_meta['name']}")

    # Process complete PDF to journal
    journal = process_pdf_to_journal(
        "data/pdfs/module.pdf",
        "Lost Mine of Phandelver"
    )
    print(f"Created journal: {journal.journal_uuid}")

Error Handling:
--------------

All functions raise APIError on failure:

    from api import APIError

    try:
        result = create_actor("invalid description")
    except APIError as e:
        logger.error(f"Failed: {e}")
        logger.error(f"Original cause: {e.__cause__}")

Configuration:
-------------

Requires .env file with:
    - GeminiImageAPI: Google Gemini API key
    - FOUNDRY_LOCAL_URL: FoundryVTT server URL
    - FOUNDRY_LOCAL_API_KEY: FoundryVTT API key

See CLAUDE.md for complete setup instructions.
"""

import logging
import asyncio
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from actors.orchestrate import create_actor_from_description_sync as orchestrate_create_actor_from_description_sync
from pdf_processing.image_asset_processing.extract_map_assets import extract_maps_from_pdf

logger = logging.getLogger(__name__)


class APIError(Exception):
    """Raised when API operations fail.

    This exception wraps internal errors to provide a clean boundary
    between the public API and internal implementation details.

    The original exception is preserved as __cause__ for debugging.
    """
    pass


@dataclass
class ActorCreationResult:
    """Result from creating a D&D actor.

    Attributes:
        foundry_uuid: FoundryVTT UUID of created actor (e.g., "Actor.abc123")
        name: Name of the actor
        challenge_rating: Creature's challenge rating
        output_dir: Directory containing intermediate files
        timestamp: ISO timestamp of creation
    """
    foundry_uuid: str
    name: str
    challenge_rating: float
    output_dir: Path
    timestamp: str


@dataclass
class MapExtractionResult:
    """Result from extracting maps from a PDF.

    Attributes:
        maps: List of map metadata dictionaries
        output_dir: Directory containing extracted map images
        total_maps: Total number of maps extracted
        timestamp: ISO timestamp of extraction
    """
    maps: List[Dict[str, Any]]
    output_dir: Path
    total_maps: int
    timestamp: str


@dataclass
class JournalCreationResult:
    """Result from creating a FoundryVTT journal.

    Attributes:
        journal_uuid: FoundryVTT UUID of created journal (e.g., "JournalEntry.xyz789")
        journal_name: Name of the journal
        output_dir: Directory containing XML/HTML files
        chapter_count: Number of chapters processed
        timestamp: ISO timestamp of creation
    """
    journal_uuid: str
    journal_name: str
    output_dir: Path
    chapter_count: int
    timestamp: str


def create_actor(
    description: str,
    challenge_rating: Optional[float] = None
) -> ActorCreationResult:
    """
    Create a D&D actor from natural language description.

    This function generates a complete FoundryVTT actor including:
    - Stat block parsing from description
    - Ability scores, skills, and attacks
    - Spell resolution (if applicable)
    - Upload to FoundryVTT server

    Args:
        description: Natural language description of the creature/NPC
                    (e.g., "A fierce goblin warrior with a poisoned blade")
        challenge_rating: CR of the creature (auto-determined from description if None)

    Returns:
        ActorCreationResult with FoundryVTT UUID and output paths

    Raises:
        APIError: If actor creation fails (missing API key, Gemini errors,
                 FoundryVTT connection issues, etc.)

    Example:
        >>> result = create_actor("A cunning kobold scout", challenge_rating=0.5)
        >>> print(f"Created: {result.name} ({result.foundry_uuid})")
        Created: Kobold Scout (Actor.abc123)
    """
    try:
        logger.info(f"Creating actor from description: {description[:50]}...")

        # Call orchestrate function
        orchestrate_result = orchestrate_create_actor_from_description_sync(
            description=description,
            challenge_rating=challenge_rating
        )

        # Extract name from parsed_actor_data
        actor_name = orchestrate_result.parsed_actor_data.name

        # Convert to simplified result
        result = ActorCreationResult(
            foundry_uuid=orchestrate_result.foundry_uuid,
            name=actor_name,
            challenge_rating=orchestrate_result.challenge_rating,
            output_dir=orchestrate_result.output_dir,
            timestamp=orchestrate_result.timestamp
        )

        logger.info(f"Actor created: {result.name} ({result.foundry_uuid})")
        return result

    except Exception as e:
        logger.error(f"Actor creation failed: {e}")
        raise APIError(f"Failed to create actor: {e}") from e


def extract_maps(
    pdf_path: str,
    chapter: Optional[str] = None
) -> MapExtractionResult:
    """
    Extract battle maps and navigation maps from a PDF.

    Uses hybrid approach: PyMuPDF extraction (fast) + Gemini segmentation
    (handles baked-in maps). All pages processed in parallel.

    Args:
        pdf_path: Path to source PDF file (absolute or relative)
        chapter: Optional chapter name for metadata

    Returns:
        MapExtractionResult with extracted maps and metadata

    Raises:
        APIError: If extraction fails (file not found, PDF corrupt,
                 Gemini errors, etc.)

    Example:
        >>> result = extract_maps("data/pdfs/module.pdf", chapter="Chapter 1")
        >>> print(f"Extracted {result.total_maps} maps")
        Extracted 3 maps
        >>> for map_meta in result.maps:
        ...     print(f"  - {map_meta['name']} ({map_meta['type']})")
    """
    try:
        logger.info(f"Extracting maps from: {pdf_path}")

        # Create output directory (extract_maps_from_pdf expects output_dir parameter)
        # Use timestamp-based directory like other pipelines
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("output/runs") / timestamp / "map_assets"
        output_dir.mkdir(parents=True, exist_ok=True)

        # Run async extraction in sync context
        maps = asyncio.run(extract_maps_from_pdf(
            pdf_path=pdf_path,
            output_dir=str(output_dir),
            chapter_name=chapter
        ))

        # Convert MapMetadata objects to dicts
        maps_dicts = [m.model_dump() for m in maps]

        result = MapExtractionResult(
            maps=maps_dicts,
            output_dir=output_dir,
            total_maps=len(maps),
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"Extracted {result.total_maps} maps to {result.output_dir}")
        return result

    except Exception as e:
        logger.error(f"Map extraction failed: {e}")
        raise APIError(f"Failed to extract maps: {e}") from e


def process_pdf_to_journal(
    pdf_path: str,
    journal_name: str,
    skip_upload: bool = False
) -> JournalCreationResult:
    """
    Process a D&D PDF into FoundryVTT journal entries.

    Runs the full pipeline:
    1. Split PDF into chapter PDFs (if not already split)
    2. Generate XML from chapters using Gemini
    3. Upload to FoundryVTT (unless skip_upload=True)

    Args:
        pdf_path: Path to source PDF file
        journal_name: Name for the FoundryVTT journal
        skip_upload: If True, generate XML but don't upload to Foundry

    Returns:
        JournalCreationResult with journal UUID and output paths

    Raises:
        APIError: If processing fails (PDF errors, Gemini errors,
                 FoundryVTT connection issues, etc.)

    Example:
        >>> result = process_pdf_to_journal(
        ...     "data/pdfs/module.pdf",
        ...     "Lost Mine of Phandelver"
        ... )
        >>> print(f"Created journal: {result.journal_uuid}")
        Created journal: JournalEntry.xyz789
    """
    try:
        logger.info(f"Processing PDF to journal: {pdf_path}")

        # Step 1: Run PDF to XML conversion
        # This returns the run directory (e.g., output/runs/20251105_120000)
        logger.info("Step 1/2: Converting PDF to XML...")
        run_dir = run_pdf_to_xml(pdf_path)

        # Count chapters by counting XML files
        xml_files = list(run_dir.glob("documents/*.xml"))
        chapter_count = len(xml_files)

        journal_uuid = ""
        if not skip_upload:
            # Step 2: Upload to FoundryVTT
            logger.info("Step 2/2: Uploading to FoundryVTT...")
            journal_uuid = upload_xml_to_foundry(run_dir, journal_name)
        else:
            logger.info("Skipping upload (skip_upload=True)")

        result = JournalCreationResult(
            journal_uuid=journal_uuid,
            journal_name=journal_name,
            output_dir=run_dir,
            chapter_count=chapter_count,
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"Journal processing complete: {result.journal_name}")
        return result

    except Exception as e:
        logger.error(f"PDF to journal processing failed: {e}")
        raise APIError(f"Failed to process PDF to journal: {e}") from e


def run_pdf_to_xml(pdf_path: str) -> Path:
    """
    Internal helper: Run PDF to XML conversion pipeline.

    This is a simplified placeholder implementation. In production,
    this would need to be refactored from full_pipeline.py to handle
    PDF splitting, XML generation, etc.

    Args:
        pdf_path: Path to source PDF file

    Returns:
        Path to run directory containing generated XML files
    """
    # TODO: Refactor full_pipeline.py to expose this as a clean function
    # For now, this is a placeholder that would need actual implementation
    raise NotImplementedError(
        "run_pdf_to_xml needs to be refactored from scripts/full_pipeline.py"
    )


def upload_xml_to_foundry(run_dir: Path, journal_name: str) -> str:
    """
    Internal helper: Upload XML files to FoundryVTT.

    This is a simplified placeholder implementation. In production,
    this would need to be refactored from upload_to_foundry.py.

    Args:
        run_dir: Directory containing XML files to upload
        journal_name: Name for the journal in FoundryVTT

    Returns:
        Journal UUID (e.g., "JournalEntry.xyz789")
    """
    # TODO: Refactor upload_to_foundry.py to expose this as a clean function
    # For now, this is a placeholder that would need actual implementation
    raise NotImplementedError(
        "upload_xml_to_foundry needs to be refactored from src/foundry/upload_to_foundry.py"
    )
</file>

<file path="src/models/xml_document.py">
"""Models for representing XML documents and converting them to FoundryVTT journals.

This module provides Pydantic models that represent the structure of D&D module XML
documents and methods to parse XML files/strings and convert them to FoundryVTT
journal page format.
"""

import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Literal, Union
from pydantic import BaseModel, ConfigDict


class TableRow(BaseModel):
    """Represents a single row in a table."""
    model_config = ConfigDict(frozen=True)

    cells: List[str]


class Table(BaseModel):
    """Represents a table with rows and cells."""
    model_config = ConfigDict(frozen=True)

    rows: List[TableRow]


class ListItem(BaseModel):
    """Represents a single item in a list."""
    model_config = ConfigDict(frozen=True)

    text: str


class ListContent(BaseModel):
    """Represents an ordered or unordered list."""
    model_config = ConfigDict(frozen=True)

    list_type: Literal["ordered", "unordered"]
    items: List[ListItem]


class DefinitionItem(BaseModel):
    """Represents a single term/description pair in a definition list."""
    model_config = ConfigDict(frozen=True)

    term: str
    description: str


class DefinitionList(BaseModel):
    """Represents a definition list (glossary)."""
    model_config = ConfigDict(frozen=True)

    definitions: List[DefinitionItem]


class StatBlockRaw(BaseModel):
    """Represents raw stat block XML for later parsing.

    Preserves the complete stat block XML element as a string,
    along with the name attribute for identification.
    """
    model_config = ConfigDict(frozen=True)

    name: str
    xml_element: str


class ImageRef(BaseModel):
    """Represents an image placeholder from Gemini.

    Contains a key that identifies the image for later extraction
    and insertion into the rendered output.
    """
    model_config = ConfigDict(frozen=True)

    key: str


class Content(BaseModel):
    """Represents a single content element within a page.

    Content IDs are auto-generated in the format: page_{num}_content_{idx}
    """
    model_config = ConfigDict(frozen=True)

    id: str
    type: Literal["paragraph", "section", "subsection", "subsubsection", "chapter_title", "table", "list", "definition_list", "stat_block", "image_ref", "boxed_text", "header", "footer", "page_number"]
    data: Union[str, Table, ListContent, DefinitionList, StatBlockRaw, ImageRef]


class Page(BaseModel):
    """Represents a single page within an XML document."""
    model_config = ConfigDict(frozen=True)

    number: int
    content: List[Content]


class XMLDocument(BaseModel):
    """Represents a complete XML document (chapter).

    This is an immutable model that can be parsed from XML strings or files.
    """
    model_config = ConfigDict(frozen=True)

    title: str
    pages: List[Page]

    @classmethod
    def from_xml(cls, xml_string: str) -> 'XMLDocument':
        """Parse XML string to XMLDocument.

        Args:
            xml_string: XML content as a string

        Returns:
            XMLDocument representation of the XML

        Raises:
            xml.etree.ElementTree.ParseError: If the XML is malformed
        """
        root = ET.fromstring(xml_string)
        title = root.tag
        pages = []

        for page_elem in root.findall('page'):
            page_num = int(page_elem.get('number', '1'))
            content = []

            for idx, child in enumerate(page_elem):
                content_id = f"page_{page_num}_content_{idx}"
                # Normalize tag names (e.g., <p> -> paragraph)
                content_type = cls._normalize_tag(child.tag)
                content_data = cls._parse_content_data(child)
                content.append(Content(
                    id=content_id,
                    type=content_type,
                    data=content_data
                ))

            pages.append(Page(number=page_num, content=content))

        return cls(title=title, pages=pages)

    @staticmethod
    def _normalize_tag(tag: str) -> str:
        """Normalize XML tag names to standard content types.

        Args:
            tag: The XML tag name

        Returns:
            Normalized content type
        """
        # Map legacy/alternative tag names to standard content types
        tag_mapping = {
            "p": "paragraph",
            "heading": "section",  # Legacy XML files use <heading> for sections
        }
        return tag_mapping.get(tag, tag)

    @staticmethod
    def _parse_content_data(element: ET.Element) -> Union[str, Table, ListContent, DefinitionList, StatBlockRaw, ImageRef]:
        """Parse content data from XML element.

        Args:
            element: XML element to parse

        Returns:
            Parsed content data (string for simple types, model for complex types)
        """
        if element.tag == "table":
            rows = []
            for row_elem in element.findall('row'):
                cells = [cell.text or "" for cell in row_elem.findall('cell')]
                rows.append(TableRow(cells=cells))
            return Table(rows=rows)

        elif element.tag == "list":
            list_type = element.get('type', 'unordered')
            items = []
            for item_elem in element.findall('item'):
                items.append(ListItem(text=item_elem.text or ""))
            return ListContent(list_type=list_type, items=items)

        elif element.tag == "definition_list":
            definitions = []
            # Handle both <definition> and <definition_item> tags
            for def_elem in element.findall('definition'):
                term_elem = def_elem.find('term')
                desc_elem = def_elem.find('description')
                term = term_elem.text or "" if term_elem is not None else ""
                description = desc_elem.text or "" if desc_elem is not None else ""
                definitions.append(DefinitionItem(term=term, description=description))
            for def_elem in element.findall('definition_item'):
                term_elem = def_elem.find('term')
                desc_elem = def_elem.find('definition')
                term = term_elem.text or "" if term_elem is not None else ""
                description = desc_elem.text or "" if desc_elem is not None else ""
                definitions.append(DefinitionItem(term=term, description=description))
            return DefinitionList(definitions=definitions)

        elif element.tag == "stat_block":
            name = element.get('name', 'Unknown')
            # Preserve complete XML element as string
            # Clear tail to avoid whitespace issues during round-trip
            original_tail = element.tail
            element.tail = None
            xml_str = ET.tostring(element, encoding='unicode')
            element.tail = original_tail  # Restore for original tree
            return StatBlockRaw(name=name, xml_element=xml_str)

        elif element.tag == "image_ref":
            key = element.get('key', '')
            return ImageRef(key=key)

        elif element.tag in ("boxed_text", "footer"):
            # Nested content: concatenate all child elements
            parts = []
            if element.text:
                parts.append(element.text)
            for child in element:
                if child.text:
                    parts.append(child.text)
                if child.tail:
                    parts.append(child.tail)
            return " ".join(parts).strip()

        else:
            # Simple text content for paragraph, section, etc.
            # Handle both <paragraph> and <p> tags
            return element.text or ""

    def to_journal_pages(self) -> List[dict]:
        """Convert the document to FoundryVTT journal page format.

        Returns:
            List of dicts with 'name' and 'content' keys for each page
        """
        journal_pages = []

        for page in self.pages:
            html_content = self._page_to_html(page)
            journal_pages.append({
                "name": f"Page {page.number}",
                "content": html_content
            })

        return journal_pages

    def _page_to_html(self, page: Page) -> str:
        """Convert a page to HTML content.

        Args:
            page: The Page to convert

        Returns:
            HTML string representation of the page
        """
        html_parts = []

        for content in page.content:
            if content.type == "chapter_title":
                html_parts.append(f"<h1>{content.data}</h1>")
            elif content.type == "section":
                html_parts.append(f"<h2>{content.data}</h2>")
            elif content.type == "subsection":
                html_parts.append(f"<h3>{content.data}</h3>")
            elif content.type == "subsubsection":
                html_parts.append(f"<h4>{content.data}</h4>")
            elif content.type == "paragraph":
                html_parts.append(self._paragraph_to_html(content.data))
            elif content.type == "table":
                html_parts.append(self._table_to_html(content.data))
            elif content.type == "list":
                html_parts.append(self._list_to_html(content.data))
            elif content.type == "definition_list":
                html_parts.append(self._definition_list_to_html(content.data))

        return "\n".join(html_parts)

    def _paragraph_to_html(self, text: str) -> str:
        """Convert paragraph to HTML, handling markdown-style formatting.

        Args:
            text: Text with markdown formatting

        Returns:
            HTML string with markdown converted to HTML tags
        """
        # Convert **bold** to <strong>
        text = self._convert_markdown_bold(text)
        # Convert *italic* to <em>
        text = self._convert_markdown_italic(text)
        return f"<p>{text}</p>"

    def _convert_markdown_bold(self, text: str) -> str:
        """Convert **bold** markdown to <strong> HTML tags.

        Args:
            text: Text with markdown formatting

        Returns:
            Text with bold converted to HTML
        """
        import re
        return re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', text)

    def _convert_markdown_italic(self, text: str) -> str:
        """Convert *italic* markdown to <em> HTML tags.

        Args:
            text: Text with markdown formatting

        Returns:
            Text with italic converted to HTML
        """
        import re
        # Use negative lookbehind/lookahead to avoid matching ** from bold
        return re.sub(r'(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)', r'<em>\1</em>', text)

    def _table_to_html(self, table: Table) -> str:
        """Convert table to HTML table.

        Args:
            table: Table model to convert

        Returns:
            HTML table string
        """
        html_parts = ["<table>"]
        for row in table.rows:
            html_parts.append("<tr>")
            for cell in row.cells:
                html_parts.append(f"<td>{cell}</td>")
            html_parts.append("</tr>")
        html_parts.append("</table>")
        return "\n".join(html_parts)

    def _list_to_html(self, list_content: ListContent) -> str:
        """Convert list to HTML list.

        Args:
            list_content: ListContent model to convert

        Returns:
            HTML list string (ul or ol)
        """
        tag = "ol" if list_content.list_type == "ordered" else "ul"
        html_parts = [f"<{tag}>"]
        for item in list_content.items:
            html_parts.append(f"<li>{item.text}</li>")
        html_parts.append(f"</{tag}>")
        return "\n".join(html_parts)

    def _definition_list_to_html(self, def_list: DefinitionList) -> str:
        """Convert definition list to HTML definition list.

        Args:
            def_list: DefinitionList model to convert

        Returns:
            HTML definition list string
        """
        html_parts = ["<dl>"]
        for definition in def_list.definitions:
            html_parts.append(f"<dt>{definition.term}</dt>")
            html_parts.append(f"<dd>{definition.description}</dd>")
        html_parts.append("</dl>")
        return "\n".join(html_parts)

    def to_xml(self) -> str:
        """Serialize XMLDocument back to XML string.

        Returns:
            XML string representation of the document with 2-space indentation
        """
        # Create root element with title as tag
        root = ET.Element(self.title)

        # Add all pages
        for page in self.pages:
            page_elem = ET.SubElement(root, 'page')
            page_elem.set('number', str(page.number))

            # Add all content elements
            for content in page.content:
                self._add_content_to_element(page_elem, content)

        # Pretty-print with 2-space indentation
        ET.indent(root, space='  ')

        # Convert to string
        return ET.tostring(root, encoding='unicode')

    def _add_content_to_element(self, parent: ET.Element, content: Content) -> None:
        """Add content element to parent XML element.

        Args:
            parent: Parent XML element to add content to
            content: Content model to serialize
        """
        if content.type == "table":
            # Table with rows and cells
            table_elem = ET.SubElement(parent, 'table')
            table = content.data
            for row in table.rows:
                row_elem = ET.SubElement(table_elem, 'row')
                for cell in row.cells:
                    cell_elem = ET.SubElement(row_elem, 'cell')
                    cell_elem.text = cell

        elif content.type == "list":
            # List with items
            list_elem = ET.SubElement(parent, 'list')
            list_content = content.data
            list_elem.set('type', list_content.list_type)
            for item in list_content.items:
                item_elem = ET.SubElement(list_elem, 'item')
                item_elem.text = item.text

        elif content.type == "definition_list":
            # Definition list with term/description pairs
            def_list_elem = ET.SubElement(parent, 'definition_list')
            def_list = content.data
            for definition in def_list.definitions:
                def_elem = ET.SubElement(def_list_elem, 'definition')
                term_elem = ET.SubElement(def_elem, 'term')
                term_elem.text = definition.term
                desc_elem = ET.SubElement(def_elem, 'description')
                desc_elem.text = definition.description

        elif content.type == "stat_block":
            # Stat block - parse and reconstruct from preserved XML
            stat_block = content.data
            # Parse the preserved XML element
            stat_block_elem = ET.fromstring(stat_block.xml_element)
            # Clear tail text to avoid whitespace mismatches after indentation
            stat_block_elem.tail = None
            parent.append(stat_block_elem)

        elif content.type == "image_ref":
            # Image reference with key attribute
            img_ref = content.data
            img_elem = ET.SubElement(parent, 'image_ref')
            img_elem.set('key', img_ref.key)

        else:
            # Simple text content (paragraph, section, etc.)
            elem = ET.SubElement(parent, content.type)
            elem.text = content.data


def parse_xml_file(file_path: Path) -> XMLDocument:
    """Parse an XML file into an XMLDocument model.

    Args:
        file_path: Path to the XML file

    Returns:
        XMLDocument representation of the file

    Raises:
        FileNotFoundError: If the file doesn't exist
        xml.etree.ElementTree.ParseError: If the XML is malformed
    """
    if not file_path.exists():
        raise FileNotFoundError(f"XML file not found: {file_path}")

    xml_string = file_path.read_text()
    return XMLDocument.from_xml(xml_string)


def parse_xml_string(xml_string: str) -> XMLDocument:
    """Parse an XML string into an XMLDocument model.

    Args:
        xml_string: XML content as a string

    Returns:
        XMLDocument representation of the XML

    Raises:
        xml.etree.ElementTree.ParseError: If the XML is malformed
    """
    return XMLDocument.from_xml(xml_string)
</file>

<file path="src/models/journal.py">
"""Models for Journal representation - mutable working representation with semantic hierarchy.

This module provides the Journal class that transforms XMLDocument's flat page structure
into a semantic hierarchy (Chapters -> Sections -> Subsections -> Subsubsections).
Journal is mutable and owns the image registry for managing image references.
"""

from typing import List, Dict, Optional
from pydantic import BaseModel, Field, PrivateAttr

from models.xml_document import XMLDocument, Content


class ImageMetadata(BaseModel):
    """Metadata for an image reference in the journal.

    Tracks image references from the XML document and their metadata.
    """
    key: str
    source_page: int
    type: str  # "map", "illustration", "diagram", etc.
    description: Optional[str] = None
    file_path: Optional[str] = None
    insert_before_content_id: Optional[str] = None  # For repositioning images


class Subsubsection(BaseModel):
    """Represents a level-4 heading (subsubsection) in the journal hierarchy."""
    title: str
    content: List[Content] = Field(default_factory=list)


class Subsection(BaseModel):
    """Represents a level-3 heading (subsection) in the journal hierarchy."""
    title: str
    subsubsections: List[Subsubsection] = Field(default_factory=list)
    content: List[Content] = Field(default_factory=list)


class Section(BaseModel):
    """Represents a level-2 heading (section) in the journal hierarchy."""
    title: str
    subsections: List[Subsection] = Field(default_factory=list)
    content: List[Content] = Field(default_factory=list)


class Chapter(BaseModel):
    """Represents a chapter in the journal hierarchy."""
    title: str
    sections: List[Section] = Field(default_factory=list)
    content: List[Content] = Field(default_factory=list)


class Journal(BaseModel):
    """Mutable working representation of a D&D module with semantic hierarchy.

    Transforms XMLDocument's flat page structure into a hierarchical structure:
    Chapters -> Sections -> Subsections -> Subsubsections

    The Journal owns the image registry and reassigns content IDs to semantic format:
    chapter_N_section_M_content_K instead of page_X_content_Y.
    """
    title: str
    chapters: List[Chapter] = Field(default_factory=list)
    image_registry: Dict[str, ImageMetadata] = Field(default_factory=dict)
    source: Optional[XMLDocument] = Field(default=None, exclude=True)
    _page_to_semantic_id_map: Dict[str, str] = PrivateAttr(default_factory=dict)

    @classmethod
    def from_xml_document(cls, xml_doc: XMLDocument) -> 'Journal':
        """Create a Journal from an XMLDocument.

        Flattens the page-based structure into a semantic hierarchy where
        sections can span multiple pages.

        Args:
            xml_doc: The source XMLDocument to convert

        Returns:
            Journal with semantic hierarchy
        """
        # Extract image references and build registry
        image_registry = cls._extract_image_refs(xml_doc)

        # Build semantic hierarchy from pages and capture ID mapping
        chapters, id_map = cls._build_hierarchy(xml_doc)

        journal = cls(
            title=xml_doc.title,
            chapters=chapters,
            image_registry=image_registry,
            source=xml_doc
        )
        journal._page_to_semantic_id_map = id_map

        return journal

    @staticmethod
    def _extract_image_refs(xml_doc: XMLDocument) -> Dict[str, ImageMetadata]:
        """Extract image references from XMLDocument and build registry.

        Extracts all ImageRef elements from the XMLDocument's pages and creates
        ImageMetadata entries for each one. The ImageRef elements remain in the
        content stream for later rendering.

        Page number parsing:
        - If key format is "page_X_..." (e.g., "page_5_top_battle_map"), extract X
        - Otherwise, use the actual page number where the ImageRef appears

        Type inference:
        - If key contains "battle_map" or "map": type = "map"
        - If key contains "illustration": type = "illustration"
        - If key contains "diagram": type = "diagram"
        - Otherwise: type = "unknown"

        Args:
            xml_doc: The source XMLDocument

        Returns:
            Dictionary mapping image keys to ImageMetadata
        """
        import re
        registry = {}

        for page in xml_doc.pages:
            for content in page.content:
                if content.type == "image_ref":
                    image_ref = content.data
                    key = image_ref.key

                    # Parse page number from key if format is page_X_...
                    page_num_match = re.match(r'page_(\d+)_', key)
                    if page_num_match:
                        source_page = int(page_num_match.group(1))
                    else:
                        # Fall back to actual page number
                        source_page = page.number

                    # Infer type from key
                    if "battle_map" in key or "_map" in key or key.endswith("_map"):
                        img_type = "map"
                    elif "illustration" in key:
                        img_type = "illustration"
                    elif "diagram" in key:
                        img_type = "diagram"
                    else:
                        img_type = "unknown"

                    # Create ImageMetadata entry
                    registry[key] = ImageMetadata(
                        key=key,
                        source_page=source_page,
                        type=img_type
                    )

        return registry

    @staticmethod
    def _build_hierarchy(xml_doc: XMLDocument) -> tuple[List[Chapter], Dict[str, str]]:
        """Build semantic hierarchy from page-based XMLDocument structure.

        Converts flat page structure to hierarchical structure where sections
        can span multiple pages. Reassigns content IDs to semantic format.

        Args:
            xml_doc: The source XMLDocument

        Returns:
            Tuple of (chapters, id_map) where id_map maps original page-based IDs
            to new semantic IDs (e.g., "page_5_content_3" -> "chapter_0_section_2_content_5")
        """
        chapters = []
        id_map = {}  # Maps original ID -> semantic ID
        current_chapter = None
        current_section = None
        current_subsection = None
        current_subsubsection = None

        chapter_idx = -1
        section_idx = -1
        subsection_idx = -1
        subsubsection_idx = -1
        content_counter = 0

        # Iterate through all pages and content
        for page in xml_doc.pages:
            for content in page.content:
                if content.type == "chapter_title":
                    # Close previous containers
                    if current_subsubsection is not None and current_subsection is not None:
                        current_subsection.subsubsections.append(current_subsubsection)
                    if current_subsection is not None and current_section is not None:
                        current_section.subsections.append(current_subsection)
                    if current_section is not None and current_chapter is not None:
                        current_chapter.sections.append(current_section)
                    if current_chapter is not None:
                        chapters.append(current_chapter)

                    # Start a new chapter
                    chapter_idx += 1
                    section_idx = -1
                    subsection_idx = -1
                    subsubsection_idx = -1
                    content_counter = 0

                    current_chapter = Chapter(title=content.data)
                    current_section = None
                    current_subsection = None
                    current_subsubsection = None

                elif content.type == "section":
                    # Close previous containers
                    if current_subsubsection is not None and current_subsection is not None:
                        current_subsection.subsubsections.append(current_subsubsection)
                    if current_subsection is not None and current_section is not None:
                        current_section.subsections.append(current_subsection)
                    if current_section is not None and current_chapter is not None:
                        current_chapter.sections.append(current_section)

                    # Start a new section
                    section_idx += 1
                    subsection_idx = -1
                    subsubsection_idx = -1
                    content_counter = 0

                    current_section = Section(title=content.data)
                    current_subsection = None
                    current_subsubsection = None

                elif content.type == "subsection":
                    # Close previous containers
                    if current_subsubsection is not None and current_subsection is not None:
                        current_subsection.subsubsections.append(current_subsubsection)
                    if current_subsection is not None and current_section is not None:
                        current_section.subsections.append(current_subsection)

                    # Start a new subsection
                    subsection_idx += 1
                    subsubsection_idx = -1
                    content_counter = 0

                    current_subsection = Subsection(title=content.data)
                    current_subsubsection = None

                elif content.type == "subsubsection":
                    # Close previous container
                    if current_subsubsection is not None and current_subsection is not None:
                        current_subsection.subsubsections.append(current_subsubsection)

                    # Start a new subsubsection
                    subsubsection_idx += 1
                    content_counter = 0

                    current_subsubsection = Subsubsection(title=content.data)

                else:
                    # Regular content - add to current container
                    # Reassign content ID to semantic format (include subsection/subsubsection to ensure uniqueness)
                    if current_subsubsection is not None:
                        new_id = f"chapter_{chapter_idx}_section_{section_idx}_subsection_{subsection_idx}_subsubsection_{subsubsection_idx}_content_{content_counter}"
                    elif current_subsection is not None:
                        new_id = f"chapter_{chapter_idx}_section_{section_idx}_subsection_{subsection_idx}_content_{content_counter}"
                    else:
                        new_id = f"chapter_{chapter_idx}_section_{section_idx}_content_{content_counter}"
                    content_counter += 1

                    # Track the ID mapping (original -> semantic)
                    if content.id:
                        id_map[content.id] = new_id

                    # Create new content with reassigned ID
                    new_content = Content(
                        id=new_id,
                        type=content.type,
                        data=content.data
                    )

                    # Add to the most specific container available
                    if current_subsubsection is not None:
                        current_subsubsection.content.append(new_content)
                    elif current_subsection is not None:
                        current_subsection.content.append(new_content)
                    elif current_section is not None:
                        current_section.content.append(new_content)
                    elif current_chapter is not None:
                        current_chapter.content.append(new_content)

        # Close final containers
        if current_subsubsection is not None and current_subsection is not None:
            current_subsection.subsubsections.append(current_subsubsection)
        if current_subsection is not None and current_section is not None:
            current_section.subsections.append(current_subsection)
        if current_section is not None and current_chapter is not None:
            current_chapter.sections.append(current_section)
        if current_chapter is not None:
            chapters.append(current_chapter)

        return chapters, id_map

    def add_image(self, key: str, metadata: ImageMetadata):
        """Add new image (scene artwork, custom, etc.) to registry.

        Args:
            key: Unique identifier for the image
            metadata: ImageMetadata object with image details
        """
        self.image_registry[key] = metadata

    def reposition_image(self, key: str, new_content_id: str):
        """Move image to different location by setting insert_before_content_id.

        Args:
            key: Image key in the registry
            new_content_id: Content ID to insert the image before
        """
        if key in self.image_registry:
            self.image_registry[key].insert_before_content_id = new_content_id

    def remove_image(self, key: str):
        """Remove image from registry.

        Args:
            key: Image key to remove
        """
        if key in self.image_registry:
            del self.image_registry[key]

    def add_map_assets(self, maps_metadata: List[Dict], image_dir, positioning_mode: str = "page"):
        """Add map assets from extraction metadata to image registry.

        Automatically positions maps using one of two strategies:
        - "page": Position based on source page number (uses section boundaries)
        - "semantic": Use Gemini to match map name to best section/subsection

        Args:
            maps_metadata: List of map metadata dicts from maps_metadata.json
            image_dir: Path to directory containing map image files (can be str or Path)
            positioning_mode: "page" (default) or "semantic"
        """
        from pathlib import Path

        image_dir = Path(image_dir)

        for map_data in maps_metadata:
            # Generate key from page number and name
            page_num = map_data["page_num"]
            safe_name = map_data["name"].lower().replace(" ", "_")
            key = f"page_{page_num:03d}_{safe_name}"

            # Find file path
            file_path = None
            for ext in [".png", ".jpg", ".jpeg"]:
                candidate = image_dir / f"{key}{ext}"
                if candidate.exists():
                    file_path = candidate
                    break

            # Create ImageMetadata
            metadata = ImageMetadata(
                key=key,
                source_page=page_num,
                type="map",
                description=map_data.get("name"),
                file_path=str(file_path) if file_path else None
            )

            # Find insertion point based on mode
            if positioning_mode == "semantic":
                insert_id = self._find_content_by_semantic_match(map_data["name"])
            else:  # "page" mode (default)
                insert_id = self._find_content_after_page(page_num)

            if insert_id:
                metadata.insert_before_content_id = insert_id

            self.image_registry[key] = metadata

    def _find_content_after_page(self, page_num: int) -> Optional[str]:
        """Find the first content ID that appears after a given source page.

        Prefers to position at the start of the section that contains the page
        (better for maps that represent entire sections). Falls back to first
        content on the page if no section is found.

        Args:
            page_num: Source page number (1-indexed)

        Returns:
            Semantic content ID or None if not found
        """
        if not self.source:
            return None

        # Strategy 1: Find the section that contains this page and position at its first content
        current_section_title = None
        for page in self.source.pages:
            for content in page.content:
                if content.type == "section":
                    current_section_title = content.data

                # If we're on the target page and have a section title, find its first content
                if page.number == page_num and current_section_title:
                    # Find the first non-heading content in this section (from any page)
                    found_section = False
                    for search_page in self.source.pages:
                        for search_content in search_page.content:
                            if search_content.type == "section" and search_content.data == current_section_title:
                                found_section = True
                            elif found_section and search_content.type not in ["chapter_title", "section", "subsection", "subsubsection", "header", "footer"]:
                                if search_content.id and search_content.id in self._page_to_semantic_id_map:
                                    return self._page_to_semantic_id_map[search_content.id]

        # Strategy 2: Fallback to first non-heading content on the page
        for page in self.source.pages:
            if page.number >= page_num:
                for content in page.content:
                    if content.type not in ["chapter_title", "section", "subsection", "subsubsection", "header", "footer"]:
                        if content.id and content.id in self._page_to_semantic_id_map:
                            return self._page_to_semantic_id_map[content.id]

        # Strategy 3: Ultimate fallback
        return self._get_first_content_id_heuristic()

    def _get_first_content_id_heuristic(self) -> Optional[str]:
        """Get first content ID as fallback heuristic."""
        for chapter in self.chapters:
            if chapter.content:
                return chapter.content[0].id
            for section in chapter.sections:
                if section.content:
                    return section.content[0].id
        return None

    def _find_content_by_semantic_match(self, map_name: str) -> Optional[str]:
        """Use Gemini to find the best section/subsection for a map based on its name.

        Args:
            map_name: Name of the map (e.g., "Cragmaw Hideout", "Goblin Den")

        Returns:
            Content ID for insertion point, or None if no match found
        """
        import os
        from util.gemini import GeminiAPI

        # Build list of all sections and subsections with their first content IDs
        sections_data = []
        for ch_idx, chapter in enumerate(self.chapters):
            for sec_idx, section in enumerate(chapter.sections):
                if section.content:
                    sections_data.append({
                        "path": f"{chapter.title} ‚Üí {section.title}",
                        "content_id": section.content[0].id
                    })

                for subsec_idx, subsection in enumerate(section.subsections):
                    if subsection.content:
                        sections_data.append({
                            "path": f"{chapter.title} ‚Üí {section.title} ‚Üí {subsection.title}",
                            "content_id": subsection.content[0].id
                        })

        if not sections_data:
            return None

        # Create prompt for Gemini
        sections_list = "\n".join([f"{i+1}. {s['path']}" for i, s in enumerate(sections_data)])

        prompt = f"""You are analyzing a D&D adventure module to position a map named "{map_name}".

Below is a list of all sections in the module:

{sections_list}

Which section number (1-{len(sections_data)}) is the BEST place to insert the "{map_name}" map?
The map should appear at the START of the most relevant section.

Respond with ONLY the number (e.g., "5"), nothing else."""

        try:
            gemini_api = GeminiAPI()
            response = gemini_api.generate_content(prompt)

            if response and response.text:
                # Parse the number from response
                import re
                match = re.search(r'\d+', response.text.strip())
                if match:
                    section_idx = int(match.group()) - 1  # Convert to 0-indexed
                    if 0 <= section_idx < len(sections_data):
                        return sections_data[section_idx]["content_id"]
        except Exception as e:
            # If Gemini fails, fall back to first content
            pass

        return self._get_first_content_id_heuristic()

    def add_scene_artwork(self, scenes: List[Dict], image_dir):
        """Add scene artwork to image registry with intelligent positioning.

        Positions scenes at section/subsection boundaries by fuzzy-matching
        section_path to Journal hierarchy.

        Args:
            scenes: List of scene dicts with section_path, name, description
            image_dir: Path to scene_artwork/images directory (can be str or Path)
        """
        import re
        from pathlib import Path

        image_dir = Path(image_dir)

        for i, scene in enumerate(scenes, start=1):
            # Generate key from scene name
            safe_name = re.sub(r'[^\w\s-]', '', scene["name"].lower())
            safe_name = re.sub(r'[-\s]+', '_', safe_name)
            key = f"scene_{safe_name}"

            # Find file path (format: scene_NNN_name.png)
            file_path = None
            for image_file in image_dir.glob(f"scene_{i:03d}_*.png"):
                file_path = image_file
                break

            # Create ImageMetadata
            metadata = ImageMetadata(
                key=key,
                source_page=0,  # Scene artwork doesn't have source page
                type="illustration",
                description=scene.get("description"),
                file_path=str(file_path) if file_path else None
            )

            # Find insertion point by matching section_path
            insert_id = self._find_section_by_path(scene["section_path"])
            if insert_id:
                metadata.insert_before_content_id = insert_id

            self.image_registry[key] = metadata

    def _find_section_by_path(self, section_path: str) -> Optional[str]:
        """Find content ID for a section by fuzzy-matching section_path.

        Section path format: "Chapter Title ‚Üí Section Title ‚Üí Subsection Title"

        Args:
            section_path: Hierarchical path from scene extraction

        Returns:
            Content ID of first content in matched section/subsection
        """
        import re

        # Parse section path
        parts = [p.strip() for p in section_path.split("‚Üí")]
        if len(parts) < 2:
            return None

        chapter_title = parts[0]
        section_title = parts[1] if len(parts) > 1 else None
        subsection_title = parts[2] if len(parts) > 2 else None

        # Normalize titles for fuzzy matching (lowercase, remove punctuation)
        def normalize(text):
            return re.sub(r'[^\w\s]', '', text.lower())

        chapter_norm = normalize(chapter_title)

        # Find matching chapter
        for chapter in self.chapters:
            if normalize(chapter.title) == chapter_norm:
                # If only chapter specified, insert at first section
                if not section_title:
                    if chapter.sections and chapter.sections[0].content:
                        return chapter.sections[0].content[0].id
                    return None

                # Find matching section
                section_norm = normalize(section_title)
                for section in chapter.sections:
                    if normalize(section.title) == section_norm:
                        # If only chapter + section, insert at first content
                        if not subsection_title:
                            if section.content:
                                return section.content[0].id
                            return None

                        # Find matching subsection
                        subsection_norm = normalize(subsection_title)
                        for subsection in section.subsections:
                            if normalize(subsection.title) == subsection_norm:
                                if subsection.content:
                                    return subsection.content[0].id

        return None

    def to_foundry_html(self, image_mapping: Optional[Dict[str, str]] = None) -> str:
        """Export journal to FoundryVTT-ready HTML format.

        Renders the semantic hierarchy (Chapters -> Sections -> Subsections -> Subsubsections)
        into HTML with proper heading levels (h1 -> h2 -> h3 -> h4) and inserts images
        from the image_mapping.

        Args:
            image_mapping: Dictionary mapping image keys to URLs/paths for rendering.
                          Keys should match those in image_registry.

        Returns:
            HTML string with semantic structure and embedded images
        """
        if image_mapping is None:
            image_mapping = {}

        html_parts = []

        # Render each chapter
        for chapter in self.chapters:
            html_parts.append(f"<h1>{chapter.title}</h1>\n")

            # Render chapter-level content
            for content in chapter.content:
                html_parts.append(self._render_content(content, image_mapping))

            # Render sections
            for section in chapter.sections:
                html_parts.append(self._render_section(section, image_mapping, level=2))

        return "".join(html_parts)

    def _render_section(self, section: Section, image_mapping: Dict[str, str], level: int) -> str:
        """Render a section with proper heading level.

        Args:
            section: Section to render
            image_mapping: Dictionary mapping image keys to URLs/paths
            level: Heading level (2 for section, increments for nested levels)

        Returns:
            HTML string for the section
        """
        html_parts = []

        # Render section title
        html_parts.append(f"<h{level}>{section.title}</h{level}>\n")

        # Render section-level content
        for content in section.content:
            html_parts.append(self._render_content(content, image_mapping))

        # Render subsections
        for subsection in section.subsections:
            html_parts.append(self._render_subsection(subsection, image_mapping, level + 1))

        return "".join(html_parts)

    def _render_subsection(self, subsection: Subsection, image_mapping: Dict[str, str], level: int) -> str:
        """Render a subsection with proper heading level.

        Args:
            subsection: Subsection to render
            image_mapping: Dictionary mapping image keys to URLs/paths
            level: Heading level (3 for subsection, increments for nested levels)

        Returns:
            HTML string for the subsection
        """
        html_parts = []

        # Render subsection title
        html_parts.append(f"<h{level}>{subsection.title}</h{level}>\n")

        # Render subsection-level content
        for content in subsection.content:
            html_parts.append(self._render_content(content, image_mapping))

        # Render subsubsections
        for subsubsection in subsection.subsubsections:
            html_parts.append(self._render_subsubsection(subsubsection, image_mapping, level + 1))

        return "".join(html_parts)

    def _render_subsubsection(self, subsubsection: Subsubsection, image_mapping: Dict[str, str], level: int) -> str:
        """Render a subsubsection with proper heading level.

        Args:
            subsubsection: Subsubsection to render
            image_mapping: Dictionary mapping image keys to URLs/paths
            level: Heading level (4 for subsubsection)

        Returns:
            HTML string for the subsubsection
        """
        html_parts = []

        # Render subsubsection title
        html_parts.append(f"<h{level}>{subsubsection.title}</h{level}>\n")

        # Render content
        for content in subsubsection.content:
            html_parts.append(self._render_content(content, image_mapping))

        return "".join(html_parts)

    def _render_content(self, content: Content, image_mapping: Dict[str, str]) -> str:
        """Render a single content element with image insertion support.

        Checks if any images should be inserted before this content element
        (via insert_before_content_id in image_registry) and renders them first.
        Then renders the content element itself.

        Args:
            content: Content element to render
            image_mapping: Dictionary mapping image keys to URLs/paths

        Returns:
            HTML string for the content element (including any images to insert before it)
        """
        import re
        html_parts = []

        # Check if any images should be inserted before this content
        for key, metadata in self.image_registry.items():
            if metadata.insert_before_content_id == content.id:
                # Insert image before this content
                if key in image_mapping:
                    html_parts.append(f'<img src="{image_mapping[key]}" alt="{metadata.type}" />\n')

        # Render the content element itself
        if content.type == "paragraph":
            # Convert markdown formatting to HTML
            text = self._convert_markdown_to_html(content.data)
            html_parts.append(f"<p>{text}</p>\n")

        elif content.type == "boxed_text":
            # Render boxed text using <aside> with decorative styling
            text = self._convert_markdown_to_html(content.data)
            html_parts.append(
                '<aside style="position: relative; background: #fef9e7; padding: 20px 50px; '
                'margin: 20px 0; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);">\n'
                '  <span style="position: absolute; left: 15px; top: 10px; bottom: 10px; width: 4px; '
                'background: linear-gradient(to bottom, #5c3317 0%, #8b4513 20%, #5c3317 50%, #8b4513 80%, #5c3317 100%); '
                'border-radius: 2px;"></span>\n'
                '  <span style="position: absolute; right: 15px; top: 10px; bottom: 10px; width: 4px; '
                'background: linear-gradient(to bottom, #5c3317 0%, #8b4513 20%, #5c3317 50%, #8b4513 80%, #5c3317 100%); '
                'border-radius: 2px;"></span>\n'
                f'  <p>{text}</p>\n'
                '</aside>\n'
            )

        elif content.type == "image_ref":
            # Render image_ref at its original location
            key = content.data.key
            if key in image_mapping:
                img_type = self.image_registry.get(key, ImageMetadata(key=key, source_page=0, type="image")).type
                html_parts.append(f'<img src="{image_mapping[key]}" alt="{img_type}" />\n')

        elif content.type == "table":
            # Render table structure
            html_parts.append('<table border="1">\n')
            for row in content.data.rows:
                html_parts.append('  <tr>\n')
                for cell in row.cells:
                    cell_text = self._convert_markdown_to_html(cell)
                    html_parts.append(f'    <td>{cell_text}</td>\n')
                html_parts.append('  </tr>\n')
            html_parts.append('</table>\n')

        elif content.type == "list":
            # Render list (ordered or unordered)
            list_tag = "ol" if content.data.list_type == "ordered" else "ul"
            html_parts.append(f'<{list_tag}>\n')
            for item in content.data.items:
                item_text = self._convert_markdown_to_html(item.text)
                html_parts.append(f'  <li>{item_text}</li>\n')
            html_parts.append(f'</{list_tag}>\n')

        elif content.type == "definition_list":
            # Render definition list
            html_parts.append('<dl>\n')
            for definition in content.data.definitions:
                term_text = self._convert_markdown_to_html(definition.term)
                desc_text = self._convert_markdown_to_html(definition.description)
                html_parts.append(f'  <dt>{term_text}</dt>\n')
                html_parts.append(f'  <dd>{desc_text}</dd>\n')
            html_parts.append('</dl>\n')

        # Skip other content types (footer, page_number, stat_block, etc.)
        # These are handled elsewhere or not rendered in journal HTML

        return "".join(html_parts)

    def _convert_markdown_to_html(self, text: str) -> str:
        """Convert Markdown formatting to HTML tags.

        Args:
            text: Text with Markdown formatting (**bold**, *italic*)

        Returns:
            Text with HTML tags (<strong>, <em>)
        """
        import re

        if not text:
            return text

        # Convert bold first (to avoid conflicts with italic)
        text = re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', text)
        # Then convert italic
        text = re.sub(r'\*(.+?)\*', r'<em>\1</em>', text)

        return text

    def to_html(self, image_mapping: Optional[Dict[str, str]] = None) -> str:
        """Export journal to HTML format.

        Currently a stub that calls to_foundry_html(). May be extended in the future
        to support different HTML export formats (e.g., standalone HTML pages).

        Args:
            image_mapping: Dictionary mapping image keys to URLs/paths for rendering

        Returns:
            HTML string
        """
        return self.to_foundry_html(image_mapping)

    def to_markdown(self, image_mapping: Optional[Dict[str, str]] = None) -> str:
        """Export journal to Markdown format.

        Args:
            image_mapping: Dictionary mapping image keys to URLs/paths for rendering

        Returns:
            Markdown string (currently a placeholder)
        """
        return "TODO: Markdown export not yet implemented"

    def export_standalone_html(self, output_dir) -> str:
        """Export journal as standalone HTML with embedded images.

        Creates a self-contained HTML export with images copied to a local directory.
        Useful for viewing/sharing journals without FoundryVTT.

        Directory structure created:
            output_dir/
                journal.html       - Main HTML file
                images/            - Copied image files
                    *.png

        Args:
            output_dir: Path to output directory (can be str or Path)

        Returns:
            Path to generated journal.html file

        Raises:
            ValueError: If output_dir creation fails
        """
        from pathlib import Path
        import shutil

        output_dir = Path(output_dir)
        images_dir = output_dir / "images"

        # Create directories
        output_dir.mkdir(parents=True, exist_ok=True)
        images_dir.mkdir(exist_ok=True)

        # Build image mapping with relative paths and copy images
        image_mapping = {}
        for key, metadata in self.image_registry.items():
            if metadata.file_path:
                source_path = Path(metadata.file_path)
                if source_path.exists():
                    # Copy to images directory
                    dest_path = images_dir / source_path.name
                    shutil.copy2(source_path, dest_path)
                    # Use relative path in HTML
                    image_mapping[key] = f"images/{source_path.name}"

        # Generate HTML content
        html_content = self.to_foundry_html(image_mapping)

        # Wrap in complete HTML document with styling
        full_html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{self.title}</title>
    <style>
        body {{
            font-family: 'Bookman Old Style', Georgia, serif;
            max-width: 900px;
            margin: 40px auto;
            padding: 20px;
            background: #f5f5dc;
            color: #2c1810;
            line-height: 1.6;
        }}
        h1 {{
            color: #8b4513;
            border-bottom: 3px solid #8b4513;
            padding-bottom: 10px;
            font-size: 2.5em;
        }}
        h2 {{
            color: #a0522d;
            margin-top: 40px;
            font-size: 2em;
            border-bottom: 2px solid #cd853f;
        }}
        h3 {{
            color: #cd853f;
            margin-top: 30px;
            font-size: 1.5em;
        }}
        h4 {{
            color: #d2691e;
            margin-top: 20px;
            font-size: 1.2em;
        }}
        p {{
            margin: 15px 0;
            text-align: justify;
        }}
        img {{
            display: block;
            max-width: 100%;
            height: auto;
            margin: 30px auto;
            border: 3px solid #8b4513;
            box-shadow: 0 4px 8px rgba(0,0,0,0.3);
            border-radius: 4px;
        }}
        table {{
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            background: white;
        }}
        table td {{
            border: 1px solid #8b4513;
            padding: 8px;
        }}
        aside {{
            background: #fffaf0 !important;
            border-left: 4px solid #8b4513 !important;
            padding: 15px 20px !important;
            margin: 20px 0 !important;
            font-style: italic;
        }}
        ul, ol {{
            margin: 15px 0;
            padding-left: 40px;
        }}
        li {{
            margin: 8px 0;
        }}
    </style>
</head>
<body>
{html_content}
</body>
</html>"""

        # Write to file
        output_file = output_dir / "journal.html"
        output_file.write_text(full_html, encoding='utf-8')

        return str(output_file)
</file>

<file path="tests/models/test_xml_document.py">
"""Tests for XMLDocument models."""

import pytest
from pathlib import Path
from xml.etree.ElementTree import ParseError

from models.xml_document import (
    XMLDocument,
    Page,
    Content,
    parse_xml_file,
    parse_xml_string,
)


class TestContent:
    """Test Content class."""

    def test_content_creation(self):
        """Test creating a content element."""
        content = Content(
            id="page_1_content_0",
            type="paragraph",
            data="This is a test paragraph."
        )
        assert content.id == "page_1_content_0"
        assert content.type == "paragraph"
        assert content.data == "This is a test paragraph."

    def test_content_immutability(self):
        """Test that content is immutable (frozen)."""
        content = Content(
            id="page_1_content_0",
            type="paragraph",
            data="Test"
        )
        with pytest.raises(Exception):  # Pydantic raises ValidationError
            content.data = "Modified"

    def test_content_with_section_type(self):
        """Test creating section type content."""
        content = Content(
            id="page_1_content_0",
            type="section",
            data="Introduction"
        )
        assert content.type == "section"
        assert content.data == "Introduction"


class TestPage:
    """Test Page class."""

    def test_page_creation(self):
        """Test creating a page."""
        content1 = Content(id="page_1_content_0", type="section", data="Test")
        content2 = Content(id="page_1_content_1", type="paragraph", data="Content")
        page = Page(number=1, content=[content1, content2])

        assert page.number == 1
        assert len(page.content) == 2
        assert page.content[0].type == "section"
        assert page.content[1].type == "paragraph"

    def test_page_immutability(self):
        """Test that page is immutable (frozen)."""
        page = Page(number=1, content=[])
        with pytest.raises(Exception):  # Pydantic raises ValidationError
            page.number = 2

    def test_empty_page(self):
        """Test creating page with no content."""
        page = Page(number=1, content=[])
        assert len(page.content) == 0


class TestXMLDocument:
    """Test XMLDocument class."""

    def test_document_creation(self):
        """Test creating a document."""
        page1 = Page(number=1, content=[])
        page2 = Page(number=2, content=[])
        doc = XMLDocument(title="Chapter_01_Introduction", pages=[page1, page2])

        assert doc.title == "Chapter_01_Introduction"
        assert len(doc.pages) == 2

    def test_document_immutability(self):
        """Test that document is immutable (frozen)."""
        doc = XMLDocument(title="Test", pages=[])
        with pytest.raises(Exception):  # Pydantic raises ValidationError
            doc.title = "Modified"

    def test_from_xml_simple(self):
        """Test parsing simple XML string."""
        xml_string = """<Chapter_01_Introduction>
    <page number="1">
        <section>Introduction</section>
        <paragraph>This is a test paragraph.</paragraph>
    </page>
</Chapter_01_Introduction>"""

        doc = XMLDocument.from_xml(xml_string)
        assert doc.title == "Chapter_01_Introduction"
        assert len(doc.pages) == 1
        assert doc.pages[0].number == 1
        assert len(doc.pages[0].content) == 2

        # Check auto-generated IDs
        assert doc.pages[0].content[0].id == "page_1_content_0"
        assert doc.pages[0].content[1].id == "page_1_content_1"

        # Check content types and data
        assert doc.pages[0].content[0].type == "section"
        assert doc.pages[0].content[0].data == "Introduction"
        assert doc.pages[0].content[1].type == "paragraph"
        assert doc.pages[0].content[1].data == "This is a test paragraph."

    def test_from_xml_multiple_pages(self):
        """Test parsing XML with multiple pages."""
        xml_string = """<Chapter_01>
    <page number="1">
        <section>Section 1</section>
        <paragraph>Page 1 content.</paragraph>
    </page>
    <page number="2">
        <section>Section 2</section>
        <paragraph>Page 2 content.</paragraph>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        assert len(doc.pages) == 2
        assert doc.pages[0].number == 1
        assert doc.pages[1].number == 2

        # Check IDs are unique per page
        assert doc.pages[0].content[0].id == "page_1_content_0"
        assert doc.pages[1].content[0].id == "page_2_content_0"

    def test_from_xml_all_content_types(self):
        """Test parsing XML with all content types."""
        xml_string = """<Chapter_01>
    <page number="1">
        <chapter_title>Chapter Title</chapter_title>
        <section>Section</section>
        <subsection>Subsection</subsection>
        <subsubsection>Subsubsection</subsubsection>
        <paragraph>Paragraph text.</paragraph>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        content = doc.pages[0].content

        assert len(content) == 5
        assert content[0].type == "chapter_title"
        assert content[1].type == "section"
        assert content[2].type == "subsection"
        assert content[3].type == "subsubsection"
        assert content[4].type == "paragraph"

    def test_from_xml_empty_text(self):
        """Test parsing XML with empty text elements."""
        xml_string = """<Chapter_01>
    <page number="1">
        <section></section>
        <paragraph/>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        assert doc.pages[0].content[0].data == ""
        assert doc.pages[0].content[1].data == ""


class TestParseXMLFile:
    """Test parsing XML files into XMLDocument models."""

    def test_parse_simple_file(self, tmp_path):
        """Test parsing a simple XML file."""
        xml_content = """<Chapter_01_Introduction>
    <page number="1">
        <section>Introduction</section>
        <paragraph>This is a test paragraph.</paragraph>
    </page>
</Chapter_01_Introduction>"""

        xml_file = tmp_path / "chapter_01.xml"
        xml_file.write_text(xml_content)

        doc = parse_xml_file(xml_file)
        assert doc.title == "Chapter_01_Introduction"
        assert len(doc.pages) == 1
        assert doc.pages[0].number == 1

    def test_parse_multiple_pages(self, tmp_path):
        """Test parsing XML file with multiple pages."""
        xml_content = """<Chapter_01>
    <page number="1">
        <section>Section 1</section>
        <paragraph>Page 1 content.</paragraph>
    </page>
    <page number="2">
        <section>Section 2</section>
        <paragraph>Page 2 content.</paragraph>
    </page>
</Chapter_01>"""

        xml_file = tmp_path / "chapter.xml"
        xml_file.write_text(xml_content)

        doc = parse_xml_file(xml_file)
        assert len(doc.pages) == 2
        assert doc.pages[0].number == 1
        assert doc.pages[1].number == 2

    def test_parse_nonexistent_file(self):
        """Test parsing nonexistent file raises error."""
        with pytest.raises(FileNotFoundError):
            parse_xml_file(Path("/nonexistent/file.xml"))

    def test_parse_invalid_xml(self, tmp_path):
        """Test parsing invalid XML raises error."""
        xml_file = tmp_path / "invalid.xml"
        xml_file.write_text("<invalid><unclosed>")

        with pytest.raises(ParseError):
            parse_xml_file(xml_file)


class TestParseXMLString:
    """Test parsing XML strings into XMLDocument models."""

    def test_parse_string_simple(self):
        """Test parsing a simple XML string."""
        xml_string = """<Chapter_01>
    <page number="1">
        <section>Test</section>
        <paragraph>Content</paragraph>
    </page>
</Chapter_01>"""

        doc = parse_xml_string(xml_string)
        assert doc.title == "Chapter_01"
        assert len(doc.pages) == 1

    def test_parse_string_invalid_xml(self):
        """Test parsing invalid XML string raises error."""
        with pytest.raises(ParseError):
            parse_xml_string("<invalid><unclosed>")

    def test_parse_empty_string(self):
        """Test parsing empty string raises error."""
        with pytest.raises(ParseError):
            parse_xml_string("")


class TestXMLDocumentToJournal:
    """Test converting XMLDocument to FoundryVTT journal format."""

    def test_document_to_journal_pages(self):
        """Test converting document to journal page format."""
        xml_string = """<Chapter_01>
    <page number="1">
        <section>Introduction</section>
        <paragraph>First paragraph.</paragraph>
        <paragraph>Second paragraph.</paragraph>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()

        assert len(journal_pages) == 1
        assert journal_pages[0]["name"] == "Page 1"
        assert "content" in journal_pages[0]
        assert "<h2>Introduction</h2>" in journal_pages[0]["content"]
        assert "<p>First paragraph.</p>" in journal_pages[0]["content"]

    def test_document_multiple_pages_to_journal(self):
        """Test converting multi-page document to journal format."""
        xml_string = """<Chapter_01>
    <page number="1">
        <section>Intro</section>
        <paragraph>Page 1</paragraph>
    </page>
    <page number="2">
        <section>Content</section>
        <paragraph>Page 2</paragraph>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()

        assert len(journal_pages) == 2
        assert journal_pages[0]["name"] == "Page 1"
        assert journal_pages[1]["name"] == "Page 2"

    def test_html_heading_levels(self):
        """Test that content types map to correct HTML heading levels."""
        xml_string = """<Chapter_01>
    <page number="1">
        <chapter_title>Chapter Title</chapter_title>
        <section>Section</section>
        <subsection>Subsection</subsection>
        <subsubsection>Subsubsection</subsubsection>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()
        html = journal_pages[0]["content"]

        assert "<h1>Chapter Title</h1>" in html
        assert "<h2>Section</h2>" in html
        assert "<h3>Subsection</h3>" in html
        assert "<h4>Subsubsection</h4>" in html

    def test_markdown_conversion(self):
        """Test markdown-style formatting in paragraphs."""
        xml_string = """<Chapter_01>
    <page number="1">
        <paragraph>This has **bold** and *italic* text.</paragraph>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()
        html = journal_pages[0]["content"]

        assert "<strong>bold</strong>" in html
        assert "<em>italic</em>" in html
        assert "**" not in html  # Markdown should be converted
        assert "*italic*" not in html  # Single asterisks should be converted


class TestTableContent:
    """Test Table and TableRow content types."""

    def test_table_parsing_from_xml(self):
        """Test parsing table content from XML."""
        xml_string = """<Chapter_01>
    <page number="1">
        <table>
            <row>
                <cell>Header 1</cell>
                <cell>Header 2</cell>
            </row>
            <row>
                <cell>Data 1</cell>
                <cell>Data 2</cell>
            </row>
        </table>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        assert len(doc.pages[0].content) == 1
        content = doc.pages[0].content[0]
        assert content.type == "table"

    def test_table_to_html_conversion(self):
        """Test table converts to HTML table."""
        xml_string = """<Chapter_01>
    <page number="1">
        <table>
            <row>
                <cell>Name</cell>
                <cell>CR</cell>
            </row>
            <row>
                <cell>Goblin</cell>
                <cell>1/4</cell>
            </row>
        </table>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()
        html = journal_pages[0]["content"]

        assert "<table>" in html
        assert "<tr>" in html
        assert "<td>Name</td>" in html
        assert "<td>Goblin</td>" in html
        assert "</table>" in html


class TestListContent:
    """Test ListContent for ordered and unordered lists."""

    def test_unordered_list_parsing(self):
        """Test parsing unordered list from XML."""
        xml_string = """<Chapter_01>
    <page number="1">
        <list type="unordered">
            <item>First item</item>
            <item>Second item</item>
            <item>Third item</item>
        </list>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        assert len(doc.pages[0].content) == 1
        content = doc.pages[0].content[0]
        assert content.type == "list"

    def test_ordered_list_parsing(self):
        """Test parsing ordered list from XML."""
        xml_string = """<Chapter_01>
    <page number="1">
        <list type="ordered">
            <item>Step one</item>
            <item>Step two</item>
        </list>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        content = doc.pages[0].content[0]
        assert content.type == "list"

    def test_unordered_list_to_html(self):
        """Test unordered list converts to HTML <ul>."""
        xml_string = """<Chapter_01>
    <page number="1">
        <list type="unordered">
            <item>Item A</item>
            <item>Item B</item>
        </list>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()
        html = journal_pages[0]["content"]

        assert "<ul>" in html
        assert "<li>Item A</li>" in html
        assert "<li>Item B</li>" in html
        assert "</ul>" in html

    def test_ordered_list_to_html(self):
        """Test ordered list converts to HTML <ol>."""
        xml_string = """<Chapter_01>
    <page number="1">
        <list type="ordered">
            <item>First</item>
            <item>Second</item>
        </list>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()
        html = journal_pages[0]["content"]

        assert "<ol>" in html
        assert "<li>First</li>" in html
        assert "<li>Second</li>" in html
        assert "</ol>" in html


class TestDefinitionListContent:
    """Test DefinitionList and DefinitionItem for glossaries."""

    def test_definition_list_parsing(self):
        """Test parsing definition list from XML."""
        xml_string = """<Chapter_01>
    <page number="1">
        <definition_list>
            <definition>
                <term>Hit Points</term>
                <description>A measure of a creature's health.</description>
            </definition>
            <definition>
                <term>Armor Class</term>
                <description>How difficult a creature is to hit.</description>
            </definition>
        </definition_list>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        assert len(doc.pages[0].content) == 1
        content = doc.pages[0].content[0]
        assert content.type == "definition_list"

    def test_definition_list_to_html(self):
        """Test definition list converts to HTML <dl>."""
        xml_string = """<Chapter_01>
    <page number="1">
        <definition_list>
            <definition>
                <term>Goblin</term>
                <description>Small humanoid creature.</description>
            </definition>
            <definition>
                <term>Dragon</term>
                <description>Powerful reptilian creature.</description>
            </definition>
        </definition_list>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        journal_pages = doc.to_journal_pages()
        html = journal_pages[0]["content"]

        assert "<dl>" in html
        assert "<dt>Goblin</dt>" in html
        assert "<dd>Small humanoid creature.</dd>" in html
        assert "<dt>Dragon</dt>" in html
        assert "<dd>Powerful reptilian creature.</dd>" in html
        assert "</dl>" in html


class TestStatBlockContent:
    """Test StatBlockRaw for preserving complete stat block XML."""

    def test_stat_block_parsing(self):
        """Test parsing stat block from XML preserves raw XML."""
        xml_string = """<Chapter_01>
    <page number="1">
        <stat_block name="Goblin">
GOBLIN
Small humanoid, neutral evil
Armor Class 15
Hit Points 7 (2d6)
        </stat_block>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        assert len(doc.pages[0].content) == 1
        content = doc.pages[0].content[0]
        assert content.type == "stat_block"

    def test_stat_block_preserves_name(self):
        """Test stat block preserves name attribute."""
        xml_string = """<Chapter_01>
    <page number="1">
        <stat_block name="Goblin">
GOBLIN
Small humanoid, neutral evil
        </stat_block>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        from models.xml_document import StatBlockRaw

        stat_block = doc.pages[0].content[0].data
        assert isinstance(stat_block, StatBlockRaw)
        assert stat_block.name == "Goblin"

    def test_stat_block_preserves_xml(self):
        """Test stat block preserves complete XML element."""
        xml_string = """<Chapter_01>
    <page number="1">
        <stat_block name="Goblin">
GOBLIN
Small humanoid, neutral evil
Armor Class 15
Hit Points 7 (2d6)
        </stat_block>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        from models.xml_document import StatBlockRaw

        stat_block = doc.pages[0].content[0].data
        assert isinstance(stat_block, StatBlockRaw)
        assert "GOBLIN" in stat_block.xml_element
        assert "Armor Class 15" in stat_block.xml_element
        assert "Hit Points 7" in stat_block.xml_element


class TestImageRefContent:
    """Test ImageRef for image placeholders."""

    def test_image_ref_parsing(self):
        """Test parsing image_ref from XML."""
        xml_string = """<Chapter_01>
    <page number="5">
        <paragraph>Before image</paragraph>
        <image_ref key="page_5_top_battle_map" />
        <paragraph>After image</paragraph>
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        assert len(doc.pages[0].content) == 3
        assert doc.pages[0].content[0].type == "paragraph"
        assert doc.pages[0].content[1].type == "image_ref"
        assert doc.pages[0].content[2].type == "paragraph"

    def test_image_ref_preserves_key(self):
        """Test image_ref preserves key attribute."""
        xml_string = """<Chapter_01>
    <page number="5">
        <image_ref key="page_5_top_battle_map" />
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        from models.xml_document import ImageRef

        img_ref = doc.pages[0].content[0].data
        assert isinstance(img_ref, ImageRef)
        assert img_ref.key == "page_5_top_battle_map"

    def test_image_ref_with_different_keys(self):
        """Test multiple image_refs with different keys."""
        xml_string = """<Chapter_01>
    <page number="5">
        <image_ref key="page_5_map_1" />
        <paragraph>Some text</paragraph>
        <image_ref key="page_5_map_2" />
    </page>
</Chapter_01>"""

        doc = XMLDocument.from_xml(xml_string)
        from models.xml_document import ImageRef

        img_ref1 = doc.pages[0].content[0].data
        img_ref2 = doc.pages[0].content[2].data

        assert isinstance(img_ref1, ImageRef)
        assert isinstance(img_ref2, ImageRef)
        assert img_ref1.key == "page_5_map_1"
        assert img_ref2.key == "page_5_map_2"


class TestXMLDocumentSerialization:
    """Test XMLDocument serialization with to_xml() method."""

    def test_xmldocument_round_trip(self):
        """Test round-trip conversion: XML ‚Üí XMLDocument ‚Üí XML ‚Üí XMLDocument.

        This validates that to_xml() produces valid XML that can be parsed back
        into an equivalent XMLDocument structure.
        """
        # Original XML with all content types
        original_xml = """<Chapter_01_Introduction>
    <page number="1">
        <chapter_title>Introduction</chapter_title>
        <section>The Story</section>
        <paragraph>This is a **bold** test with *italic* text.</paragraph>
        <table>
            <row>
                <cell>Name</cell>
                <cell>CR</cell>
            </row>
            <row>
                <cell>Goblin</cell>
                <cell>1/4</cell>
            </row>
        </table>
        <list type="unordered">
            <item>First item</item>
            <item>Second item</item>
        </list>
        <list type="ordered">
            <item>Step one</item>
            <item>Step two</item>
        </list>
        <definition_list>
            <definition>
                <term>Hit Points</term>
                <description>A measure of health.</description>
            </definition>
        </definition_list>
        <stat_block name="Goblin">
GOBLIN
Small humanoid, neutral evil
Armor Class 15
        </stat_block>
        <image_ref key="page_1_battle_map" />
    </page>
    <page number="2">
        <section>Second Page</section>
        <paragraph>More content here.</paragraph>
    </page>
</Chapter_01_Introduction>"""

        # First parse: XML ‚Üí XMLDocument
        doc1 = XMLDocument.from_xml(original_xml)

        # Serialize: XMLDocument ‚Üí XML
        serialized_xml = doc1.to_xml()

        # Second parse: XML ‚Üí XMLDocument
        doc2 = XMLDocument.from_xml(serialized_xml)

        # Verify both documents are equivalent
        assert doc1.title == doc2.title
        assert len(doc1.pages) == len(doc2.pages)

        for page1, page2 in zip(doc1.pages, doc2.pages):
            assert page1.number == page2.number
            assert len(page1.content) == len(page2.content)

            for content1, content2 in zip(page1.content, page2.content):
                assert content1.type == content2.type
                # For complex types, compare the model representation
                if isinstance(content1.data, str):
                    assert content1.data == content2.data
                else:
                    # Use model_dump for deep comparison
                    assert content1.data.model_dump() == content2.data.model_dump()


class TestRealXMLIntegration:
    """Test XMLDocument can parse real generated XML files."""

    @pytest.mark.smoke
    @pytest.mark.integration
    def test_xmldocument_parses_real_xml(self):
        """Smoke test: Parse real XML files from pdf_to_xml.py

        This test validates that the XMLDocument model can handle real-world
        XML files produced by pdf_to_xml.py. It gracefully skips if no XML
        files are found (e.g., in fresh worktrees or CI environments).
        """
        # Use test fixture XML file
        xml_file = Path("tests/fixtures/xml/01_Introduction.xml")

        assert xml_file.exists(), f"Test XML file not found: {xml_file}"

        # Parse the real XML file
        xml_string = xml_file.read_text()
        doc = XMLDocument.from_xml(xml_string)

        # Validate basic structure
        assert doc.title.startswith("Chapter_"), f"Expected title to start with 'Chapter_', got: {doc.title}"
        assert len(doc.pages) > 0, "Document should have at least one page"
        assert all(page.number > 0 for page in doc.pages), "All page numbers should be positive"
        assert all(len(page.content) > 0 for page in doc.pages), "All pages should have content"

        # Validate content IDs are unique across entire document
        all_ids = [c.id for page in doc.pages for c in page.content]
        assert len(all_ids) == len(set(all_ids)), "Content IDs must be unique across document"

        # Validate page-based ID format
        for page in doc.pages:
            for idx, content in enumerate(page.content):
                expected_id = f"page_{page.number}_content_{idx}"
                assert content.id == expected_id, f"Expected ID {expected_id}, got {content.id}"
</file>

<file path="src/foundry/actors/converter.py">
"""Convert ParsedActorData to FoundryVTT actor JSON format."""

import asyncio
import logging
import secrets
from typing import Dict, Any, Optional, TYPE_CHECKING, List, Tuple
from .models import ParsedActorData, Attack, AttackSave

if TYPE_CHECKING:
    from .spell_cache import SpellCache
    from ..icon_cache import IconCache

logger = logging.getLogger(__name__)


def _generate_activity_id() -> str:
    """Generate a unique 16-character ID for activities and items.

    FoundryVTT requires exactly 16 alphanumeric characters [a-zA-Z0-9].
    """
    import string
    alphabet = string.ascii_letters + string.digits  # a-zA-Z0-9
    return ''.join(secrets.choice(alphabet) for _ in range(16))


def _base_activity_structure() -> dict:
    """Common fields for all activities."""
    return {
        "activation": {
            "type": "action",
            "value": None,
            "override": False,
            "condition": ""
        },
        "consumption": {
            "scaling": {"allowed": False},
            "spellSlot": True,
            "targets": []
        },
        "description": {"chatFlavor": ""},
        "duration": {
            "units": "inst",
            "concentration": False,
            "override": False
        },
        "effects": [],
        "range": {"override": False, "units": "self"},
        "target": {
            "template": {"contiguous": False, "units": "ft", "type": ""},
            "affects": {"choice": False, "type": "creature", "count": "1", "special": ""},
            "override": False,
            "prompt": True
        },
        "uses": {"spent": 0, "recovery": [], "max": ""}
    }


def _create_attack_activity(attack: Attack, activity_id: str) -> dict:
    """Create an attack-type activity for a weapon."""
    base = _base_activity_structure()
    base.update({
        "type": "attack",
        "_id": activity_id,
        "sort": 0,
        "attack": {
            "bonus": str(attack.attack_bonus),
            "flat": True,
            "critical": {"threshold": None},
            "type": {"value": attack.attack_type, "classification": "weapon"},
            "ability": ""
        },
        "damage": {
            "includeBase": True,
            "parts": [],
            "critical": {"bonus": ""}
        },
        "name": ""
    })
    return base


def _create_save_activity(save: AttackSave, activity_id: str) -> dict:
    """Create a save-type activity."""
    base = _base_activity_structure()
    base.update({
        "type": "save",
        "_id": activity_id,
        "sort": 0,
        "save": {
            "ability": [save.ability],
            "dc": {"calculation": "", "formula": str(save.dc)}
        },
        "damage": {
            "parts": [
                {
                    "custom": {"enabled": False, "formula": ""},
                    "number": d.number,
                    "denomination": d.denomination,
                    "bonus": d.bonus,
                    "types": [d.type],
                    "scaling": {"number": 1}
                }
                for d in save.damage
            ],
            "onSave": save.on_save
        },
        "name": ""
    })
    return base


def _create_ongoing_damage_activity(save: AttackSave, activity_id: str) -> dict:
    """Create ongoing damage activity (e.g., poison each turn)."""
    base = _base_activity_structure()
    base["activation"]["type"] = "turnStart"
    base.update({
        "type": "damage",
        "_id": activity_id,
        "sort": 0,
        "damage": {
            "critical": {"allow": False},
            "parts": [
                {
                    "custom": {"enabled": False, "formula": ""},
                    "number": d.number,
                    "denomination": d.denomination,
                    "bonus": d.bonus,
                    "types": [d.type],
                    "scaling": {"number": 1}
                }
                for d in save.ongoing_damage
            ]
        },
        "name": f"Add'l {save.ongoing_damage[0].type.capitalize()} Damage" if save.ongoing_damage else ""
    })
    return base


def _is_save_based_action(trait) -> bool:
    """Detect if a trait is a save-based action (e.g., breath weapon, gaze attack)."""
    import re

    # Must be an action or bonus action (not passive)
    if trait.activation not in ["action", "bonus"]:
        return False

    # Check description for saving throw pattern
    desc_lower = trait.description.lower()
    if re.search(r'dc \d+.*saving throw', desc_lower):
        return True

    return False


def _parse_save_action(trait) -> dict:
    """Parse save-based action description to extract save data."""
    import re

    desc = trait.description
    result = {
        "template_type": "cone",
        "template_size": "",
        "damage_parts": [],
        "on_save": "half",
        "save_ability": "dex",
        "save_dc": ""
    }

    # Extract template type and size (e.g., "60-foot cone", "30-foot line")
    template_match = re.search(r'(\d+)-foot (cone|line|cube|sphere|cylinder)', desc, re.IGNORECASE)
    if template_match:
        result["template_size"] = template_match.group(1)
        result["template_type"] = template_match.group(2).lower()

    # Extract damage (e.g., "63 (18d6) fire damage")
    damage_match = re.search(r'(\d+)\s*\((\d+)d(\d+)\)\s+(\w+)\s+damage', desc, re.IGNORECASE)
    if damage_match:
        num_dice = int(damage_match.group(2))
        die_size = int(damage_match.group(3))
        damage_type = damage_match.group(4).lower()

        result["damage_parts"] = [{
            "custom": {"enabled": False, "formula": ""},
            "number": num_dice,
            "denomination": die_size,
            "bonus": "",
            "types": [damage_type],
            "scaling": {"number": 1}
        }]

    # Extract save DC (e.g., "DC 21 Dexterity saving throw")
    save_match = re.search(r'DC (\d+)\s+(\w+)\s+saving throw', desc, re.IGNORECASE)
    if save_match:
        result["save_dc"] = save_match.group(1)
        ability = save_match.group(2).lower()[:3]  # "dex", "con", etc.
        result["save_ability"] = ability

    # Extract on-save behavior (e.g., "half as much damage on a successful one")
    if re.search(r'half.*damage.*success', desc, re.IGNORECASE):
        result["on_save"] = "half"
    elif re.search(r'no.*damage.*success', desc, re.IGNORECASE):
        result["on_save"] = "none"

    return result


async def convert_to_foundry(
    parsed_actor: ParsedActorData,
    spell_cache: Optional['SpellCache'] = None,
    icon_cache: Optional['IconCache'] = None,
    include_spells_in_payload: bool = False,
    use_ai_icons: bool = True
) -> tuple[Dict[str, Any], list[str]]:
    """
    Convert ParsedActorData to FoundryVTT actor JSON structure.

    This function transforms our parsed actor data into the complete JSON
    structure expected by FoundryVTT's D&D 5e system, including:
    - Actor system data (abilities, defenses, movement, senses)
    - Items array (attacks as weapon items, traits as feat items)
    - Spell UUIDs to be added via /give endpoint (returned separately)

    Args:
        parsed_actor: Fully parsed actor data with attacks, traits, spells
        spell_cache: Optional spell cache for UUID lookups
        include_spells_in_payload: If True, include spell stubs in CREATE payload
            (not recommended - spells will lack full compendium data)

    Returns:
        Tuple of (actor_json, spell_uuids):
            - actor_json: Dict containing FoundryVTT actor JSON structure
            - spell_uuids: List of compendium spell UUIDs to add via /give

    Example:
        >>> goblin_data = ParsedActorData(...)
        >>> actor_json, spell_uuids = convert_to_foundry(goblin_data, spell_cache)
        >>> actor_json['name']
        'Goblin'
        >>> spell_uuids
        ['Compendium.dnd5e.spells.ztgcdrWPshKRpFd0']
    """
    logger.info(f"Converting actor '{parsed_actor.name}' to FoundryVTT format...")

    # Calculate ability modifiers
    def ability_mod(score: int) -> int:
        return (score - 10) // 2

    # Build abilities
    abilities = {}
    for ability in ["str", "dex", "con", "int", "wis", "cha"]:
        score = parsed_actor.abilities.get(ability.upper(), 10)
        abilities[ability] = {
            "value": score,
            "proficient": 1 if ability in parsed_actor.saving_throw_proficiencies else 0
        }

    # Build attributes
    attributes = {
        "ac": {"value": parsed_actor.armor_class},
        "hp": {
            "value": parsed_actor.hit_points,
            "max": parsed_actor.hit_points
        },
        "movement": {
            "walk": parsed_actor.speed_walk or 30
        }
    }

    # Add senses
    senses = {}
    if parsed_actor.darkvision:
        senses["darkvision"] = parsed_actor.darkvision
    if parsed_actor.blindsight:
        senses["blindsight"] = parsed_actor.blindsight
    if parsed_actor.tremorsense:
        senses["tremorsense"] = parsed_actor.tremorsense
    if parsed_actor.truesight:
        senses["truesight"] = parsed_actor.truesight
    if senses:
        attributes["senses"] = senses

    # Add spellcasting info if present
    if parsed_actor.spellcasting_ability:
        attributes["spellcasting"] = parsed_actor.spellcasting_ability
        attributes["spelldc"] = parsed_actor.spell_save_dc or 10

    # Build skills
    # FoundryVTT skill abbreviation mapping
    skill_abbrev_map = {
        "acrobatics": "acr",
        "animal handling": "ani",
        "arcana": "arc",
        "athletics": "ath",
        "deception": "dec",
        "history": "his",
        "insight": "ins",
        "intimidation": "itm",
        "investigation": "inv",
        "medicine": "med",
        "nature": "nat",
        "perception": "prc",
        "performance": "prf",
        "persuasion": "per",
        "religion": "rel",
        "sleight of hand": "slt",
        "stealth": "ste",
        "survival": "sur"
    }

    skills = {}
    for skill_prof in parsed_actor.skill_proficiencies:
        skill_key = skill_abbrev_map.get(skill_prof.skill.lower())
        if skill_key:
            skills[skill_key] = {"value": skill_prof.proficiency_level}

    # Build details (simplified like create_creature_actor)
    details = {
        "cr": parsed_actor.challenge_rating,
        "type": {
            "value": parsed_actor.creature_type or "humanoid",
            "subtype": ""
        },
        "alignment": parsed_actor.alignment or "",
        "biography": {
            "value": parsed_actor.biography or "",
            "public": ""
        }
    }

    # Build traits (damage modifiers and condition immunities)
    traits_dict = {}
    if parsed_actor.damage_resistances:
        traits_dict["dr"] = {"value": parsed_actor.damage_resistances.types}
    if parsed_actor.damage_immunities:
        traits_dict["di"] = {"value": parsed_actor.damage_immunities.types}
    if parsed_actor.damage_vulnerabilities:
        traits_dict["dv"] = {"value": parsed_actor.damage_vulnerabilities.types}
    if parsed_actor.condition_immunities:
        traits_dict["ci"] = {"value": parsed_actor.condition_immunities}

    # Build system object (minimal like create_creature_actor)
    system = {
        "abilities": abilities,
        "attributes": attributes,
        "details": details,
        "skills": skills,
        "traits": traits_dict
    }

    # Pre-fetch icons using AI if enabled
    icon_map = {}  # Map from item name to icon path
    if use_ai_icons and icon_cache and icon_cache.loaded:
        logger.info(f"Using AI icon selection for {parsed_actor.name}...")

        # Collect all items that need icons
        items_for_icons: List[Tuple[str, Optional[List[str]]]] = []

        # Helper: check if attack is a natural weapon
        def is_natural_weapon(name: str) -> bool:
            """Detect natural weapons by name (bite, claw, etc.)."""
            natural_keywords = {
                "bite", "claw", "gore", "tail", "slam", "sting", "tentacle",
                "horn", "tusk", "talon", "wing", "fist", "pseudopod", "tendril"
            }
            name_lower = name.lower()
            return any(keyword in name_lower for keyword in natural_keywords)

        # Add attacks (natural weapons use creatures folder only, weapons use both)
        for attack in parsed_actor.attacks:
            if is_natural_weapon(attack.name):
                items_for_icons.append((attack.name, ["creatures"]))
            else:
                items_for_icons.append((attack.name, ["weapons", "creatures"]))

        # Add traits (search both magic and skills folders)
        for trait in parsed_actor.traits:
            items_for_icons.append((trait.name, ["magic", "skills"]))

        # Add multiattack if present (search both magic and skills folders)
        if parsed_actor.multiattack:
            items_for_icons.append((parsed_actor.multiattack.name, ["magic", "skills"]))

        # Batch fetch icons in parallel (perfect word match + Gemini)
        icon_results = await icon_cache.get_icons_batch(
            items_for_icons,
            model_name="gemini-2.0-flash"
        )

        # Build map for easy lookup
        for (item_name, _), icon_path in zip(items_for_icons, icon_results):
            if icon_path:
                icon_map[item_name] = icon_path

        logger.info(f"‚úì Selected {len(icon_map)} icons using AI")

    # Build items array (attacks, traits, spells)
    items = []
    spell_uuids = []  # Collect spell UUIDs to add via /give

    # Convert attacks to weapon items (NEW v10+ structure with activities)
    for attack in parsed_actor.attacks:
        activities = {}

        # Determine if this is a save-only attack (e.g., breath weapon)
        # Save-only attacks have attack_save but no attack roll (attack_bonus is None or 0)
        is_save_only = attack.attack_save is not None and (attack.attack_bonus is None or attack.attack_bonus == 0)

        # 1. Create attack activity (only for attacks with attack rolls)
        if not is_save_only:
            attack_id = _generate_activity_id()
            activities[attack_id] = _create_attack_activity(attack, attack_id)

        # 2. Add save activity if present
        if attack.attack_save:
            save_id = _generate_activity_id()
            activities[save_id] = _create_save_activity(attack.attack_save, save_id)

            # 3. Add ongoing damage activity if present
            if attack.attack_save.ongoing_damage:
                dmg_id = _generate_activity_id()
                activities[dmg_id] = _create_ongoing_damage_activity(attack.attack_save, dmg_id)

        # Select appropriate icon
        # 1. Check for common hardcoded natural weapons first
        weapon_icon = None
        attack_name_lower = attack.name.lower()
        if "bite" in attack_name_lower:
            weapon_icon = "icons/creatures/abilities/mouth-teeth-long-red.webp"
        elif "claw" in attack_name_lower:
            weapon_icon = "icons/creatures/claws/claw-talons-glowing-orange.webp"
        # 2. Use AI-selected icon if available
        elif attack.name in icon_map:
            weapon_icon = icon_map[attack.name]
        # 3. Fallback to fuzzy match or default
        elif icon_cache and icon_cache.loaded:
            matched_icon = icon_cache.get_icon(attack.name, category="weapons")
            if matched_icon:
                weapon_icon = matched_icon

        # Final fallback
        if not weapon_icon:
            weapon_icon = "icons/weapons/swords/scimitar-guard-purple.webp"

        # Build weapon item (v10+ structure)
        item = {
            "_id": _generate_activity_id(),
            "name": attack.name,
            "type": "weapon",
            "img": weapon_icon,
            "system": {
                "description": {"value": attack.additional_effects or ""},
                "activities": activities,
                "damage": {
                    "base": {
                        "number": attack.damage[0].number,
                        "denomination": attack.damage[0].denomination,
                        "bonus": attack.damage[0].bonus.replace("+", ""),
                        "types": [attack.damage[0].type],
                        "custom": {"enabled": False, "formula": ""},
                        "scaling": {"mode": "", "number": None, "formula": ""}
                    },
                    "versatile": {
                        "number": None,
                        "denomination": None,
                        "types": [],
                        "custom": {"enabled": False},
                        "scaling": {"number": 1}
                    }
                },
                "range": {
                    "value": attack.range_short,
                    "long": attack.range_long,
                    "reach": attack.reach,
                    "units": "ft"
                },
                "type": {"value": "natural", "baseItem": ""},
                "properties": [],
                "uses": {"spent": 0, "recovery": [], "max": ""}
            }
        }
        items.append(item)

    # Convert traits to feat items
    for trait in parsed_actor.traits:
        # Create activity for non-passive traits
        activities = {}
        if trait.activation != "passive":
            activity_id = _generate_activity_id()
            activity = _base_activity_structure()

            # Check if this is a save-based action (breath weapon, gaze attack, etc.)
            is_save_action = _is_save_based_action(trait)

            if is_save_action:
                # Create save activity for save-based actions
                save_data = _parse_save_action(trait)
                activity.update({
                    "type": "save",
                    "_id": activity_id,
                    "sort": 0,
                    "name": "",
                    "activation": {
                        "type": trait.activation,
                        "value": None,
                        "override": False,
                        "condition": ""
                    },
                    "target": {
                        "template": {
                            "contiguous": False,
                            "units": "ft",
                            "type": save_data.get("template_type", "cone"),
                            "size": str(save_data.get("template_size", "")),
                            "count": "",
                            "width": "5"
                        },
                        "affects": {
                            "choice": False,
                            "count": "",
                            "type": "creature",
                            "special": ""
                        },
                        "override": False,
                        "prompt": True
                    },
                    "damage": {
                        "parts": save_data.get("damage_parts", []),
                        "onSave": save_data.get("on_save", "half")
                    },
                    "save": {
                        "ability": [save_data.get("save_ability", "dex")],
                        "dc": {
                            "calculation": "",
                            "formula": str(save_data.get("save_dc", ""))
                        }
                    }
                })
            else:
                # Regular utility activity
                activity.update({
                    "type": "utility",
                    "_id": activity_id,
                    "sort": 0,
                    "name": "",
                    "activation": {
                        "type": trait.activation,
                        "value": None,
                        "override": False,
                        "condition": ""
                    }
                })
            activities[activity_id] = activity

        # Select appropriate icon (from AI map if available, else fuzzy match)
        trait_icon = "icons/magic/movement/trail-streak-zigzag-yellow.webp"  # Default
        if trait.name in icon_map:
            trait_icon = icon_map[trait.name]
        elif icon_cache and icon_cache.loaded:
            # Try multiple keyword matches
            keywords = trait.name.lower().split()
            matched_icon = icon_cache.get_icon_by_keywords(keywords, category="magic")
            if matched_icon:
                trait_icon = matched_icon

        item = {
            "_id": _generate_activity_id(),
            "name": trait.name,
            "type": "feat",
            "img": trait_icon,
            "system": {
                "description": {"value": trait.description},
                "activation": {
                    "type": trait.activation,
                    "value": None,
                    "condition": ""
                },
                "activities": activities,
                "uses": {"value": trait.uses, "max": trait.uses} if trait.uses else {}
            }
        }
        items.append(item)

    # Convert multiattack to feat item
    if parsed_actor.multiattack:
        # Create activity for multiattack
        activity_id = _generate_activity_id()
        activity = _base_activity_structure()
        activity.update({
            "type": "utility",
            "_id": activity_id,
            "sort": 0,
            "name": "",
            "activation": {
                "type": parsed_actor.multiattack.activation,
                "value": None,
                "override": False,
                "condition": ""
            }
        })

        # Get icon from AI map, but override generic "Multiattack" with better hardcoded icon
        if parsed_actor.multiattack.name.lower() == "multiattack":
            # Always use combat icon for generic multiattack (don't trust AI for this common case)
            multiattack_icon = "icons/skills/melee/blade-tips-triple-steel.webp"
        else:
            # Other special multiattacks use AI suggestion or generic default
            multiattack_icon = icon_map.get(
                parsed_actor.multiattack.name,
                "icons/magic/movement/trail-streak-zigzag-yellow.webp"
            )

        item = {
            "_id": _generate_activity_id(),
            "name": parsed_actor.multiattack.name,
            "type": "feat",
            "img": multiattack_icon,
            "system": {
                "description": {"value": parsed_actor.multiattack.description},
                "activation": {
                    "type": parsed_actor.multiattack.activation,
                    "value": None,
                    "condition": ""
                },
                "activities": {activity_id: activity},
                "uses": {}
            }
        }
        items.append(item)

    # Convert spells - collect UUIDs for /give endpoint
    for spell in parsed_actor.spells:
        # Get UUID from cache or spell object
        spell_uuid = None
        if spell_cache:
            spell_uuid = spell_cache.get_spell_uuid(spell.name)
        elif spell.uuid:
            spell_uuid = spell.uuid

        if spell_uuid:
            spell_uuids.append(spell_uuid)
        else:
            logger.warning(f"No UUID found for spell '{spell.name}', skipping")

        # Optionally include stub in payload (not recommended)
        if include_spells_in_payload:
            item = {
                "_id": _generate_activity_id(),
                "name": spell.name,
                "type": "spell",
                "img": "icons/magic/air/wind-tornado-wall-blue.webp",
                "system": {
                    "level": spell.level,
                    "school": spell.school or ""
                }
            }
            if spell_uuid:
                item["uuid"] = spell_uuid
            items.append(item)

    # Convert innate spellcasting to feat + spell items
    if parsed_actor.innate_spellcasting:
        innate = parsed_actor.innate_spellcasting

        # Build description from spell list
        spell_lines = []
        # Group by frequency
        by_frequency = {}
        for spell in innate.spells:
            if spell.frequency not in by_frequency:
                by_frequency[spell.frequency] = []
            by_frequency[spell.frequency].append(spell.name)

        for freq, spell_names in sorted(by_frequency.items()):
            spell_list = ", ".join(spell_names)
            spell_lines.append(f"{freq}: {spell_list}")

        description = (
            f"The {parsed_actor.name.lower()}'s spellcasting ability is "
            f"{innate.ability.capitalize()} (spell save DC {innate.save_dc or 10}). "
            f"It can innately cast the following spells, requiring no material components:\n\n"
            + "\n".join(spell_lines)
        )

        # Create Innate Spellcasting feat
        item = {
            "_id": _generate_activity_id(),
            "name": "Innate Spellcasting",
            "type": "feat",
            "img": "icons/magic/air/wind-tornado-wall-blue.webp",
            "system": {
                "description": {"value": description},
                "activation": {"type": "passive"},
                "uses": {}
            }
        }
        items.append(item)

        # Collect UUIDs for innate spells (to add via /give)
        for spell in innate.spells:
            # Look up UUID from spell cache
            spell_uuid = None
            if spell_cache:
                spell_uuid = spell_cache.get_spell_uuid(spell.name)

            if spell_uuid:
                spell_uuids.append(spell_uuid)
            else:
                logger.warning(f"No UUID found for innate spell '{spell.name}', skipping")

            # Optionally include stub in payload (not recommended)
            if include_spells_in_payload:
                spell_item = {
                    "_id": _generate_activity_id(),
                    "name": spell.name,
                    "type": "spell",
                    "img": "icons/magic/air/wind-tornado-wall-blue.webp",
                    "system": {
                        "level": 0,
                        "school": ""
                    }
                }

                if spell_uuid:
                    spell_item["uuid"] = spell_uuid

                # Get spell details from cache
                if spell_cache:
                    spell_data = spell_cache.get_spell_data(spell.name)
                    if spell_data:
                        system_data = spell_data.get("data", {}).get("system") or spell_data.get("system", {})
                        spell_item["system"]["level"] = system_data.get("level", 0)
                        spell_item["system"]["school"] = system_data.get("school", "")

                # Add uses if limited
                if spell.uses:
                    spell_item["system"]["uses"] = {
                        "value": spell.uses,
                        "max": spell.uses,
                        "per": "day"
                    }

                items.append(spell_item)

    # Build final actor structure
    actor = {
        "name": parsed_actor.name,
        "type": "npc",
        "img": "icons/svg/mystery-man.svg",
        "system": system,
        "items": items,
        "effects": [],
        "flags": {}
    }

    logger.info(f"‚úì Converted actor '{parsed_actor.name}' ({len(items)} items, {len(spell_uuids)} spells via /give)")
    return actor, spell_uuids
</file>

</files>
